wandb: Currently logged in as: emanon47. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/qi47/codes/project_reward/text2reward/run_highway/wandb/run-20240507_173743-x6ltxj5r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run highway-custom-lag_3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/emanon47/highway
wandb: üöÄ View run at https://wandb.ai/emanon47/highway/runs/x6ltxj5r/workspace
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
/home/qi47/anaconda3/envs/text2reward/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: [33mWARN: Overriding environment highway-custom-v0 already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./highway_ppo_lagrangian_tensorboard/PPOLagrangian_24
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.27     |
|    ep_rew_mean     | 6.66     |
| time/              |          |
|    fps             | 1        |
|    iterations      | 1        |
|    time_elapsed    | 79       |
|    total_timesteps | 128      |
---------------------------------
--------------------------------------------
| rollout/                     |           |
|    ep_len_mean               | 6.74      |
|    ep_rew_mean               | 5.44      |
| time/                        |           |
|    fps                       | 1         |
|    iterations                | 2         |
|    time_elapsed              | 188       |
|    total_timesteps           | 256       |
| train/                       |           |
|    approx_kl                 | 0.2809744 |
|    average_cost              | 2.5390625 |
|    clip_fraction             | 0.521     |
|    clip_range                | 0.2       |
|    cost_explained_variance   | -1.43e+04 |
|    cost_value_loss           | 237       |
|    early_stop_epoch          | 10        |
|    entropy_loss              | -1.53     |
|    learning_rate             | 0.0005    |
|    loss                      | 94.4      |
|    mean_cost_advantages      | 10.625431 |
|    mean_reward_advantages    | 3.330742  |
|    n_updates                 | 10        |
|    nu                        | 1.01      |
|    nu_loss                   | -2.54     |
|    policy_gradient_loss      | -0.515    |
|    reward_explained_variance | -1.43e+03 |
|    reward_value_loss         | 7.8       |
|    total_cost                | 325.0     |
--------------------------------------------
--------------------------------------------
| rollout/                     |           |
|    ep_len_mean               | 6.23      |
|    ep_rew_mean               | 5.05      |
| time/                        |           |
|    fps                       | 1         |
|    iterations                | 3         |
|    time_elapsed              | 302       |
|    total_timesteps           | 384       |
| train/                       |           |
|    approx_kl                 | 1.3468449 |
|    average_cost              | 0.5859375 |
|    clip_fraction             | 0.86      |
|    clip_range                | 0.2       |
|    cost_explained_variance   | -5.4e+03  |
|    cost_value_loss           | 36.3      |
|    early_stop_epoch          | 10        |
|    entropy_loss              | -0.536    |
|    learning_rate             | 0.0005    |
|    loss                      | 20.9      |
|    mean_cost_advantages      | 1.8422161 |
|    mean_reward_advantages    | -1.049999 |
|    n_updates                 | 20        |
|    nu                        | 1.01      |
|    nu_loss                   | -0.59     |
|    policy_gradient_loss      | -0.427    |
|    reward_explained_variance | -385      |
|    reward_value_loss         | 6.83      |
|    total_cost                | 75.0      |
--------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 5.87        |
|    ep_rew_mean               | 4.74        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 4           |
|    time_elapsed              | 404         |
|    total_timesteps           | 512         |
| train/                       |             |
|    approx_kl                 | 0.011582129 |
|    average_cost              | 0.0         |
|    clip_fraction             | 0.0281      |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.511      |
|    cost_value_loss           | 0.106       |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0529     |
|    learning_rate             | 0.0005      |
|    loss                      | 5.8         |
|    mean_cost_advantages      | -0.2901743  |
|    mean_reward_advantages    | 0.55032414  |
|    n_updates                 | 30          |
|    nu                        | 1.02        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 0.00245     |
|    reward_explained_variance | -160        |
|    reward_value_loss         | 12.3        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.02          |
|    ep_rew_mean               | 4.93          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 5             |
|    time_elapsed              | 502           |
|    total_timesteps           | 640           |
| train/                       |               |
|    approx_kl                 | 2.0557549e-05 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -1.08         |
|    cost_value_loss           | 0.0974        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0427       |
|    learning_rate             | 0.0005        |
|    loss                      | 2.49          |
|    mean_cost_advantages      | -0.29614234   |
|    mean_reward_advantages    | -0.80104625   |
|    n_updates                 | 40            |
|    nu                        | 1.02          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.77e-06      |
|    reward_explained_variance | -144          |
|    reward_value_loss         | 6.31          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 5.86         |
|    ep_rew_mean               | 4.84         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 6            |
|    time_elapsed              | 599          |
|    total_timesteps           | 768          |
| train/                       |              |
|    approx_kl                 | 6.239861e-08 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.431       |
|    cost_value_loss           | 0.0809       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0393      |
|    learning_rate             | 0.0005       |
|    loss                      | 3.92         |
|    mean_cost_advantages      | -0.27246225  |
|    mean_reward_advantages    | 0.6640394    |
|    n_updates                 | 50           |
|    nu                        | 1.02         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.51e-07     |
|    reward_explained_variance | -11.6        |
|    reward_value_loss         | 9.14         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.17        |
|    ep_rew_mean               | 5.16        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 7           |
|    time_elapsed              | 696         |
|    total_timesteps           | 896         |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.882      |
|    cost_value_loss           | 0.0656      |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0381     |
|    learning_rate             | 0.0005      |
|    loss                      | 4.15        |
|    mean_cost_advantages      | -0.24194664 |
|    mean_reward_advantages    | 0.23555517  |
|    n_updates                 | 60          |
|    nu                        | 1.02        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -2.99e-07   |
|    reward_explained_variance | -10.4       |
|    reward_value_loss         | 8.43        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.5          |
|    ep_rew_mean               | 5.49         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 8            |
|    time_elapsed              | 792          |
|    total_timesteps           | 1024         |
| train/                       |              |
|    approx_kl                 | 0.0007489696 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00391      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.198       |
|    cost_value_loss           | 0.0419       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0345      |
|    learning_rate             | 0.0005       |
|    loss                      | 7.19         |
|    mean_cost_advantages      | -0.19625518  |
|    mean_reward_advantages    | 1.0441103    |
|    n_updates                 | 70           |
|    nu                        | 1.03         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.00027     |
|    reward_explained_variance | -34.1        |
|    reward_value_loss         | 16.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.9           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 9             |
|    time_elapsed              | 889           |
|    total_timesteps           | 1152          |
| train/                       |               |
|    approx_kl                 | 0.00049988553 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0.00547       |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.125        |
|    cost_value_loss           | 0.0263        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0322       |
|    learning_rate             | 0.0005        |
|    loss                      | 7.59          |
|    mean_cost_advantages      | -0.14793749   |
|    mean_reward_advantages    | 0.27334422    |
|    n_updates                 | 80            |
|    nu                        | 1.03          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -0.000792     |
|    reward_explained_variance | -28           |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.77        |
|    ep_rew_mean               | 5.75        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 10          |
|    time_elapsed              | 984         |
|    total_timesteps           | 1280        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.018      |
|    cost_value_loss           | 0.0263      |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0327     |
|    learning_rate             | 0.0005      |
|    loss                      | 4.89        |
|    mean_cost_advantages      | -0.14015388 |
|    mean_reward_advantages    | -0.50220823 |
|    n_updates                 | 90          |
|    nu                        | 1.03        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.45e-07   |
|    reward_explained_variance | -4.59       |
|    reward_value_loss         | 10.9        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.12         |
|    ep_rew_mean               | 6.1          |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 11           |
|    time_elapsed              | 1078         |
|    total_timesteps           | 1408         |
| train/                       |              |
|    approx_kl                 | 0.0062223533 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.0125       |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0277       |
|    cost_value_loss           | 0.0237       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0239      |
|    learning_rate             | 0.0005       |
|    loss                      | 5.05         |
|    mean_cost_advantages      | -0.13620633  |
|    mean_reward_advantages    | -0.43019834  |
|    n_updates                 | 100          |
|    nu                        | 1.03         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.0015      |
|    reward_explained_variance | -13.9        |
|    reward_value_loss         | 9.21         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 12            |
|    time_elapsed              | 1174          |
|    total_timesteps           | 1536          |
| train/                       |               |
|    approx_kl                 | 2.2351742e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00458       |
|    cost_value_loss           | 0.0132        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0167       |
|    learning_rate             | 0.0005        |
|    loss                      | 4.64          |
|    mean_cost_advantages      | -0.090349905  |
|    mean_reward_advantages    | 0.6216798     |
|    n_updates                 | 110           |
|    nu                        | 1.03          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.12e-08      |
|    reward_explained_variance | -34.6         |
|    reward_value_loss         | 10.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.04         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 13           |
|    time_elapsed              | 1269         |
|    total_timesteps           | 1664         |
| train/                       |              |
|    approx_kl                 | 0.0011518714 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00391      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0818      |
|    cost_value_loss           | 0.0177       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0179      |
|    learning_rate             | 0.0005       |
|    loss                      | 3.94         |
|    mean_cost_advantages      | -0.09789625  |
|    mean_reward_advantages    | 0.16378823   |
|    n_updates                 | 120          |
|    nu                        | 1.03         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.000456    |
|    reward_explained_variance | -2.7         |
|    reward_value_loss         | 9.55         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.08          |
|    ep_rew_mean               | 6.05          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 14            |
|    time_elapsed              | 1362          |
|    total_timesteps           | 1792          |
| train/                       |               |
|    approx_kl                 | 5.6345016e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.104        |
|    cost_value_loss           | 0.0136        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0218       |
|    learning_rate             | 0.0005        |
|    loss                      | 4.83          |
|    mean_cost_advantages      | -0.097920045  |
|    mean_reward_advantages    | 0.6309512     |
|    n_updates                 | 130           |
|    nu                        | 1.03          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.26e-06     |
|    reward_explained_variance | -18.2         |
|    reward_value_loss         | 11.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 15            |
|    time_elapsed              | 1459          |
|    total_timesteps           | 1920          |
| train/                       |               |
|    approx_kl                 | 2.2817403e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0548        |
|    cost_value_loss           | 0.0108        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0227       |
|    learning_rate             | 0.0005        |
|    loss                      | 7.89          |
|    mean_cost_advantages      | -0.06850459   |
|    mean_reward_advantages    | 0.49708623    |
|    n_updates                 | 140           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.61e-08     |
|    reward_explained_variance | -68.8         |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.31          |
|    ep_rew_mean               | 6.28          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 16            |
|    time_elapsed              | 1552          |
|    total_timesteps           | 2048          |
| train/                       |               |
|    approx_kl                 | 1.3038516e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0305        |
|    cost_value_loss           | 0.0065        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.023        |
|    learning_rate             | 0.0005        |
|    loss                      | 6.74          |
|    mean_cost_advantages      | -0.034759447  |
|    mean_reward_advantages    | 0.5842478     |
|    n_updates                 | 150           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.42e-08     |
|    reward_explained_variance | -97.5         |
|    reward_value_loss         | 14.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.01          |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 17            |
|    time_elapsed              | 1650          |
|    total_timesteps           | 2176          |
| train/                       |               |
|    approx_kl                 | 1.0244548e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0489        |
|    cost_value_loss           | 0.0102        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0232       |
|    learning_rate             | 0.0005        |
|    loss                      | 3.69          |
|    mean_cost_advantages      | -0.051556528  |
|    mean_reward_advantages    | -0.54296887   |
|    n_updates                 | 160           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.09e-07     |
|    reward_explained_variance | -45.7         |
|    reward_value_loss         | 8.49          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.63          |
|    ep_rew_mean               | 5.6           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 18            |
|    time_elapsed              | 1744          |
|    total_timesteps           | 2304          |
| train/                       |               |
|    approx_kl                 | 0.00026499992 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0.00313       |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00167       |
|    cost_value_loss           | 0.00465       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0242       |
|    learning_rate             | 0.0005        |
|    loss                      | 3.49          |
|    mean_cost_advantages      | -0.018106688  |
|    mean_reward_advantages    | -0.5023277    |
|    n_updates                 | 170           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.33e-05     |
|    reward_explained_variance | -32.6         |
|    reward_value_loss         | 8.12          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.61         |
|    ep_rew_mean               | 5.58         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 19           |
|    time_elapsed              | 1838         |
|    total_timesteps           | 2432         |
| train/                       |              |
|    approx_kl                 | 0.0016760556 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00469      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0532       |
|    cost_value_loss           | 0.0053       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0219      |
|    learning_rate             | 0.0005       |
|    loss                      | 3.64         |
|    mean_cost_advantages      | -0.022945158 |
|    mean_reward_advantages    | -0.38732886  |
|    n_updates                 | 180          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.000499    |
|    reward_explained_variance | -25          |
|    reward_value_loss         | 8.63         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.78          |
|    ep_rew_mean               | 5.75          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 20            |
|    time_elapsed              | 1934          |
|    total_timesteps           | 2560          |
| train/                       |               |
|    approx_kl                 | 1.4901161e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.135         |
|    cost_value_loss           | 0.00542       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0171       |
|    learning_rate             | 0.0005        |
|    loss                      | 5.23          |
|    mean_cost_advantages      | -0.011313098  |
|    mean_reward_advantages    | 0.8501831     |
|    n_updates                 | 190           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.79e-07      |
|    reward_explained_variance | -34.6         |
|    reward_value_loss         | 10.6          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.81         |
|    ep_rew_mean               | 5.77         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 21           |
|    time_elapsed              | 2029         |
|    total_timesteps           | 2688         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0555      |
|    cost_value_loss           | 0.00598      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0164      |
|    learning_rate             | 0.0005       |
|    loss                      | 9.42         |
|    mean_cost_advantages      | -0.014349535 |
|    mean_reward_advantages    | 0.7113889    |
|    n_updates                 | 200          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.1e-07     |
|    reward_explained_variance | -63.7        |
|    reward_value_loss         | 17.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.72         |
|    ep_rew_mean               | 5.69         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 22           |
|    time_elapsed              | 2125         |
|    total_timesteps           | 2816         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0357       |
|    cost_value_loss           | 0.00654      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0162      |
|    learning_rate             | 0.0005       |
|    loss                      | 9.98         |
|    mean_cost_advantages      | -0.025970114 |
|    mean_reward_advantages    | 0.53998905   |
|    n_updates                 | 210          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.51e-08    |
|    reward_explained_variance | -60.5        |
|    reward_value_loss         | 20.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.15        |
|    ep_rew_mean               | 6.11        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 23          |
|    time_elapsed              | 2222        |
|    total_timesteps           | 2944        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.118       |
|    cost_value_loss           | 0.00455     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0165     |
|    learning_rate             | 0.0005      |
|    loss                      | 4.4         |
|    mean_cost_advantages      | 0.013925357 |
|    mean_reward_advantages    | -2.0313623  |
|    n_updates                 | 220         |
|    nu                        | 1.04        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.45e-08   |
|    reward_explained_variance | -8.35       |
|    reward_value_loss         | 10.4        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.44         |
|    ep_rew_mean               | 6.4          |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 24           |
|    time_elapsed              | 2316         |
|    total_timesteps           | 3072         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0041       |
|    cost_value_loss           | 0.0044       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0163      |
|    learning_rate             | 0.0005       |
|    loss                      | 6.61         |
|    mean_cost_advantages      | 0.0016585342 |
|    mean_reward_advantages    | 1.2229646    |
|    n_updates                 | 230          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.02e-07    |
|    reward_explained_variance | -19.9        |
|    reward_value_loss         | 12.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.53         |
|    ep_rew_mean               | 6.5          |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 25           |
|    time_elapsed              | 2412         |
|    total_timesteps           | 3200         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0471       |
|    cost_value_loss           | 0.00572      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0163      |
|    learning_rate             | 0.0005       |
|    loss                      | 11.7         |
|    mean_cost_advantages      | 0.0056004925 |
|    mean_reward_advantages    | 0.29602358   |
|    n_updates                 | 240          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.54e-08    |
|    reward_explained_variance | -63.5        |
|    reward_value_loss         | 20.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.4          |
|    ep_rew_mean               | 6.37         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 26           |
|    time_elapsed              | 2506         |
|    total_timesteps           | 3328         |
| train/                       |              |
|    approx_kl                 | 0.0017342549 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00469      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0388       |
|    cost_value_loss           | 0.0037       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0141      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.62         |
|    mean_cost_advantages      | 0.0034056313 |
|    mean_reward_advantages    | -1.2650851   |
|    n_updates                 | 250          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.000529    |
|    reward_explained_variance | -30.2        |
|    reward_value_loss         | 9.42         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.29         |
|    ep_rew_mean               | 6.26         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 27           |
|    time_elapsed              | 2603         |
|    total_timesteps           | 3456         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0602      |
|    cost_value_loss           | 0.00479      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0106      |
|    learning_rate             | 0.0005       |
|    loss                      | 6.58         |
|    mean_cost_advantages      | -0.011596145 |
|    mean_reward_advantages    | 0.6843936    |
|    n_updates                 | 260          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.45e-07     |
|    reward_explained_variance | -29.3        |
|    reward_value_loss         | 12.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.67          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 28            |
|    time_elapsed              | 2699          |
|    total_timesteps           | 3584          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0234        |
|    cost_value_loss           | 0.00426       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0101       |
|    learning_rate             | 0.0005        |
|    loss                      | 8.58          |
|    mean_cost_advantages      | 0.00093237753 |
|    mean_reward_advantages    | 1.0114352     |
|    n_updates                 | 270           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.65e-08     |
|    reward_explained_variance | -45           |
|    reward_value_loss         | 17.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.85          |
|    ep_rew_mean               | 6.81          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 29            |
|    time_elapsed              | 2797          |
|    total_timesteps           | 3712          |
| train/                       |               |
|    approx_kl                 | 0.00010355981 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0713        |
|    cost_value_loss           | 0.00419       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00985      |
|    learning_rate             | 0.0005        |
|    loss                      | 9.96          |
|    mean_cost_advantages      | -0.005125432  |
|    mean_reward_advantages    | 1.0381608     |
|    n_updates                 | 280           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.12e-05     |
|    reward_explained_variance | -14.6         |
|    reward_value_loss         | 20.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.96          |
|    ep_rew_mean               | 6.92          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 30            |
|    time_elapsed              | 2897          |
|    total_timesteps           | 3840          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0538        |
|    cost_value_loss           | 0.00411       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00914      |
|    learning_rate             | 0.0005        |
|    loss                      | 7.05          |
|    mean_cost_advantages      | -0.0030294787 |
|    mean_reward_advantages    | -0.6988258    |
|    n_updates                 | 290           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.61e-07      |
|    reward_explained_variance | -28.6         |
|    reward_value_loss         | 14.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.98         |
|    ep_rew_mean               | 6.94         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 31           |
|    time_elapsed              | 2996         |
|    total_timesteps           | 3968         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0125       |
|    cost_value_loss           | 0.00318      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.009       |
|    learning_rate             | 0.0005       |
|    loss                      | 6.84         |
|    mean_cost_advantages      | -0.012686176 |
|    mean_reward_advantages    | -0.5940422   |
|    n_updates                 | 300          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.3e-09     |
|    reward_explained_variance | -95.4        |
|    reward_value_loss         | 15.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.86         |
|    ep_rew_mean               | 6.82         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 32           |
|    time_elapsed              | 3095         |
|    total_timesteps           | 4096         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0103       |
|    cost_value_loss           | 0.00318      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00899     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.14         |
|    mean_cost_advantages      | 0.0005826887 |
|    mean_reward_advantages    | 0.27486178   |
|    n_updates                 | 310          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.89e-09     |
|    reward_explained_variance | -53.4        |
|    reward_value_loss         | 13.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.69        |
|    ep_rew_mean               | 6.65        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 33          |
|    time_elapsed              | 3197        |
|    total_timesteps           | 4224        |
| train/                       |             |
|    approx_kl                 | 0.009208785 |
|    average_cost              | 0.0         |
|    clip_fraction             | 0.0102      |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0521      |
|    cost_value_loss           | 0.00255     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0126     |
|    learning_rate             | 0.0005      |
|    loss                      | 5.09        |
|    mean_cost_advantages      | 0.01060256  |
|    mean_reward_advantages    | -0.49282616 |
|    n_updates                 | 320         |
|    nu                        | 1.04        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -0.000682   |
|    reward_explained_variance | -23.2       |
|    reward_value_loss         | 11.5        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.14          |
|    ep_rew_mean               | 7.09          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 34            |
|    time_elapsed              | 3297          |
|    total_timesteps           | 4352          |
| train/                       |               |
|    approx_kl                 | 2.561137e-08  |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0841       |
|    cost_value_loss           | 0.00265       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0166       |
|    learning_rate             | 0.0005        |
|    loss                      | 6.65          |
|    mean_cost_advantages      | -0.0013305182 |
|    mean_reward_advantages    | 0.7117641     |
|    n_updates                 | 330           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.98e-08     |
|    reward_explained_variance | -11.2         |
|    reward_value_loss         | 13.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.5           |
|    ep_rew_mean               | 6.46          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 35            |
|    time_elapsed              | 3397          |
|    total_timesteps           | 4480          |
| train/                       |               |
|    approx_kl                 | 2.0954758e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0257       |
|    cost_value_loss           | 0.00277       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0171       |
|    learning_rate             | 0.0005        |
|    loss                      | 10            |
|    mean_cost_advantages      | -0.006308921  |
|    mean_reward_advantages    | 1.1363711     |
|    n_updates                 | 340           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.33e-07     |
|    reward_explained_variance | -21.6         |
|    reward_value_loss         | 19.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.57          |
|    ep_rew_mean               | 6.53          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 36            |
|    time_elapsed              | 3498          |
|    total_timesteps           | 4608          |
| train/                       |               |
|    approx_kl                 | 2.0023435e-08 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0272        |
|    cost_value_loss           | 0.00268       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0173       |
|    learning_rate             | 0.0005        |
|    loss                      | 8.15          |
|    mean_cost_advantages      | -0.009347668  |
|    mean_reward_advantages    | -1.6918716    |
|    n_updates                 | 350           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.5e-08      |
|    reward_explained_variance | -63.3         |
|    reward_value_loss         | 15.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.26          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 37            |
|    time_elapsed              | 3598          |
|    total_timesteps           | 4736          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0736       |
|    cost_value_loss           | 0.00299       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0173       |
|    learning_rate             | 0.0005        |
|    loss                      | 5.2           |
|    mean_cost_advantages      | 0.00023391045 |
|    mean_reward_advantages    | -0.29747042   |
|    n_updates                 | 360           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.88e-09     |
|    reward_explained_variance | -22.5         |
|    reward_value_loss         | 9.09          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.35         |
|    ep_rew_mean               | 6.3          |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 38           |
|    time_elapsed              | 3698         |
|    total_timesteps           | 4864         |
| train/                       |              |
|    approx_kl                 | 0.003701101  |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00625      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.123       |
|    cost_value_loss           | 0.00253      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0132      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.8          |
|    mean_cost_advantages      | -0.012427373 |
|    mean_reward_advantages    | -0.2558239   |
|    n_updates                 | 370          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.00115     |
|    reward_explained_variance | -9.97        |
|    reward_value_loss         | 9.3          |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.5         |
|    ep_rew_mean               | 6.46        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 39          |
|    time_elapsed              | 3812        |
|    total_timesteps           | 4992        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0159      |
|    cost_value_loss           | 0.00285     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00993    |
|    learning_rate             | 0.0005      |
|    loss                      | 6.48        |
|    mean_cost_advantages      | 0.009507036 |
|    mean_reward_advantages    | 0.08795801  |
|    n_updates                 | 380         |
|    nu                        | 1.04        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 2.14e-07    |
|    reward_explained_variance | -21.1       |
|    reward_value_loss         | 13.2        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.2          |
|    ep_rew_mean               | 6.17         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 40           |
|    time_elapsed              | 3924         |
|    total_timesteps           | 5120         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00126     |
|    cost_value_loss           | 0.0026       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00945     |
|    learning_rate             | 0.0005       |
|    loss                      | 10.7         |
|    mean_cost_advantages      | -0.010665974 |
|    mean_reward_advantages    | 1.228183     |
|    n_updates                 | 390          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.11e-08    |
|    reward_explained_variance | -28.5        |
|    reward_value_loss         | 18.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.11         |
|    ep_rew_mean               | 6.07         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 41           |
|    time_elapsed              | 4038         |
|    total_timesteps           | 5248         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0153       |
|    cost_value_loss           | 0.00257      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00947     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.84         |
|    mean_cost_advantages      | 0.0035925726 |
|    mean_reward_advantages    | -1.645762    |
|    n_updates                 | 400          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.8e-08     |
|    reward_explained_variance | -21          |
|    reward_value_loss         | 8.92         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.08          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 42            |
|    time_elapsed              | 4152          |
|    total_timesteps           | 5376          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0136       |
|    cost_value_loss           | 0.00179       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0094       |
|    learning_rate             | 0.0005        |
|    loss                      | 4.29          |
|    mean_cost_advantages      | -0.0012184677 |
|    mean_reward_advantages    | 0.11409646    |
|    n_updates                 | 410           |
|    nu                        | 1.04          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.32e-08     |
|    reward_explained_variance | -15.4         |
|    reward_value_loss         | 8.97          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.23         |
|    ep_rew_mean               | 6.19         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 43           |
|    time_elapsed              | 4265         |
|    total_timesteps           | 5504         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0929      |
|    cost_value_loss           | 0.00255      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00957     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.81         |
|    mean_cost_advantages      | 0.0030335598 |
|    mean_reward_advantages    | 0.046016023  |
|    n_updates                 | 420          |
|    nu                        | 1.04         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.38e-08    |
|    reward_explained_variance | -19          |
|    reward_value_loss         | 10           |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.86          |
|    ep_rew_mean               | 5.83          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 44            |
|    time_elapsed              | 4378          |
|    total_timesteps           | 5632          |
| train/                       |               |
|    approx_kl                 | 5.9711747e-06 |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0312        |
|    cost_value_loss           | 0.00206       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00964      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.37          |
|    mean_cost_advantages      | -0.0011568663 |
|    mean_reward_advantages    | 0.5502743     |
|    n_updates                 | 430           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.06e-06      |
|    reward_explained_variance | -12.4         |
|    reward_value_loss         | 10.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 45            |
|    time_elapsed              | 4488          |
|    total_timesteps           | 5760          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0337        |
|    cost_value_loss           | 0.00151       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00957      |
|    learning_rate             | 0.0005        |
|    loss                      | 7.33          |
|    mean_cost_advantages      | -0.0054741567 |
|    mean_reward_advantages    | -0.10461582   |
|    n_updates                 | 440           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.02e-08      |
|    reward_explained_variance | -21.9         |
|    reward_value_loss         | 14.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.16         |
|    ep_rew_mean               | 6.13         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 46           |
|    time_elapsed              | 4597         |
|    total_timesteps           | 5888         |
| train/                       |              |
|    approx_kl                 | 0.0007582796 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00313      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0553       |
|    cost_value_loss           | 0.00178      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0104      |
|    learning_rate             | 0.0005       |
|    loss                      | 4            |
|    mean_cost_advantages      | -0.010477924 |
|    mean_reward_advantages    | -0.057922244 |
|    n_updates                 | 450          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.77e-05    |
|    reward_explained_variance | -11.7        |
|    reward_value_loss         | 9.63         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.77        |
|    ep_rew_mean               | 6.73        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 47          |
|    time_elapsed              | 4709        |
|    total_timesteps           | 6016        |
| train/                       |             |
|    approx_kl                 | 0.03171666  |
|    average_cost              | 0.0390625   |
|    clip_fraction             | 0.00625     |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -131        |
|    cost_value_loss           | 0.199       |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00678    |
|    learning_rate             | 0.0005      |
|    loss                      | 4.56        |
|    mean_cost_advantages      | 0.046496402 |
|    mean_reward_advantages    | 0.11794005  |
|    n_updates                 | 460         |
|    nu                        | 1.05        |
|    nu_loss                   | -0.0408     |
|    policy_gradient_loss      | -0.0105     |
|    reward_explained_variance | -10.4       |
|    reward_value_loss         | 9.51        |
|    total_cost                | 5.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.53         |
|    ep_rew_mean               | 6.49         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 48           |
|    time_elapsed              | 4830         |
|    total_timesteps           | 6144         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0399      |
|    cost_value_loss           | 0.00153      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00246     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.33         |
|    mean_cost_advantages      | -0.010120286 |
|    mean_reward_advantages    | 1.1898422    |
|    n_updates                 | 470          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.02e-07    |
|    reward_explained_variance | -13.2        |
|    reward_value_loss         | 14.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.39         |
|    ep_rew_mean               | 6.36         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 49           |
|    time_elapsed              | 4943         |
|    total_timesteps           | 6272         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000725    |
|    cost_value_loss           | 0.00193      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00213     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.05         |
|    mean_cost_advantages      | -0.016899565 |
|    mean_reward_advantages    | -1.3837401   |
|    n_updates                 | 480          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.71e-08     |
|    reward_explained_variance | -8.53        |
|    reward_value_loss         | 11.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.9          |
|    ep_rew_mean               | 6.85         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 50           |
|    time_elapsed              | 5046         |
|    total_timesteps           | 6400         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0284      |
|    cost_value_loss           | 0.00186      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00212     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.13         |
|    mean_cost_advantages      | -0.012958199 |
|    mean_reward_advantages    | 0.0072733164 |
|    n_updates                 | 490          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.29e-09     |
|    reward_explained_variance | -14          |
|    reward_value_loss         | 10           |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.72         |
|    ep_rew_mean               | 6.68         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 51           |
|    time_elapsed              | 5168         |
|    total_timesteps           | 6528         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.011       |
|    cost_value_loss           | 0.00141      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00218     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.84         |
|    mean_cost_advantages      | -0.008357159 |
|    mean_reward_advantages    | 0.32213596   |
|    n_updates                 | 500          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.07e-09     |
|    reward_explained_variance | -11.7        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.41          |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 52            |
|    time_elapsed              | 5278          |
|    total_timesteps           | 6656          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0021        |
|    cost_value_loss           | 0.00111       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00207      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.76          |
|    mean_cost_advantages      | -0.0059941215 |
|    mean_reward_advantages    | -0.50633204   |
|    n_updates                 | 510           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.64e-10      |
|    reward_explained_variance | -14.7         |
|    reward_value_loss         | 9.09          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.33           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 53             |
|    time_elapsed              | 5381           |
|    total_timesteps           | 6784           |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0235         |
|    cost_value_loss           | 0.000972       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00214       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.82           |
|    mean_cost_advantages      | -0.00021775882 |
|    mean_reward_advantages    | -0.5282656     |
|    n_updates                 | 520            |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.93e-09      |
|    reward_explained_variance | -20.3          |
|    reward_value_loss         | 10.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.65         |
|    ep_rew_mean               | 6.61         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 54           |
|    time_elapsed              | 5481         |
|    total_timesteps           | 6912         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0111       |
|    cost_value_loss           | 0.000968     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0021      |
|    learning_rate             | 0.0005       |
|    loss                      | 5.01         |
|    mean_cost_advantages      | 0.0014021755 |
|    mean_reward_advantages    | 1.52289      |
|    n_updates                 | 530          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.25e-08    |
|    reward_explained_variance | -9.21        |
|    reward_value_loss         | 11.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.26          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 55            |
|    time_elapsed              | 5587          |
|    total_timesteps           | 7040          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0127        |
|    cost_value_loss           | 0.00124       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 7.19          |
|    mean_cost_advantages      | -0.0022214088 |
|    mean_reward_advantages    | 0.048352733   |
|    n_updates                 | 540           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.23e-10     |
|    reward_explained_variance | -20.4         |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.9         |
|    ep_rew_mean               | 5.86        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 56          |
|    time_elapsed              | 5695        |
|    total_timesteps           | 7168        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0185      |
|    cost_value_loss           | 0.00128     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00213    |
|    learning_rate             | 0.0005      |
|    loss                      | 4.5         |
|    mean_cost_advantages      | 0.004313167 |
|    mean_reward_advantages    | -0.580818   |
|    n_updates                 | 550         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -2.44e-10   |
|    reward_explained_variance | -8.3        |
|    reward_value_loss         | 12.2        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.81         |
|    ep_rew_mean               | 5.77         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 57           |
|    time_elapsed              | 5803         |
|    total_timesteps           | 7296         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000295    |
|    cost_value_loss           | 0.000966     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00214     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.89         |
|    mean_cost_advantages      | 0.0025993625 |
|    mean_reward_advantages    | -0.1940099   |
|    n_updates                 | 560          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.26e-09    |
|    reward_explained_variance | -14.3        |
|    reward_value_loss         | 8.91         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.96         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 58           |
|    time_elapsed              | 5912         |
|    total_timesteps           | 7424         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0655      |
|    cost_value_loss           | 0.00109      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00208     |
|    learning_rate             | 0.0005       |
|    loss                      | 8.49         |
|    mean_cost_advantages      | 0.0034392476 |
|    mean_reward_advantages    | 1.1555156    |
|    n_updates                 | 570          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.45e-09     |
|    reward_explained_variance | -35          |
|    reward_value_loss         | 19.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.16          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 59            |
|    time_elapsed              | 6020          |
|    total_timesteps           | 7552          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0179        |
|    cost_value_loss           | 0.000838      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00214      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.77          |
|    mean_cost_advantages      | -0.0036813212 |
|    mean_reward_advantages    | -0.65340537   |
|    n_updates                 | 580           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.74e-09      |
|    reward_explained_variance | -10.9         |
|    reward_value_loss         | 9.65          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.3          |
|    ep_rew_mean               | 6.26         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 60           |
|    time_elapsed              | 6125         |
|    total_timesteps           | 7680         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.107       |
|    cost_value_loss           | 0.00102      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00205     |
|    learning_rate             | 0.0005       |
|    loss                      | 10           |
|    mean_cost_advantages      | 0.0013793412 |
|    mean_reward_advantages    | 2.4036257    |
|    n_updates                 | 590          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.8e-09     |
|    reward_explained_variance | -16.7        |
|    reward_value_loss         | 20.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.49          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 61            |
|    time_elapsed              | 6230          |
|    total_timesteps           | 7808          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00781      |
|    cost_value_loss           | 0.000804      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00213      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.64          |
|    mean_cost_advantages      | -0.0023746993 |
|    mean_reward_advantages    | -3.0319622    |
|    n_updates                 | 600           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.82e-09     |
|    reward_explained_variance | -17           |
|    reward_value_loss         | 15            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.76          |
|    ep_rew_mean               | 6.72          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 62            |
|    time_elapsed              | 6335          |
|    total_timesteps           | 7936          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.085         |
|    cost_value_loss           | 0.000674      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00212      |
|    learning_rate             | 0.0005        |
|    loss                      | 7.36          |
|    mean_cost_advantages      | -0.0031625086 |
|    mean_reward_advantages    | 1.2663566     |
|    n_updates                 | 610           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.57e-09     |
|    reward_explained_variance | -15           |
|    reward_value_loss         | 14.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.69          |
|    ep_rew_mean               | 6.65          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 63            |
|    time_elapsed              | 6445          |
|    total_timesteps           | 8064          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0887       |
|    cost_value_loss           | 0.000835      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00212      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.57          |
|    mean_cost_advantages      | -0.0038313605 |
|    mean_reward_advantages    | -0.6871877    |
|    n_updates                 | 620           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.49e-10      |
|    reward_explained_variance | -14.9         |
|    reward_value_loss         | 9.7           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.69           |
|    ep_rew_mean               | 6.65           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 64             |
|    time_elapsed              | 6556           |
|    total_timesteps           | 8192           |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00245        |
|    cost_value_loss           | 0.000886       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00211       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.82           |
|    mean_cost_advantages      | -0.00081173424 |
|    mean_reward_advantages    | -0.3995506     |
|    n_updates                 | 630            |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.19e-09       |
|    reward_explained_variance | -14.4          |
|    reward_value_loss         | 10.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.18          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 65            |
|    time_elapsed              | 6670          |
|    total_timesteps           | 8320          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0237       |
|    cost_value_loss           | 0.000662      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.9           |
|    mean_cost_advantages      | -0.0024933273 |
|    mean_reward_advantages    | -0.07909639   |
|    n_updates                 | 640           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.45e-09     |
|    reward_explained_variance | -8.39         |
|    reward_value_loss         | 9.42          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.78          |
|    ep_rew_mean               | 5.75          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 66            |
|    time_elapsed              | 6781          |
|    total_timesteps           | 8448          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0436        |
|    cost_value_loss           | 0.000661      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00214      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.54          |
|    mean_cost_advantages      | 0.00047953054 |
|    mean_reward_advantages    | 0.40325332    |
|    n_updates                 | 650           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.78e-09     |
|    reward_explained_variance | -14           |
|    reward_value_loss         | 12.2          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.81        |
|    ep_rew_mean               | 5.78        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 67          |
|    time_elapsed              | 6888        |
|    total_timesteps           | 8576        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0061      |
|    cost_value_loss           | 0.000637    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00208    |
|    learning_rate             | 0.0005      |
|    loss                      | 4.47        |
|    mean_cost_advantages      | 0.004099989 |
|    mean_reward_advantages    | 0.31330135  |
|    n_updates                 | 660         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.12e-09   |
|    reward_explained_variance | -6.34       |
|    reward_value_loss         | 8.92        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.81         |
|    ep_rew_mean               | 5.79         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 68           |
|    time_elapsed              | 6995         |
|    total_timesteps           | 8704         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0521       |
|    cost_value_loss           | 0.000536     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00209     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.18         |
|    mean_cost_advantages      | 0.0021822345 |
|    mean_reward_advantages    | 0.61516356   |
|    n_updates                 | 670          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.89e-09    |
|    reward_explained_variance | -9.88        |
|    reward_value_loss         | 13.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 69            |
|    time_elapsed              | 7099          |
|    total_timesteps           | 8832          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0836        |
|    cost_value_loss           | 0.000466      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00209      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.12          |
|    mean_cost_advantages      | 0.00064761133 |
|    mean_reward_advantages    | -0.5438148    |
|    n_updates                 | 680           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.47e-09      |
|    reward_explained_variance | -14.9         |
|    reward_value_loss         | 8.59          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.08         |
|    ep_rew_mean               | 6.06         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 70           |
|    time_elapsed              | 7196         |
|    total_timesteps           | 8960         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0422       |
|    cost_value_loss           | 0.000628     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00213     |
|    learning_rate             | 0.0005       |
|    loss                      | 11           |
|    mean_cost_advantages      | -0.005678706 |
|    mean_reward_advantages    | 1.401267     |
|    n_updates                 | 690          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.67e-08    |
|    reward_explained_variance | -31.3        |
|    reward_value_loss         | 23.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.29          |
|    ep_rew_mean               | 6.27          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 71            |
|    time_elapsed              | 7296          |
|    total_timesteps           | 9088          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0157        |
|    cost_value_loss           | 0.00046       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00206      |
|    learning_rate             | 0.0005        |
|    loss                      | 8.9           |
|    mean_cost_advantages      | 0.00070669217 |
|    mean_reward_advantages    | -0.3152702    |
|    n_updates                 | 700           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.59e-09      |
|    reward_explained_variance | -35.8         |
|    reward_value_loss         | 18.1          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.58        |
|    ep_rew_mean               | 6.55        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 72          |
|    time_elapsed              | 7393        |
|    total_timesteps           | 9216        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0212     |
|    cost_value_loss           | 0.000451    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0021     |
|    learning_rate             | 0.0005      |
|    loss                      | 5.15        |
|    mean_cost_advantages      | 0.002348217 |
|    mean_reward_advantages    | -0.36923897 |
|    n_updates                 | 710         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -5.28e-09   |
|    reward_explained_variance | -13.7       |
|    reward_value_loss         | 13.8        |
|    total_cost                | 0.0         |
----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.69        |
|    ep_rew_mean               | 6.66        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 73          |
|    time_elapsed              | 7491        |
|    total_timesteps           | 9344        |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0239      |
|    cost_value_loss           | 0.000446    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0021     |
|    learning_rate             | 0.0005      |
|    loss                      | 10.3        |
|    mean_cost_advantages      | 0.003524549 |
|    mean_reward_advantages    | 0.21559265  |
|    n_updates                 | 720         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 4.53e-09    |
|    reward_explained_variance | -10.7       |
|    reward_value_loss         | 20.8        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.96          |
|    ep_rew_mean               | 6.92          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 74            |
|    time_elapsed              | 7588          |
|    total_timesteps           | 9472          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0815        |
|    cost_value_loss           | 0.000572      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 10.8          |
|    mean_cost_advantages      | -0.0029376221 |
|    mean_reward_advantages    | 0.447226      |
|    n_updates                 | 730           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.05e-09      |
|    reward_explained_variance | -34.6         |
|    reward_value_loss         | 19.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.73          |
|    ep_rew_mean               | 6.69          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 75            |
|    time_elapsed              | 7684          |
|    total_timesteps           | 9600          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.154        |
|    cost_value_loss           | 0.000603      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 9.41          |
|    mean_cost_advantages      | -0.0029735658 |
|    mean_reward_advantages    | 1.1711981     |
|    n_updates                 | 740           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.41e-10     |
|    reward_explained_variance | -5.77         |
|    reward_value_loss         | 19.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.12          |
|    ep_rew_mean               | 7.08          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 76            |
|    time_elapsed              | 7781          |
|    total_timesteps           | 9728          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0183       |
|    cost_value_loss           | 0.000452      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00213      |
|    learning_rate             | 0.0005        |
|    loss                      | 8.46          |
|    mean_cost_advantages      | -0.0019843031 |
|    mean_reward_advantages    | -0.96294767   |
|    n_updates                 | 750           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.68e-09      |
|    reward_explained_variance | -4.92         |
|    reward_value_loss         | 20.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.87          |
|    ep_rew_mean               | 6.84          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 77            |
|    time_elapsed              | 7877          |
|    total_timesteps           | 9856          |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00388      |
|    cost_value_loss           | 0.000517      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00208      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.07          |
|    mean_cost_advantages      | -0.0029219845 |
|    mean_reward_advantages    | 0.14811605    |
|    n_updates                 | 760           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.4e-10       |
|    reward_explained_variance | -10.8         |
|    reward_value_loss         | 12.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.08         |
|    ep_rew_mean               | 7.04         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 78           |
|    time_elapsed              | 7974         |
|    total_timesteps           | 9984         |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0437       |
|    cost_value_loss           | 0.00034      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00213     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.01         |
|    mean_cost_advantages      | 0.0030721442 |
|    mean_reward_advantages    | -0.99386823  |
|    n_updates                 | 770          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.95e-09     |
|    reward_explained_variance | -19.4        |
|    reward_value_loss         | 11.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.87          |
|    ep_rew_mean               | 6.84          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 79            |
|    time_elapsed              | 8072          |
|    total_timesteps           | 10112         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0361        |
|    cost_value_loss           | 0.000433      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00212      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.44          |
|    mean_cost_advantages      | -0.0005120785 |
|    mean_reward_advantages    | 0.26753497    |
|    n_updates                 | 780           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.65e-11      |
|    reward_explained_variance | -5.97         |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.43           |
|    ep_rew_mean               | 6.4            |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 80             |
|    time_elapsed              | 8166           |
|    total_timesteps           | 10240          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0213        |
|    cost_value_loss           | 0.000434       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00207       |
|    learning_rate             | 0.0005         |
|    loss                      | 11.2           |
|    mean_cost_advantages      | -0.00043157671 |
|    mean_reward_advantages    | 2.196317       |
|    n_updates                 | 790            |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.65e-09       |
|    reward_explained_variance | -21.4          |
|    reward_value_loss         | 25.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.75         |
|    ep_rew_mean               | 6.72         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 81           |
|    time_elapsed              | 8265         |
|    total_timesteps           | 10368        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.015        |
|    cost_value_loss           | 0.000289     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00208     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.03         |
|    mean_cost_advantages      | 0.0034624022 |
|    mean_reward_advantages    | -2.8421016   |
|    n_updates                 | 800          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.14e-09     |
|    reward_explained_variance | -7.7         |
|    reward_value_loss         | 17.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.36          |
|    ep_rew_mean               | 6.32          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 82            |
|    time_elapsed              | 8368          |
|    total_timesteps           | 10496         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.012         |
|    cost_value_loss           | 0.000356      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 8.06          |
|    mean_cost_advantages      | 0.00091465353 |
|    mean_reward_advantages    | 1.8582085     |
|    n_updates                 | 810           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.72e-09      |
|    reward_explained_variance | -6.01         |
|    reward_value_loss         | 16.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.58          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 83            |
|    time_elapsed              | 8474          |
|    total_timesteps           | 10624         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0312        |
|    cost_value_loss           | 0.000291      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00213      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.82          |
|    mean_cost_advantages      | -0.0031618315 |
|    mean_reward_advantages    | -1.34551      |
|    n_updates                 | 820           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.38e-09      |
|    reward_explained_variance | -3.76         |
|    reward_value_loss         | 12.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.49         |
|    ep_rew_mean               | 6.45         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 84           |
|    time_elapsed              | 8583         |
|    total_timesteps           | 10752        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0276      |
|    cost_value_loss           | 0.000422     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0021      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.13         |
|    mean_cost_advantages      | 0.0026828838 |
|    mean_reward_advantages    | 0.15438417   |
|    n_updates                 | 830          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.43e-09     |
|    reward_explained_variance | -7.11        |
|    reward_value_loss         | 8.97         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.15         |
|    ep_rew_mean               | 6.11         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 85           |
|    time_elapsed              | 8691         |
|    total_timesteps           | 10880        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.101       |
|    cost_value_loss           | 0.000508     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00214     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.09         |
|    mean_cost_advantages      | 0.004138154  |
|    mean_reward_advantages    | -0.053417012 |
|    n_updates                 | 840          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.04e-09    |
|    reward_explained_variance | -3.2         |
|    reward_value_loss         | 7.32         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.5           |
|    ep_rew_mean               | 6.45          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 86            |
|    time_elapsed              | 8793          |
|    total_timesteps           | 11008         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00396      |
|    cost_value_loss           | 0.000322      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 6.95          |
|    mean_cost_advantages      | -0.0037256214 |
|    mean_reward_advantages    | 0.77188563    |
|    n_updates                 | 850           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.69e-11      |
|    reward_explained_variance | -22.1         |
|    reward_value_loss         | 13.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.42          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 87            |
|    time_elapsed              | 8890          |
|    total_timesteps           | 11136         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0168       |
|    cost_value_loss           | 0.000265      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00212      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.17          |
|    mean_cost_advantages      | 0.00039881747 |
|    mean_reward_advantages    | 0.0076069087  |
|    n_updates                 | 860           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.97e-09     |
|    reward_explained_variance | -4.8          |
|    reward_value_loss         | 9.08          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.95         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 88           |
|    time_elapsed              | 8989         |
|    total_timesteps           | 11264        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0204       |
|    cost_value_loss           | 0.000339     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00216     |
|    learning_rate             | 0.0005       |
|    loss                      | 7.76         |
|    mean_cost_advantages      | -0.005709866 |
|    mean_reward_advantages    | 0.94239694   |
|    n_updates                 | 870          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.15e-08    |
|    reward_explained_variance | -18.1        |
|    reward_value_loss         | 15.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.15         |
|    ep_rew_mean               | 6.11         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 89           |
|    time_elapsed              | 9092         |
|    total_timesteps           | 11392        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0955       |
|    cost_value_loss           | 0.000278     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00209     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.62         |
|    mean_cost_advantages      | 0.0007748962 |
|    mean_reward_advantages    | -0.044845745 |
|    n_updates                 | 880          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.41e-10    |
|    reward_explained_variance | -26.6        |
|    reward_value_loss         | 14.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 90            |
|    time_elapsed              | 9198          |
|    total_timesteps           | 11520         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0574       |
|    cost_value_loss           | 0.000297      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00208      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.75          |
|    mean_cost_advantages      | 0.00040831894 |
|    mean_reward_advantages    | 0.3889687     |
|    n_updates                 | 890           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.61e-09     |
|    reward_explained_variance | -3.59         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.86         |
|    ep_rew_mean               | 5.84         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 91           |
|    time_elapsed              | 9302         |
|    total_timesteps           | 11648        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0171       |
|    cost_value_loss           | 0.000323     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00213     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.7          |
|    mean_cost_advantages      | 0.0009654976 |
|    mean_reward_advantages    | -0.87654006  |
|    n_updates                 | 900          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.05e-09     |
|    reward_explained_variance | -4.35        |
|    reward_value_loss         | 13.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.82        |
|    ep_rew_mean               | 5.8         |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 92          |
|    time_elapsed              | 9403        |
|    total_timesteps           | 11776       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0398      |
|    cost_value_loss           | 0.0002      |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00209    |
|    learning_rate             | 0.0005      |
|    loss                      | 5.45        |
|    mean_cost_advantages      | 0.001664855 |
|    mean_reward_advantages    | 0.07168538  |
|    n_updates                 | 910         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -3.94e-09   |
|    reward_explained_variance | -7.88       |
|    reward_value_loss         | 10.8        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.04           |
|    ep_rew_mean               | 6.02           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 93             |
|    time_elapsed              | 9503           |
|    total_timesteps           | 11904          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0284         |
|    cost_value_loss           | 0.000223       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00209       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.06           |
|    mean_cost_advantages      | -0.00014255289 |
|    mean_reward_advantages    | 0.6636748      |
|    n_updates                 | 920            |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.37e-10      |
|    reward_explained_variance | -2.6           |
|    reward_value_loss         | 14.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.03          |
|    ep_rew_mean               | 6.01          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 94            |
|    time_elapsed              | 9599          |
|    total_timesteps           | 12032         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0543       |
|    cost_value_loss           | 0.000236      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00207      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.63          |
|    mean_cost_advantages      | 0.00038496812 |
|    mean_reward_advantages    | -0.8736045    |
|    n_updates                 | 930           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.91e-09     |
|    reward_explained_variance | -3.87         |
|    reward_value_loss         | 11.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.95          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 95            |
|    time_elapsed              | 9698          |
|    total_timesteps           | 12160         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0132        |
|    cost_value_loss           | 0.000255      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00215      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.75          |
|    mean_cost_advantages      | 0.00032087837 |
|    mean_reward_advantages    | 1.6137131     |
|    n_updates                 | 940           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.87e-10      |
|    reward_explained_variance | -4.17         |
|    reward_value_loss         | 13.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.36          |
|    ep_rew_mean               | 6.33          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 96            |
|    time_elapsed              | 9796          |
|    total_timesteps           | 12288         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0408        |
|    cost_value_loss           | 0.000233      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00215      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.85          |
|    mean_cost_advantages      | -0.0018726737 |
|    mean_reward_advantages    | -0.93698025   |
|    n_updates                 | 950           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.74e-09      |
|    reward_explained_variance | -4.61         |
|    reward_value_loss         | 7.49          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.39         |
|    ep_rew_mean               | 6.36         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 97           |
|    time_elapsed              | 9895         |
|    total_timesteps           | 12416        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00135     |
|    cost_value_loss           | 0.00023      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00206     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.2          |
|    mean_cost_advantages      | 0.0012856076 |
|    mean_reward_advantages    | 0.0659548    |
|    n_updates                 | 960          |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.59e-09    |
|    reward_explained_variance | -7.02        |
|    reward_value_loss         | 9.17         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.57           |
|    ep_rew_mean               | 6.53           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 98             |
|    time_elapsed              | 9996           |
|    total_timesteps           | 12544          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00523       |
|    cost_value_loss           | 0.000225       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.0021        |
|    learning_rate             | 0.0005         |
|    loss                      | 3.22           |
|    mean_cost_advantages      | -0.00022759731 |
|    mean_reward_advantages    | 0.20791356     |
|    n_updates                 | 970            |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.83e-09      |
|    reward_explained_variance | -4.1           |
|    reward_value_loss         | 7.66           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.67          |
|    ep_rew_mean               | 6.63          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 99            |
|    time_elapsed              | 10103         |
|    total_timesteps           | 12672         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00567      |
|    cost_value_loss           | 0.000163      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00213      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.1           |
|    mean_cost_advantages      | -0.0018589275 |
|    mean_reward_advantages    | 0.59161866    |
|    n_updates                 | 980           |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.18e-09      |
|    reward_explained_variance | -5.81         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.48        |
|    ep_rew_mean               | 6.45        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 100         |
|    time_elapsed              | 10214       |
|    total_timesteps           | 12800       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0453      |
|    cost_value_loss           | 0.000293    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00211    |
|    learning_rate             | 0.0005      |
|    loss                      | 9.41        |
|    mean_cost_advantages      | 0.001722412 |
|    mean_reward_advantages    | 0.7915418   |
|    n_updates                 | 990         |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.33e-09   |
|    reward_explained_variance | -7.88       |
|    reward_value_loss         | 16.1        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.45         |
|    ep_rew_mean               | 6.42         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 101          |
|    time_elapsed              | 10312        |
|    total_timesteps           | 12928        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0259      |
|    cost_value_loss           | 0.000194     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00211     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.91         |
|    mean_cost_advantages      | 0.0013654834 |
|    mean_reward_advantages    | -1.5571167   |
|    n_updates                 | 1000         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.18e-09    |
|    reward_explained_variance | -8.9         |
|    reward_value_loss         | 11.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.1           |
|    ep_rew_mean               | 6.07          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 102           |
|    time_elapsed              | 10408         |
|    total_timesteps           | 13056         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.015        |
|    cost_value_loss           | 0.000227      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 4.29          |
|    mean_cost_advantages      | -0.0017052281 |
|    mean_reward_advantages    | -0.05154112   |
|    n_updates                 | 1010          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.95e-09      |
|    reward_explained_variance | -4.88         |
|    reward_value_loss         | 13.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.9          |
|    ep_rew_mean               | 5.87         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 103          |
|    time_elapsed              | 10503        |
|    total_timesteps           | 13184        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0579       |
|    cost_value_loss           | 0.000158     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00213     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.55         |
|    mean_cost_advantages      | 0.0011515426 |
|    mean_reward_advantages    | -0.7574512   |
|    n_updates                 | 1020         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.03e-09     |
|    reward_explained_variance | -4.67        |
|    reward_value_loss         | 7.1          |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.68         |
|    ep_rew_mean               | 5.65         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 104          |
|    time_elapsed              | 10600        |
|    total_timesteps           | 13312        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.139       |
|    cost_value_loss           | 0.000212     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0021      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.43         |
|    mean_cost_advantages      | 0.0023369216 |
|    mean_reward_advantages    | 0.7777018    |
|    n_updates                 | 1030         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.47e-09    |
|    reward_explained_variance | -7.84        |
|    reward_value_loss         | 9.61         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 105           |
|    time_elapsed              | 10695         |
|    total_timesteps           | 13440         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0192       |
|    cost_value_loss           | 0.000212      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.54          |
|    mean_cost_advantages      | -0.0042514876 |
|    mean_reward_advantages    | 0.5924598     |
|    n_updates                 | 1040          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.42e-09      |
|    reward_explained_variance | -6.21         |
|    reward_value_loss         | 12.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.67           |
|    ep_rew_mean               | 5.64           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 106            |
|    time_elapsed              | 10790          |
|    total_timesteps           | 13568          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0173        |
|    cost_value_loss           | 0.000235       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00209       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.74           |
|    mean_cost_advantages      | -0.00054249284 |
|    mean_reward_advantages    | 0.5250262      |
|    n_updates                 | 1050           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.85e-09       |
|    reward_explained_variance | -7.04          |
|    reward_value_loss         | 13.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.59         |
|    ep_rew_mean               | 5.55         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 107          |
|    time_elapsed              | 10886        |
|    total_timesteps           | 13696        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0101       |
|    cost_value_loss           | 0.000172     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00215     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.49         |
|    mean_cost_advantages      | 0.0025793645 |
|    mean_reward_advantages    | -1.5708534   |
|    n_updates                 | 1060         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.94e-09    |
|    reward_explained_variance | -7.47        |
|    reward_value_loss         | 10.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.58          |
|    ep_rew_mean               | 5.54          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 108           |
|    time_elapsed              | 10980         |
|    total_timesteps           | 13824         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00136       |
|    cost_value_loss           | 0.000152      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00215      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.62          |
|    mean_cost_advantages      | -0.0020496747 |
|    mean_reward_advantages    | -0.51663      |
|    n_updates                 | 1070          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.71e-09     |
|    reward_explained_variance | -2.3          |
|    reward_value_loss         | 4.2           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.2          |
|    ep_rew_mean               | 5.17         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 109          |
|    time_elapsed              | 11077        |
|    total_timesteps           | 13952        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0285       |
|    cost_value_loss           | 0.000193     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00212     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.18         |
|    mean_cost_advantages      | 0.0022887194 |
|    mean_reward_advantages    | 0.7863307    |
|    n_updates                 | 1080         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.84e-10    |
|    reward_explained_variance | -1.38        |
|    reward_value_loss         | 5.2          |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.07         |
|    ep_rew_mean               | 5.04         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 110          |
|    time_elapsed              | 11171        |
|    total_timesteps           | 14080        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00443     |
|    cost_value_loss           | 0.000207     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00211     |
|    learning_rate             | 0.0005       |
|    loss                      | 7.97         |
|    mean_cost_advantages      | 0.0009998208 |
|    mean_reward_advantages    | 1.3995426    |
|    n_updates                 | 1090         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.14e-09    |
|    reward_explained_variance | -9.37        |
|    reward_value_loss         | 14.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.12          |
|    ep_rew_mean               | 5.1           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 111           |
|    time_elapsed              | 11277         |
|    total_timesteps           | 14208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0226       |
|    cost_value_loss           | 0.000185      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00208      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.48          |
|    mean_cost_advantages      | -0.0022947143 |
|    mean_reward_advantages    | 0.36548388    |
|    n_updates                 | 1100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.16e-09     |
|    reward_explained_variance | -10.4         |
|    reward_value_loss         | 14.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.49          |
|    ep_rew_mean               | 5.47          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 112           |
|    time_elapsed              | 11384         |
|    total_timesteps           | 14336         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0727        |
|    cost_value_loss           | 0.000141      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00215      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.94          |
|    mean_cost_advantages      | 0.00094275456 |
|    mean_reward_advantages    | -1.3775952    |
|    n_updates                 | 1110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.8e-09      |
|    reward_explained_variance | -7.64         |
|    reward_value_loss         | 6.99          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.94          |
|    ep_rew_mean               | 5.92          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 113           |
|    time_elapsed              | 11492         |
|    total_timesteps           | 14464         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.135         |
|    cost_value_loss           | 0.000273      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 6.51          |
|    mean_cost_advantages      | -0.0020189318 |
|    mean_reward_advantages    | 2.007903      |
|    n_updates                 | 1120          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.77e-09      |
|    reward_explained_variance | -3.47         |
|    reward_value_loss         | 14.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.6          |
|    ep_rew_mean               | 5.58         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 114          |
|    time_elapsed              | 11598        |
|    total_timesteps           | 14592        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0163       |
|    cost_value_loss           | 0.000228     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00209     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.44         |
|    mean_cost_advantages      | -0.000604788 |
|    mean_reward_advantages    | 0.27084973   |
|    n_updates                 | 1130         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.86e-09    |
|    reward_explained_variance | -4.88        |
|    reward_value_loss         | 10.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.91         |
|    ep_rew_mean               | 5.89         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 115          |
|    time_elapsed              | 11699        |
|    total_timesteps           | 14720        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0209      |
|    cost_value_loss           | 0.000148     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00212     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.4          |
|    mean_cost_advantages      | 0.0023559262 |
|    mean_reward_advantages    | -1.6624928   |
|    n_updates                 | 1140         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.6e-09      |
|    reward_explained_variance | -4.82        |
|    reward_value_loss         | 7.86         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.93           |
|    ep_rew_mean               | 5.91           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 116            |
|    time_elapsed              | 11804          |
|    total_timesteps           | 14848          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0103         |
|    cost_value_loss           | 0.000129       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00209       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.86           |
|    mean_cost_advantages      | -0.00092866394 |
|    mean_reward_advantages    | 0.581211       |
|    n_updates                 | 1150           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.58e-09      |
|    reward_explained_variance | -2.81          |
|    reward_value_loss         | 12.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.27          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 117           |
|    time_elapsed              | 11913         |
|    total_timesteps           | 14976         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0149       |
|    cost_value_loss           | 0.000224      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00212      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.66          |
|    mean_cost_advantages      | 0.00040328025 |
|    mean_reward_advantages    | -0.06647222   |
|    n_updates                 | 1160          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.93e-09     |
|    reward_explained_variance | -3.53         |
|    reward_value_loss         | 9.57          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.72          |
|    ep_rew_mean               | 5.69          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 118           |
|    time_elapsed              | 12021         |
|    total_timesteps           | 15104         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0177        |
|    cost_value_loss           | 0.000121      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 2.47          |
|    mean_cost_advantages      | 4.6372938e-05 |
|    mean_reward_advantages    | -0.016933367  |
|    n_updates                 | 1170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.99e-09      |
|    reward_explained_variance | -3.6          |
|    reward_value_loss         | 6.56          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.88           |
|    ep_rew_mean               | 5.85           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 119            |
|    time_elapsed              | 12120          |
|    total_timesteps           | 15232          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0286         |
|    cost_value_loss           | 0.00016        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00213       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.53           |
|    mean_cost_advantages      | -0.00053196313 |
|    mean_reward_advantages    | 0.47971523     |
|    n_updates                 | 1180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.01e-10      |
|    reward_explained_variance | -5.21          |
|    reward_value_loss         | 10.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.22        |
|    ep_rew_mean               | 6.19        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 120         |
|    time_elapsed              | 12223       |
|    total_timesteps           | 15360       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.00527     |
|    cost_value_loss           | 0.000157    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00211    |
|    learning_rate             | 0.0005      |
|    loss                      | 3.13        |
|    mean_cost_advantages      | 0.002126669 |
|    mean_reward_advantages    | -0.27083787 |
|    n_updates                 | 1190        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 1.33e-09    |
|    reward_explained_variance | -1.14       |
|    reward_value_loss         | 5.3         |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.27          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 121           |
|    time_elapsed              | 12319         |
|    total_timesteps           | 15488         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0284       |
|    cost_value_loss           | 0.000216      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00207      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.42          |
|    mean_cost_advantages      | -0.0025889794 |
|    mean_reward_advantages    | 0.55297387    |
|    n_updates                 | 1200          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.18e-09     |
|    reward_explained_variance | -3.39         |
|    reward_value_loss         | 11.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.27          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 122           |
|    time_elapsed              | 12425         |
|    total_timesteps           | 15616         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0582       |
|    cost_value_loss           | 0.000172      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00211      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.09          |
|    mean_cost_advantages      | -0.0016032184 |
|    mean_reward_advantages    | -0.5271401    |
|    n_updates                 | 1210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.83e-09      |
|    reward_explained_variance | -5.52         |
|    reward_value_loss         | 8.02          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.43        |
|    ep_rew_mean               | 6.4         |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 123         |
|    time_elapsed              | 12533       |
|    total_timesteps           | 15744       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0127     |
|    cost_value_loss           | 0.00013     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.0021     |
|    learning_rate             | 0.0005      |
|    loss                      | 5.04        |
|    mean_cost_advantages      | 0.001419141 |
|    mean_reward_advantages    | 0.7260951   |
|    n_updates                 | 1220        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -2.79e-09   |
|    reward_explained_variance | -4.31       |
|    reward_value_loss         | 10          |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.79          |
|    ep_rew_mean               | 5.77          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 124           |
|    time_elapsed              | 12646         |
|    total_timesteps           | 15872         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0509        |
|    cost_value_loss           | 0.000204      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00209      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.5           |
|    mean_cost_advantages      | 0.00039717345 |
|    mean_reward_advantages    | 0.43174425    |
|    n_updates                 | 1230          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.47e-10      |
|    reward_explained_variance | -4.05         |
|    reward_value_loss         | 12.2          |
|    total_cost                | 0.0           |
------------------------------------------------
---------------------------------------------
| rollout/                     |            |
|    ep_len_mean               | 6.75       |
|    ep_rew_mean               | 5.72       |
| time/                        |            |
|    fps                       | 1          |
|    iterations                | 125        |
|    time_elapsed              | 12750      |
|    total_timesteps           | 16000      |
| train/                       |            |
|    approx_kl                 | 0.0        |
|    average_cost              | 0.0        |
|    clip_fraction             | 0          |
|    clip_range                | 0.2        |
|    cost_explained_variance   | 0.0329     |
|    cost_value_loss           | 0.000126   |
|    early_stop_epoch          | 10         |
|    entropy_loss              | -0.00211   |
|    learning_rate             | 0.0005     |
|    loss                      | 3.09       |
|    mean_cost_advantages      | 0.00061121 |
|    mean_reward_advantages    | -1.1617119 |
|    n_updates                 | 1240       |
|    nu                        | 1.05       |
|    nu_loss                   | -0         |
|    policy_gradient_loss      | -9.91e-09  |
|    reward_explained_variance | -1.82      |
|    reward_value_loss         | 6.57       |
|    total_cost                | 0.0        |
---------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.97          |
|    ep_rew_mean               | 5.94          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 126           |
|    time_elapsed              | 12856         |
|    total_timesteps           | 16128         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0699       |
|    cost_value_loss           | 0.000203      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0021       |
|    learning_rate             | 0.0005        |
|    loss                      | 7.28          |
|    mean_cost_advantages      | -0.0023844847 |
|    mean_reward_advantages    | 1.777142      |
|    n_updates                 | 1250          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.24e-09      |
|    reward_explained_variance | -8.9          |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.52           |
|    ep_rew_mean               | 5.5            |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 127            |
|    time_elapsed              | 12960          |
|    total_timesteps           | 16256          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0171         |
|    cost_value_loss           | 0.000143       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00211       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.14           |
|    mean_cost_advantages      | -0.00087942986 |
|    mean_reward_advantages    | -0.2810814     |
|    n_updates                 | 1260           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.68e-09       |
|    reward_explained_variance | -2.67          |
|    reward_value_loss         | 10.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.8         |
|    ep_rew_mean               | 5.77        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 128         |
|    time_elapsed              | 13063       |
|    total_timesteps           | 16384       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0915      |
|    cost_value_loss           | 0.000211    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00211    |
|    learning_rate             | 0.0005      |
|    loss                      | 8           |
|    mean_cost_advantages      | 0.001646786 |
|    mean_reward_advantages    | -0.5680131  |
|    n_updates                 | 1270        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 1.02e-09    |
|    reward_explained_variance | -7.38       |
|    reward_value_loss         | 15.5        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 129           |
|    time_elapsed              | 13164         |
|    total_timesteps           | 16512         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0441        |
|    cost_value_loss           | 0.000152      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00214      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.46          |
|    mean_cost_advantages      | -0.0004510489 |
|    mean_reward_advantages    | 1.3906901     |
|    n_updates                 | 1280          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.74e-09     |
|    reward_explained_variance | -9.01         |
|    reward_value_loss         | 15            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.43          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 130           |
|    time_elapsed              | 13268         |
|    total_timesteps           | 16640         |
| train/                       |               |
|    approx_kl                 | 0.0021653865  |
|    average_cost              | 0.0           |
|    clip_fraction             | 0.00547       |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00778      |
|    cost_value_loss           | 0.000163      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00167      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.04          |
|    mean_cost_advantages      | 0.00088522024 |
|    mean_reward_advantages    | -0.89024305   |
|    n_updates                 | 1290          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -0.000722     |
|    reward_explained_variance | -5.16         |
|    reward_value_loss         | 12.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.31          |
|    ep_rew_mean               | 6.27          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 131           |
|    time_elapsed              | 13366         |
|    total_timesteps           | 16768         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0189        |
|    cost_value_loss           | 0.000158      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00127      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.71          |
|    mean_cost_advantages      | 0.00096756604 |
|    mean_reward_advantages    | -0.34562588   |
|    n_updates                 | 1300          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.48e-08      |
|    reward_explained_variance | -9.97         |
|    reward_value_loss         | 7.31          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 132           |
|    time_elapsed              | 13465         |
|    total_timesteps           | 16896         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0147        |
|    cost_value_loss           | 0.000184      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00122      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.24          |
|    mean_cost_advantages      | 0.00012643577 |
|    mean_reward_advantages    | 0.5524918     |
|    n_updates                 | 1310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.07e-09      |
|    reward_explained_variance | -3.47         |
|    reward_value_loss         | 9.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.15           |
|    ep_rew_mean               | 6.13           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 133            |
|    time_elapsed              | 13569          |
|    total_timesteps           | 17024          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00673       |
|    cost_value_loss           | 0.00013        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.0012        |
|    learning_rate             | 0.0005         |
|    loss                      | 3.52           |
|    mean_cost_advantages      | -0.00038270486 |
|    mean_reward_advantages    | -0.5007931     |
|    n_updates                 | 1320           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.06e-08       |
|    reward_explained_variance | -1.12          |
|    reward_value_loss         | 7.34           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.87         |
|    ep_rew_mean               | 5.86         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 134          |
|    time_elapsed              | 13665        |
|    total_timesteps           | 17152        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0727       |
|    cost_value_loss           | 0.000112     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0012      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.09         |
|    mean_cost_advantages      | 0.0013617531 |
|    mean_reward_advantages    | 0.00837142   |
|    n_updates                 | 1330         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.21e-10    |
|    reward_explained_variance | -1           |
|    reward_value_loss         | 6.72         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.24         |
|    ep_rew_mean               | 6.22         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 135          |
|    time_elapsed              | 13763        |
|    total_timesteps           | 17280        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0226      |
|    cost_value_loss           | 0.000139     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00121     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.24         |
|    mean_cost_advantages      | -0.003914035 |
|    mean_reward_advantages    | 0.7479721    |
|    n_updates                 | 1340         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.77e-09     |
|    reward_explained_variance | -3.6         |
|    reward_value_loss         | 11.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.89         |
|    ep_rew_mean               | 5.87         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 136          |
|    time_elapsed              | 13862        |
|    total_timesteps           | 17408        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.469       |
|    cost_value_loss           | 0.000221     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00124     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.47         |
|    mean_cost_advantages      | 0.0005830239 |
|    mean_reward_advantages    | 1.6076908    |
|    n_updates                 | 1350         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.44e-09    |
|    reward_explained_variance | -7.39        |
|    reward_value_loss         | 14.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.98         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 137          |
|    time_elapsed              | 13958        |
|    total_timesteps           | 17536        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0168       |
|    cost_value_loss           | 0.000168     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00122     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.36         |
|    mean_cost_advantages      | 0.0009763482 |
|    mean_reward_advantages    | -1.0832355   |
|    n_updates                 | 1360         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.32e-09    |
|    reward_explained_variance | -2.87        |
|    reward_value_loss         | 6.96         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.04           |
|    ep_rew_mean               | 6.01           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 138            |
|    time_elapsed              | 14057          |
|    total_timesteps           | 17664          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00362        |
|    cost_value_loss           | 0.000123       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00122       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.78           |
|    mean_cost_advantages      | -0.00076961226 |
|    mean_reward_advantages    | -0.10259071    |
|    n_updates                 | 1370           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.14e-09      |
|    reward_explained_variance | -3.89          |
|    reward_value_loss         | 11.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.36          |
|    ep_rew_mean               | 6.33          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 139           |
|    time_elapsed              | 14152         |
|    total_timesteps           | 17792         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.002         |
|    cost_value_loss           | 0.000114      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00121      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.29          |
|    mean_cost_advantages      | -0.0023410877 |
|    mean_reward_advantages    | 0.38498074    |
|    n_updates                 | 1380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.47e-09      |
|    reward_explained_variance | -2.41         |
|    reward_value_loss         | 8.81          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 140           |
|    time_elapsed              | 14252         |
|    total_timesteps           | 17920         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0162        |
|    cost_value_loss           | 0.000168      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00123      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.77          |
|    mean_cost_advantages      | -0.0012959202 |
|    mean_reward_advantages    | 0.6067797     |
|    n_updates                 | 1390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.47e-09     |
|    reward_explained_variance | -6.89         |
|    reward_value_loss         | 12.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.29         |
|    ep_rew_mean               | 6.25         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 141          |
|    time_elapsed              | 14352        |
|    total_timesteps           | 18048        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.137       |
|    cost_value_loss           | 0.000177     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00121     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.54         |
|    mean_cost_advantages      | 0.0014381922 |
|    mean_reward_advantages    | -0.4965761   |
|    n_updates                 | 1400         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.14e-10     |
|    reward_explained_variance | -1.62        |
|    reward_value_loss         | 9.38         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.18         |
|    ep_rew_mean               | 6.14         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 142          |
|    time_elapsed              | 14453        |
|    total_timesteps           | 18176        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0247      |
|    cost_value_loss           | 0.00019      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00122     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.46         |
|    mean_cost_advantages      | 0.0012940073 |
|    mean_reward_advantages    | 1.1257874    |
|    n_updates                 | 1410         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.12e-09    |
|    reward_explained_variance | -6.19        |
|    reward_value_loss         | 11.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.4          |
|    ep_rew_mean               | 6.36         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 143          |
|    time_elapsed              | 14553        |
|    total_timesteps           | 18304        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.053        |
|    cost_value_loss           | 0.000164     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00125     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.37         |
|    mean_cost_advantages      | 0.0030978434 |
|    mean_reward_advantages    | -0.652655    |
|    n_updates                 | 1420         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.45e-09     |
|    reward_explained_variance | -1.86        |
|    reward_value_loss         | 7.91         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 144           |
|    time_elapsed              | 14650         |
|    total_timesteps           | 18432         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0165        |
|    cost_value_loss           | 0.000175      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00123      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.47          |
|    mean_cost_advantages      | -0.0013543193 |
|    mean_reward_advantages    | 0.9221318     |
|    n_updates                 | 1430          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.06e-10      |
|    reward_explained_variance | -1.24         |
|    reward_value_loss         | 7.26          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.88          |
|    ep_rew_mean               | 5.85          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 145           |
|    time_elapsed              | 14748         |
|    total_timesteps           | 18560         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0534        |
|    cost_value_loss           | 9.47e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0012       |
|    learning_rate             | 0.0005        |
|    loss                      | 5.35          |
|    mean_cost_advantages      | 0.00043086382 |
|    mean_reward_advantages    | 0.2945936     |
|    n_updates                 | 1440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.67e-10     |
|    reward_explained_variance | -2.53         |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.58          |
|    ep_rew_mean               | 5.55          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 146           |
|    time_elapsed              | 14845         |
|    total_timesteps           | 18688         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00508      |
|    cost_value_loss           | 0.000161      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00121      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.18          |
|    mean_cost_advantages      | -0.0013708961 |
|    mean_reward_advantages    | 0.16967079    |
|    n_updates                 | 1450          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.39e-09      |
|    reward_explained_variance | -2.51         |
|    reward_value_loss         | 10.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.21          |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 147           |
|    time_elapsed              | 14942         |
|    total_timesteps           | 18816         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0197        |
|    cost_value_loss           | 0.000128      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00124      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.93          |
|    mean_cost_advantages      | 0.00093975454 |
|    mean_reward_advantages    | -0.41390663   |
|    n_updates                 | 1460          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.26e-09      |
|    reward_explained_variance | -1.2          |
|    reward_value_loss         | 7.68          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.44         |
|    ep_rew_mean               | 6.41         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 148          |
|    time_elapsed              | 15037        |
|    total_timesteps           | 18944        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0481       |
|    cost_value_loss           | 0.000213     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00122     |
|    learning_rate             | 0.0005       |
|    loss                      | 7.73         |
|    mean_cost_advantages      | -0.002924345 |
|    mean_reward_advantages    | 1.6977932    |
|    n_updates                 | 1470         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.17e-09    |
|    reward_explained_variance | -2.32        |
|    reward_value_loss         | 15.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.28          |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 149           |
|    time_elapsed              | 15135         |
|    total_timesteps           | 19072         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00189       |
|    cost_value_loss           | 0.000245      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00118      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.8           |
|    mean_cost_advantages      | -0.0018491088 |
|    mean_reward_advantages    | -0.4850519    |
|    n_updates                 | 1480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.05e-09     |
|    reward_explained_variance | -2.52         |
|    reward_value_loss         | 11.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.24         |
|    ep_rew_mean               | 6.21         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 150          |
|    time_elapsed              | 15229        |
|    total_timesteps           | 19200        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.01         |
|    cost_value_loss           | 0.000137     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00122     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.94         |
|    mean_cost_advantages      | 0.0005173011 |
|    mean_reward_advantages    | 0.21476486   |
|    n_updates                 | 1490         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.03e-10    |
|    reward_explained_variance | -1.43        |
|    reward_value_loss         | 11.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.6           |
|    ep_rew_mean               | 6.56          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 151           |
|    time_elapsed              | 15333         |
|    total_timesteps           | 19328         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00437       |
|    cost_value_loss           | 0.000167      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00121      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.55          |
|    mean_cost_advantages      | 0.00063608075 |
|    mean_reward_advantages    | -0.15043856   |
|    n_updates                 | 1500          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.06e-09     |
|    reward_explained_variance | -1.71         |
|    reward_value_loss         | 9.18          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.57          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 152           |
|    time_elapsed              | 15441         |
|    total_timesteps           | 19456         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0606        |
|    cost_value_loss           | 0.0001        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00121      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.54          |
|    mean_cost_advantages      | -0.0006373699 |
|    mean_reward_advantages    | 0.59265006    |
|    n_updates                 | 1510          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.42e-09      |
|    reward_explained_variance | -3.65         |
|    reward_value_loss         | 13.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 153           |
|    time_elapsed              | 15551         |
|    total_timesteps           | 19584         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0276        |
|    cost_value_loss           | 0.000109      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00122      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.9           |
|    mean_cost_advantages      | -0.0005623209 |
|    mean_reward_advantages    | -1.1343782    |
|    n_updates                 | 1520          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.32e-10      |
|    reward_explained_variance | -2.39         |
|    reward_value_loss         | 9.23          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.17        |
|    ep_rew_mean               | 6.13        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 154         |
|    time_elapsed              | 15663       |
|    total_timesteps           | 19712       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0458     |
|    cost_value_loss           | 0.00021     |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00123    |
|    learning_rate             | 0.0005      |
|    loss                      | 4.63        |
|    mean_cost_advantages      | 0.002125814 |
|    mean_reward_advantages    | 0.9572482   |
|    n_updates                 | 1530        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -6.87e-09   |
|    reward_explained_variance | -4.02       |
|    reward_value_loss         | 10.7        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.18         |
|    ep_rew_mean               | 6.15         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 155          |
|    time_elapsed              | 15769        |
|    total_timesteps           | 19840        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.151       |
|    cost_value_loss           | 0.000156     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0012      |
|    learning_rate             | 0.0005       |
|    loss                      | 3.61         |
|    mean_cost_advantages      | 0.0018569507 |
|    mean_reward_advantages    | -0.32553864  |
|    n_updates                 | 1540         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.17e-09    |
|    reward_explained_variance | -1.3         |
|    reward_value_loss         | 6.49         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.12           |
|    ep_rew_mean               | 6.09           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 156            |
|    time_elapsed              | 15874          |
|    total_timesteps           | 19968          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0172         |
|    cost_value_loss           | 0.000139       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00121       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.33           |
|    mean_cost_advantages      | -0.00032132433 |
|    mean_reward_advantages    | 0.54285014     |
|    n_updates                 | 1550           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.34e-10       |
|    reward_explained_variance | -2.76          |
|    reward_value_loss         | 9.45           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.28         |
|    ep_rew_mean               | 6.25         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 157          |
|    time_elapsed              | 15982        |
|    total_timesteps           | 20096        |
| train/                       |              |
|    approx_kl                 | 0.0011724248 |
|    average_cost              | 0.0          |
|    clip_fraction             | 0.00391      |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0433       |
|    cost_value_loss           | 0.0002       |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00101     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.77         |
|    mean_cost_advantages      | 0.0006709832 |
|    mean_reward_advantages    | 0.82059455   |
|    n_updates                 | 1560         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -0.000186    |
|    reward_explained_variance | -3.04        |
|    reward_value_loss         | 9.12         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.35          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 158           |
|    time_elapsed              | 16082         |
|    total_timesteps           | 20224         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.013         |
|    cost_value_loss           | 0.000172      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000765     |
|    learning_rate             | 0.0005        |
|    loss                      | 9.7           |
|    mean_cost_advantages      | 3.0681025e-05 |
|    mean_reward_advantages    | 1.1669717     |
|    n_updates                 | 1570          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.05e-09     |
|    reward_explained_variance | -4.51         |
|    reward_value_loss         | 23.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.25         |
|    ep_rew_mean               | 6.21         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 159          |
|    time_elapsed              | 16184        |
|    total_timesteps           | 20352        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.168       |
|    cost_value_loss           | 0.000239     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000736    |
|    learning_rate             | 0.0005       |
|    loss                      | 10.3         |
|    mean_cost_advantages      | -0.004147834 |
|    mean_reward_advantages    | 0.94072974   |
|    n_updates                 | 1580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.34e-09    |
|    reward_explained_variance | -5.86        |
|    reward_value_loss         | 20.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.82         |
|    ep_rew_mean               | 5.78         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 160          |
|    time_elapsed              | 16295        |
|    total_timesteps           | 20480        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0182       |
|    cost_value_loss           | 0.000152     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000729    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.05         |
|    mean_cost_advantages      | 0.0011829974 |
|    mean_reward_advantages    | -1.3622689   |
|    n_updates                 | 1590         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.79e-09     |
|    reward_explained_variance | -2.19        |
|    reward_value_loss         | 11.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.9          |
|    ep_rew_mean               | 5.86         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 161          |
|    time_elapsed              | 16405        |
|    total_timesteps           | 20608        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0342       |
|    cost_value_loss           | 0.000112     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000739    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.89         |
|    mean_cost_advantages      | 0.0021084368 |
|    mean_reward_advantages    | -1.9094937   |
|    n_updates                 | 1600         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.13e-09     |
|    reward_explained_variance | -0.554       |
|    reward_value_loss         | 9.65         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.93          |
|    ep_rew_mean               | 5.89          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 162           |
|    time_elapsed              | 16512         |
|    total_timesteps           | 20736         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00333      |
|    cost_value_loss           | 0.000194      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00071      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.43          |
|    mean_cost_advantages      | -0.0015293487 |
|    mean_reward_advantages    | 1.592582      |
|    n_updates                 | 1610          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.8e-09       |
|    reward_explained_variance | -2.38         |
|    reward_value_loss         | 12.1          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.26        |
|    ep_rew_mean               | 5.24        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 163         |
|    time_elapsed              | 16609       |
|    total_timesteps           | 20864       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0102      |
|    cost_value_loss           | 0.000114    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000713   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.73        |
|    mean_cost_advantages      | -0.000334   |
|    mean_reward_advantages    | -0.67725873 |
|    n_updates                 | 1620        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 3.21e-09    |
|    reward_explained_variance | -2.71       |
|    reward_value_loss         | 9.46        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.43          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 164           |
|    time_elapsed              | 16706         |
|    total_timesteps           | 20992         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.196        |
|    cost_value_loss           | 0.000124      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000711     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.16          |
|    mean_cost_advantages      | -0.0032031257 |
|    mean_reward_advantages    | 0.64492255    |
|    n_updates                 | 1630          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.93e-09      |
|    reward_explained_variance | -2.12         |
|    reward_value_loss         | 15.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.88         |
|    ep_rew_mean               | 5.85         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 165          |
|    time_elapsed              | 16802        |
|    total_timesteps           | 21120        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0546       |
|    cost_value_loss           | 0.000151     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00073     |
|    learning_rate             | 0.0005       |
|    loss                      | 7.23         |
|    mean_cost_advantages      | 0.0012033342 |
|    mean_reward_advantages    | -0.12209259  |
|    n_updates                 | 1640         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.23e-09    |
|    reward_explained_variance | -4.16        |
|    reward_value_loss         | 14.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.11         |
|    ep_rew_mean               | 6.08         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 166          |
|    time_elapsed              | 16895        |
|    total_timesteps           | 21248        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0215      |
|    cost_value_loss           | 0.00016      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000723    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.23         |
|    mean_cost_advantages      | 0.0021348163 |
|    mean_reward_advantages    | -0.4531995   |
|    n_updates                 | 1650         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.79e-10    |
|    reward_explained_variance | -1.98        |
|    reward_value_loss         | 11.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 167           |
|    time_elapsed              | 17000         |
|    total_timesteps           | 21376         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0172        |
|    cost_value_loss           | 0.000134      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000703     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.26          |
|    mean_cost_advantages      | 0.00077072764 |
|    mean_reward_advantages    | -0.5747102    |
|    n_updates                 | 1660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.34e-10     |
|    reward_explained_variance | 0.0276        |
|    reward_value_loss         | 5.81          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.61           |
|    ep_rew_mean               | 6.58           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 168            |
|    time_elapsed              | 17102          |
|    total_timesteps           | 21504          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00906       |
|    cost_value_loss           | 0.00018        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000734      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.93           |
|    mean_cost_advantages      | -0.00020190189 |
|    mean_reward_advantages    | 1.455256       |
|    n_updates                 | 1670           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.89e-09      |
|    reward_explained_variance | -0.608         |
|    reward_value_loss         | 8.23           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.62          |
|    ep_rew_mean               | 6.59          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 169           |
|    time_elapsed              | 17207         |
|    total_timesteps           | 21632         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.129        |
|    cost_value_loss           | 0.00018       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.13          |
|    mean_cost_advantages      | 9.9786674e-05 |
|    mean_reward_advantages    | 1.8301588     |
|    n_updates                 | 1680          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.44e-10     |
|    reward_explained_variance | -4.19         |
|    reward_value_loss         | 21.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.64          |
|    ep_rew_mean               | 6.61          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 170           |
|    time_elapsed              | 17311         |
|    total_timesteps           | 21760         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0567       |
|    cost_value_loss           | 0.000125      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.31          |
|    mean_cost_advantages      | 0.00023820397 |
|    mean_reward_advantages    | -1.952402     |
|    n_updates                 | 1690          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.89e-09     |
|    reward_explained_variance | -0.677        |
|    reward_value_loss         | 13.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.01           |
|    ep_rew_mean               | 5.98           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 171            |
|    time_elapsed              | 17417          |
|    total_timesteps           | 21888          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00521        |
|    cost_value_loss           | 0.000107       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000731      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.28           |
|    mean_cost_advantages      | -0.00052587263 |
|    mean_reward_advantages    | 0.23024942     |
|    n_updates                 | 1700           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.34e-10       |
|    reward_explained_variance | -1.62          |
|    reward_value_loss         | 10.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.04          |
|    ep_rew_mean               | 6.01          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 172           |
|    time_elapsed              | 17516         |
|    total_timesteps           | 22016         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0407        |
|    cost_value_loss           | 0.000103      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000716     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.73          |
|    mean_cost_advantages      | -0.0008484848 |
|    mean_reward_advantages    | -0.4425975    |
|    n_updates                 | 1710          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.4e-10      |
|    reward_explained_variance | -0.768        |
|    reward_value_loss         | 4.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.47         |
|    ep_rew_mean               | 5.45         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 173          |
|    time_elapsed              | 17624        |
|    total_timesteps           | 22144        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0318       |
|    cost_value_loss           | 9.1e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000707    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.22         |
|    mean_cost_advantages      | 0.0010950961 |
|    mean_reward_advantages    | 0.7869655    |
|    n_updates                 | 1720         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.09e-09    |
|    reward_explained_variance | -1.21        |
|    reward_value_loss         | 4.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.72         |
|    ep_rew_mean               | 5.69         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 174          |
|    time_elapsed              | 17729        |
|    total_timesteps           | 22272        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0104       |
|    cost_value_loss           | 0.000124     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.99         |
|    mean_cost_advantages      | 0.0009121741 |
|    mean_reward_advantages    | 1.430925     |
|    n_updates                 | 1730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.99e-10    |
|    reward_explained_variance | -2.09        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.59        |
|    ep_rew_mean               | 5.56        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 175         |
|    time_elapsed              | 17826       |
|    total_timesteps           | 22400       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.00765     |
|    cost_value_loss           | 0.000153    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000741   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.07        |
|    mean_cost_advantages      | 0.002166887 |
|    mean_reward_advantages    | 0.40944022  |
|    n_updates                 | 1740        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.24e-09   |
|    reward_explained_variance | -1.52       |
|    reward_value_loss         | 9.77        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 176           |
|    time_elapsed              | 17923         |
|    total_timesteps           | 22528         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0712        |
|    cost_value_loss           | 0.00023       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000732     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.53          |
|    mean_cost_advantages      | -0.0008366109 |
|    mean_reward_advantages    | 0.7969516     |
|    n_updates                 | 1750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.48e-10      |
|    reward_explained_variance | -2.21         |
|    reward_value_loss         | 12            |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.71           |
|    ep_rew_mean               | 6.67           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 177            |
|    time_elapsed              | 18025          |
|    total_timesteps           | 22656          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0528         |
|    cost_value_loss           | 0.000178       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000718      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.34           |
|    mean_cost_advantages      | -0.00030239343 |
|    mean_reward_advantages    | 1.0182407      |
|    n_updates                 | 1760           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.52e-10      |
|    reward_explained_variance | -3.65          |
|    reward_value_loss         | 16.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.94          |
|    ep_rew_mean               | 6.89          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 178           |
|    time_elapsed              | 18126         |
|    total_timesteps           | 22784         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00312      |
|    cost_value_loss           | 0.000168      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.86          |
|    mean_cost_advantages      | -0.0016311044 |
|    mean_reward_advantages    | -1.1940886    |
|    n_updates                 | 1770          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.67e-09      |
|    reward_explained_variance | -1.2          |
|    reward_value_loss         | 10.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.25          |
|    ep_rew_mean               | 7.2           |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 179           |
|    time_elapsed              | 18228         |
|    total_timesteps           | 22912         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0425        |
|    cost_value_loss           | 0.000109      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000728     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.34          |
|    mean_cost_advantages      | 0.00053779996 |
|    mean_reward_advantages    | -0.21569481   |
|    n_updates                 | 1780          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.22e-09     |
|    reward_explained_variance | -0.953        |
|    reward_value_loss         | 7.75          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.12          |
|    ep_rew_mean               | 7.07          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 180           |
|    time_elapsed              | 18331         |
|    total_timesteps           | 23040         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.118         |
|    cost_value_loss           | 0.000106      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.08          |
|    mean_cost_advantages      | -0.0003629827 |
|    mean_reward_advantages    | -0.11142603   |
|    n_updates                 | 1790          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.44e-10      |
|    reward_explained_variance | -1.99         |
|    reward_value_loss         | 7.91          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.96          |
|    ep_rew_mean               | 6.92          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 181           |
|    time_elapsed              | 18432         |
|    total_timesteps           | 23168         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00452       |
|    cost_value_loss           | 0.000136      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000723     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.38          |
|    mean_cost_advantages      | 0.00095569063 |
|    mean_reward_advantages    | 0.9817269     |
|    n_updates                 | 1800          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.82e-09      |
|    reward_explained_variance | -0.47         |
|    reward_value_loss         | 7.39          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.8           |
|    ep_rew_mean               | 6.75          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 182           |
|    time_elapsed              | 18528         |
|    total_timesteps           | 23296         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00165      |
|    cost_value_loss           | 0.000111      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000713     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.31          |
|    mean_cost_advantages      | 0.00010252686 |
|    mean_reward_advantages    | 0.38433814    |
|    n_updates                 | 1810          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.75e-10      |
|    reward_explained_variance | -0.964        |
|    reward_value_loss         | 6.96          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.18          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 183           |
|    time_elapsed              | 18627         |
|    total_timesteps           | 23424         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00559      |
|    cost_value_loss           | 0.000142      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000734     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.75          |
|    mean_cost_advantages      | 0.00024927018 |
|    mean_reward_advantages    | -0.55148107   |
|    n_updates                 | 1820          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.77e-09     |
|    reward_explained_variance | -0.917        |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.19         |
|    ep_rew_mean               | 6.16         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 184          |
|    time_elapsed              | 18725        |
|    total_timesteps           | 23552        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.173       |
|    cost_value_loss           | 0.000163     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000724    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.06         |
|    mean_cost_advantages      | 0.0018050205 |
|    mean_reward_advantages    | -0.32354426  |
|    n_updates                 | 1830         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.57e-09     |
|    reward_explained_variance | -1.01        |
|    reward_value_loss         | 8.93         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.85          |
|    ep_rew_mean               | 5.82          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 185           |
|    time_elapsed              | 18823         |
|    total_timesteps           | 23680         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0281        |
|    cost_value_loss           | 0.000167      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.18          |
|    mean_cost_advantages      | -0.0024233833 |
|    mean_reward_advantages    | 0.17353606    |
|    n_updates                 | 1840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.44e-09     |
|    reward_explained_variance | -3.21         |
|    reward_value_loss         | 7.16          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.81          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 186           |
|    time_elapsed              | 18918         |
|    total_timesteps           | 23808         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.017         |
|    cost_value_loss           | 0.000161      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000709     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.85          |
|    mean_cost_advantages      | -0.0010172832 |
|    mean_reward_advantages    | -0.82028985   |
|    n_updates                 | 1850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.14e-09     |
|    reward_explained_variance | -2.3          |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.53         |
|    ep_rew_mean               | 5.5          |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 187          |
|    time_elapsed              | 19016        |
|    total_timesteps           | 23936        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0857      |
|    cost_value_loss           | 0.000134     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000726    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.33         |
|    mean_cost_advantages      | 0.0007227345 |
|    mean_reward_advantages    | 0.15577811   |
|    n_updates                 | 1860         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.87e-09    |
|    reward_explained_variance | -1.14        |
|    reward_value_loss         | 5.92         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.73           |
|    ep_rew_mean               | 5.7            |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 188            |
|    time_elapsed              | 19113          |
|    total_timesteps           | 24064          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0646        |
|    cost_value_loss           | 0.000168       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000725      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.48           |
|    mean_cost_advantages      | -0.00020874666 |
|    mean_reward_advantages    | 1.3363799      |
|    n_updates                 | 1870           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.32e-09       |
|    reward_explained_variance | -2.2           |
|    reward_value_loss         | 9.91           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.63           |
|    ep_rew_mean               | 5.59           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 189            |
|    time_elapsed              | 19210          |
|    total_timesteps           | 24192          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00884        |
|    cost_value_loss           | 0.000152       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000732      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.75           |
|    mean_cost_advantages      | -0.00056100066 |
|    mean_reward_advantages    | -0.832537      |
|    n_updates                 | 1880           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.94e-09       |
|    reward_explained_variance | -4.87          |
|    reward_value_loss         | 14.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.93         |
|    ep_rew_mean               | 5.89         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 190          |
|    time_elapsed              | 19307        |
|    total_timesteps           | 24320        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0376       |
|    cost_value_loss           | 0.000137     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000711    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.59         |
|    mean_cost_advantages      | -0.001670866 |
|    mean_reward_advantages    | 0.41446158   |
|    n_updates                 | 1890         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.32e-09     |
|    reward_explained_variance | -2.04        |
|    reward_value_loss         | 11.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.59         |
|    ep_rew_mean               | 6.54         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 191          |
|    time_elapsed              | 19404        |
|    total_timesteps           | 24448        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0535       |
|    cost_value_loss           | 0.000181     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000717    |
|    learning_rate             | 0.0005       |
|    loss                      | 9.06         |
|    mean_cost_advantages      | 0.0012869616 |
|    mean_reward_advantages    | 0.99610615   |
|    n_updates                 | 1900         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.48e-10     |
|    reward_explained_variance | -5.56        |
|    reward_value_loss         | 19           |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.94           |
|    ep_rew_mean               | 6.89           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 192            |
|    time_elapsed              | 19503          |
|    total_timesteps           | 24576          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0155        |
|    cost_value_loss           | 0.000136       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00073       |
|    learning_rate             | 0.0005         |
|    loss                      | 7.54           |
|    mean_cost_advantages      | -0.00032399947 |
|    mean_reward_advantages    | -0.7740619     |
|    n_updates                 | 1910           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.31e-09      |
|    reward_explained_variance | -3.06          |
|    reward_value_loss         | 19.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.73         |
|    ep_rew_mean               | 6.69         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 193          |
|    time_elapsed              | 19612        |
|    total_timesteps           | 24704        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0438       |
|    cost_value_loss           | 0.000165     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 7.16         |
|    mean_cost_advantages      | 0.0015154094 |
|    mean_reward_advantages    | 1.0736119    |
|    n_updates                 | 1920         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.42e-09    |
|    reward_explained_variance | -4.43        |
|    reward_value_loss         | 17.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.68         |
|    ep_rew_mean               | 6.64         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 194          |
|    time_elapsed              | 19716        |
|    total_timesteps           | 24832        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0412       |
|    cost_value_loss           | 0.000108     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000717    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.08         |
|    mean_cost_advantages      | 0.0007173852 |
|    mean_reward_advantages    | -1.6279407   |
|    n_updates                 | 1930         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.42e-09    |
|    reward_explained_variance | -0.244       |
|    reward_value_loss         | 8.86         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.05          |
|    ep_rew_mean               | 7.01          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 195           |
|    time_elapsed              | 19819         |
|    total_timesteps           | 24960         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.059        |
|    cost_value_loss           | 0.000127      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000729     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.87          |
|    mean_cost_advantages      | -0.0011891602 |
|    mean_reward_advantages    | 0.6098955     |
|    n_updates                 | 1940          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.93e-09      |
|    reward_explained_variance | -0.693        |
|    reward_value_loss         | 8.51          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.52        |
|    ep_rew_mean               | 6.49        |
| time/                        |             |
|    fps                       | 1           |
|    iterations                | 196         |
|    time_elapsed              | 19919       |
|    total_timesteps           | 25088       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0259      |
|    cost_value_loss           | 0.000129    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000713   |
|    learning_rate             | 0.0005      |
|    loss                      | 3.14        |
|    mean_cost_advantages      | 0.001109995 |
|    mean_reward_advantages    | 0.20201446  |
|    n_updates                 | 1950        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 8.51e-10    |
|    reward_explained_variance | -1.54       |
|    reward_value_loss         | 6.56        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.49         |
|    ep_rew_mean               | 6.46         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 197          |
|    time_elapsed              | 20025        |
|    total_timesteps           | 25216        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0377       |
|    cost_value_loss           | 0.000155     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000731    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.73         |
|    mean_cost_advantages      | 0.0023092625 |
|    mean_reward_advantages    | -0.57196915  |
|    n_updates                 | 1960         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.26e-10    |
|    reward_explained_variance | -0.0801      |
|    reward_value_loss         | 4.91         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.36          |
| time/                        |               |
|    fps                       | 1             |
|    iterations                | 198           |
|    time_elapsed              | 20128         |
|    total_timesteps           | 25344         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0456        |
|    cost_value_loss           | 9.45e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.78          |
|    mean_cost_advantages      | -0.0006076094 |
|    mean_reward_advantages    | 0.015556321   |
|    n_updates                 | 1970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.36e-09     |
|    reward_explained_variance | 0.108         |
|    reward_value_loss         | 4.67          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.38         |
|    ep_rew_mean               | 6.35         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 199          |
|    time_elapsed              | 20232        |
|    total_timesteps           | 25472        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.3         |
|    cost_value_loss           | 0.000228     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000708    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.56         |
|    mean_cost_advantages      | -0.003669398 |
|    mean_reward_advantages    | 1.2852023    |
|    n_updates                 | 1980         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.13e-09    |
|    reward_explained_variance | -1.81        |
|    reward_value_loss         | 13.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.1          |
|    ep_rew_mean               | 6.07         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 200          |
|    time_elapsed              | 20336        |
|    total_timesteps           | 25600        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0268       |
|    cost_value_loss           | 0.000108     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000712    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.47         |
|    mean_cost_advantages      | 0.0029875415 |
|    mean_reward_advantages    | -1.2695498   |
|    n_updates                 | 1990         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.64e-10     |
|    reward_explained_variance | -0.592       |
|    reward_value_loss         | 5.91         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.97         |
|    ep_rew_mean               | 5.94         |
| time/                        |              |
|    fps                       | 1            |
|    iterations                | 201          |
|    time_elapsed              | 20441        |
|    total_timesteps           | 25728        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.187       |
|    cost_value_loss           | 0.000169     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000733    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.75         |
|    mean_cost_advantages      | 0.0025113472 |
|    mean_reward_advantages    | 1.703699     |
|    n_updates                 | 2000         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.01e-10    |
|    reward_explained_variance | -2.18        |
|    reward_value_loss         | 8.94         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.09           |
|    ep_rew_mean               | 6.05           |
| time/                        |                |
|    fps                       | 1              |
|    iterations                | 202            |
|    time_elapsed              | 20543          |
|    total_timesteps           | 25856          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.041          |
|    cost_value_loss           | 0.000115       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000729      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.32           |
|    mean_cost_advantages      | 0.000102689606 |
|    mean_reward_advantages    | -0.48448837    |
|    n_updates                 | 2010           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 9.34e-10       |
|    reward_explained_variance | -0.467         |
|    reward_value_loss         | 7.08           |
|    total_cost                | 0.0            |
-------------------------------------------------
wandb: Network error (ProxyError), entering retry loop.
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.26          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 203           |
|    time_elapsed              | 62344         |
|    total_timesteps           | 25984         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0451        |
|    cost_value_loss           | 0.000131      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000748     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.35          |
|    mean_cost_advantages      | 0.00042574108 |
|    mean_reward_advantages    | -0.061873868  |
|    n_updates                 | 2020          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.11e-09     |
|    reward_explained_variance | -1.13         |
|    reward_value_loss         | 11.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.07           |
|    ep_rew_mean               | 6.03           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 204            |
|    time_elapsed              | 62433          |
|    total_timesteps           | 26112          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00123       |
|    cost_value_loss           | 0.000179       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000732      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.53           |
|    mean_cost_advantages      | -0.00023734965 |
|    mean_reward_advantages    | 0.9521986      |
|    n_updates                 | 2030           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.94e-09       |
|    reward_explained_variance | -1.74          |
|    reward_value_loss         | 15.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.29         |
|    ep_rew_mean               | 6.24         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 205          |
|    time_elapsed              | 62524        |
|    total_timesteps           | 26240        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0638       |
|    cost_value_loss           | 0.000132     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000702    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.1          |
|    mean_cost_advantages      | -0.002114529 |
|    mean_reward_advantages    | 0.59321094   |
|    n_updates                 | 2040         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.72e-10    |
|    reward_explained_variance | -1.37        |
|    reward_value_loss         | 8.13         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.07         |
|    ep_rew_mean               | 6.03         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 206          |
|    time_elapsed              | 62616        |
|    total_timesteps           | 26368        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0408       |
|    cost_value_loss           | 0.000117     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000737    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.67         |
|    mean_cost_advantages      | 0.0008633161 |
|    mean_reward_advantages    | 0.42120075   |
|    n_updates                 | 2050         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.42e-09    |
|    reward_explained_variance | -3.68        |
|    reward_value_loss         | 10.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.24         |
|    ep_rew_mean               | 6.2          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 207          |
|    time_elapsed              | 62707        |
|    total_timesteps           | 26496        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00235     |
|    cost_value_loss           | 0.000104     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000721    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.64         |
|    mean_cost_advantages      | 0.0016619025 |
|    mean_reward_advantages    | 0.4281789    |
|    n_updates                 | 2060         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.28e-09    |
|    reward_explained_variance | -0.763       |
|    reward_value_loss         | 7.53         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.05         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 208          |
|    time_elapsed              | 62803        |
|    total_timesteps           | 26624        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0237       |
|    cost_value_loss           | 0.000114     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 10.5         |
|    mean_cost_advantages      | -0.000182004 |
|    mean_reward_advantages    | 1.3448486    |
|    n_updates                 | 2070         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.98e-09     |
|    reward_explained_variance | -5.02        |
|    reward_value_loss         | 22.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.78           |
|    ep_rew_mean               | 5.75           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 209            |
|    time_elapsed              | 62901          |
|    total_timesteps           | 26752          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.03          |
|    cost_value_loss           | 0.000178       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000724      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.94           |
|    mean_cost_advantages      | -0.00082685763 |
|    mean_reward_advantages    | -0.7350551     |
|    n_updates                 | 2080           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.72e-09       |
|    reward_explained_variance | -0.674         |
|    reward_value_loss         | 7.58           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.8           |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 210           |
|    time_elapsed              | 63000         |
|    total_timesteps           | 26880         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00083       |
|    cost_value_loss           | 0.000109      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.31          |
|    mean_cost_advantages      | 0.00012448966 |
|    mean_reward_advantages    | -0.078907     |
|    n_updates                 | 2090          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.23e-09      |
|    reward_explained_variance | -1.11         |
|    reward_value_loss         | 9.14          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 211           |
|    time_elapsed              | 63120         |
|    total_timesteps           | 27008         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0272        |
|    cost_value_loss           | 0.00012       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000717     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.39          |
|    mean_cost_advantages      | 0.00039323536 |
|    mean_reward_advantages    | 0.3043991     |
|    n_updates                 | 2100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.04e-08      |
|    reward_explained_variance | -2.46         |
|    reward_value_loss         | 13.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.44          |
|    ep_rew_mean               | 5.42          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 212           |
|    time_elapsed              | 63242         |
|    total_timesteps           | 27136         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0233        |
|    cost_value_loss           | 0.000148      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000715     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.53          |
|    mean_cost_advantages      | -0.0008573103 |
|    mean_reward_advantages    | 0.49396235    |
|    n_updates                 | 2110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.71e-10     |
|    reward_explained_variance | -1.84         |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.72         |
|    ep_rew_mean               | 5.7          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 213          |
|    time_elapsed              | 63354        |
|    total_timesteps           | 27264        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00919      |
|    cost_value_loss           | 9.97e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000711    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.14         |
|    mean_cost_advantages      | 0.0017883922 |
|    mean_reward_advantages    | -0.7172729   |
|    n_updates                 | 2120         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.8e-09      |
|    reward_explained_variance | -3.16        |
|    reward_value_loss         | 7.93         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.75           |
|    ep_rew_mean               | 5.73           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 214            |
|    time_elapsed              | 63465          |
|    total_timesteps           | 27392          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0219         |
|    cost_value_loss           | 0.000145       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000702      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.23           |
|    mean_cost_advantages      | -0.00023421348 |
|    mean_reward_advantages    | 1.5917637      |
|    n_updates                 | 2130           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.4e-10       |
|    reward_explained_variance | -2.18          |
|    reward_value_loss         | 10.9           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.99           |
|    ep_rew_mean               | 5.97           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 215            |
|    time_elapsed              | 63573          |
|    total_timesteps           | 27520          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0162         |
|    cost_value_loss           | 0.000112       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000718      |
|    learning_rate             | 0.0005         |
|    loss                      | 10.7           |
|    mean_cost_advantages      | -5.0515664e-05 |
|    mean_reward_advantages    | 0.35623276     |
|    n_updates                 | 2140           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.19e-09       |
|    reward_explained_variance | -7.65          |
|    reward_value_loss         | 21.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.18         |
|    ep_rew_mean               | 6.16         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 216          |
|    time_elapsed              | 63679        |
|    total_timesteps           | 27648        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0168      |
|    cost_value_loss           | 0.000107     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000731    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.19         |
|    mean_cost_advantages      | 0.0011679027 |
|    mean_reward_advantages    | -1.6452749   |
|    n_updates                 | 2150         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.46e-09    |
|    reward_explained_variance | -0.858       |
|    reward_value_loss         | 7.18         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.22         |
|    ep_rew_mean               | 6.2          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 217          |
|    time_elapsed              | 63784        |
|    total_timesteps           | 27776        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0345      |
|    cost_value_loss           | 0.000157     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000713    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.63         |
|    mean_cost_advantages      | 0.0010644046 |
|    mean_reward_advantages    | 0.15116939   |
|    n_updates                 | 2160         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.18e-10    |
|    reward_explained_variance | -0.411       |
|    reward_value_loss         | 7.58         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.3            |
|    ep_rew_mean               | 6.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 218            |
|    time_elapsed              | 63887          |
|    total_timesteps           | 27904          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.12          |
|    cost_value_loss           | 0.000138       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000715      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.53           |
|    mean_cost_advantages      | -0.00054733874 |
|    mean_reward_advantages    | 1.0062466      |
|    n_updates                 | 2170           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.24e-09       |
|    reward_explained_variance | -4.09          |
|    reward_value_loss         | 11.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.92          |
|    ep_rew_mean               | 5.9           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 219           |
|    time_elapsed              | 63994         |
|    total_timesteps           | 28032         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0329        |
|    cost_value_loss           | 9.42e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000723     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.34          |
|    mean_cost_advantages      | -0.0012607339 |
|    mean_reward_advantages    | -1.6044705    |
|    n_updates                 | 2180          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.83e-09      |
|    reward_explained_variance | -0.499        |
|    reward_value_loss         | 6.46          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.85         |
|    ep_rew_mean               | 5.82         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 220          |
|    time_elapsed              | 64100        |
|    total_timesteps           | 28160        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000863    |
|    cost_value_loss           | 0.000129     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000733    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.5          |
|    mean_cost_advantages      | -0.001259459 |
|    mean_reward_advantages    | -0.41531605  |
|    n_updates                 | 2190         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.58e-09     |
|    reward_explained_variance | -0.654       |
|    reward_value_loss         | 6.17         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.66          |
|    ep_rew_mean               | 5.63          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 221           |
|    time_elapsed              | 64208         |
|    total_timesteps           | 28288         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0182        |
|    cost_value_loss           | 0.000148      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.92          |
|    mean_cost_advantages      | 0.00078354473 |
|    mean_reward_advantages    | 0.2377272     |
|    n_updates                 | 2200          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.54e-10     |
|    reward_explained_variance | -1.23         |
|    reward_value_loss         | 7.34          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.82          |
|    ep_rew_mean               | 5.79          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 222           |
|    time_elapsed              | 64314         |
|    total_timesteps           | 28416         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0252       |
|    cost_value_loss           | 0.000125      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 10.6          |
|    mean_cost_advantages      | -0.0002011593 |
|    mean_reward_advantages    | 1.0856795     |
|    n_updates                 | 2210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.1e-09       |
|    reward_explained_variance | -2.96         |
|    reward_value_loss         | 15.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.74          |
|    ep_rew_mean               | 5.71          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 223           |
|    time_elapsed              | 64422         |
|    total_timesteps           | 28544         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0206        |
|    cost_value_loss           | 9.45e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000731     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.5           |
|    mean_cost_advantages      | -0.0006298161 |
|    mean_reward_advantages    | 0.6588372     |
|    n_updates                 | 2220          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.73e-10      |
|    reward_explained_variance | -3.95         |
|    reward_value_loss         | 13.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.23         |
|    ep_rew_mean               | 6.2          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 224          |
|    time_elapsed              | 64526        |
|    total_timesteps           | 28672        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.22        |
|    cost_value_loss           | 0.000112     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000708    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.06         |
|    mean_cost_advantages      | 0.0012423266 |
|    mean_reward_advantages    | 0.038495496  |
|    n_updates                 | 2230         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.24e-09    |
|    reward_explained_variance | -1.58        |
|    reward_value_loss         | 9.44         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.49           |
|    ep_rew_mean               | 6.46           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 225            |
|    time_elapsed              | 64630          |
|    total_timesteps           | 28800          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0174         |
|    cost_value_loss           | 0.00015        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000717      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.83           |
|    mean_cost_advantages      | -0.00029419575 |
|    mean_reward_advantages    | 1.2337409      |
|    n_updates                 | 2240           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.76e-10      |
|    reward_explained_variance | -0.986         |
|    reward_value_loss         | 9.77           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.69          |
|    ep_rew_mean               | 6.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 226           |
|    time_elapsed              | 64731         |
|    total_timesteps           | 28928         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.024        |
|    cost_value_loss           | 0.00011       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000726     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.31          |
|    mean_cost_advantages      | 0.00022720735 |
|    mean_reward_advantages    | -0.032161564  |
|    n_updates                 | 2250          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.34e-09      |
|    reward_explained_variance | -2.66         |
|    reward_value_loss         | 12.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.82          |
|    ep_rew_mean               | 6.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 227           |
|    time_elapsed              | 64838         |
|    total_timesteps           | 29056         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0172        |
|    cost_value_loss           | 0.000124      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000721     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.42          |
|    mean_cost_advantages      | -9.186915e-07 |
|    mean_reward_advantages    | -0.5444609    |
|    n_updates                 | 2260          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.36e-09      |
|    reward_explained_variance | -2.35         |
|    reward_value_loss         | 8.13          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.65          |
|    ep_rew_mean               | 6.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 228           |
|    time_elapsed              | 64942         |
|    total_timesteps           | 29184         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.155        |
|    cost_value_loss           | 0.000182      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000728     |
|    learning_rate             | 0.0005        |
|    loss                      | 11.1          |
|    mean_cost_advantages      | -0.0014174541 |
|    mean_reward_advantages    | 2.148523      |
|    n_updates                 | 2270          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.36e-09      |
|    reward_explained_variance | -5.77         |
|    reward_value_loss         | 27.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.82           |
|    ep_rew_mean               | 6.78           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 229            |
|    time_elapsed              | 65050          |
|    total_timesteps           | 29312          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00134        |
|    cost_value_loss           | 0.000111       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000718      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.02           |
|    mean_cost_advantages      | -0.00040094485 |
|    mean_reward_advantages    | -1.5041533     |
|    n_updates                 | 2280           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.18e-09       |
|    reward_explained_variance | -0.844         |
|    reward_value_loss         | 13.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.27         |
|    ep_rew_mean               | 6.23         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 230          |
|    time_elapsed              | 65153        |
|    total_timesteps           | 29440        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0358      |
|    cost_value_loss           | 0.000158     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000734    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.37         |
|    mean_cost_advantages      | 0.0018537606 |
|    mean_reward_advantages    | -1.0674609   |
|    n_updates                 | 2290         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.74e-09    |
|    reward_explained_variance | -1.47        |
|    reward_value_loss         | 10.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.97          |
|    ep_rew_mean               | 5.94          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 231           |
|    time_elapsed              | 65259         |
|    total_timesteps           | 29568         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0417        |
|    cost_value_loss           | 8.93e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000713     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.45          |
|    mean_cost_advantages      | 0.00015931718 |
|    mean_reward_advantages    | -0.14514391   |
|    n_updates                 | 2300          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.63e-09     |
|    reward_explained_variance | -2.18         |
|    reward_value_loss         | 8.83          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.48          |
|    ep_rew_mean               | 5.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 232           |
|    time_elapsed              | 65363         |
|    total_timesteps           | 29696         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000926     |
|    cost_value_loss           | 9.32e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000717     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.9           |
|    mean_cost_advantages      | 0.00010576742 |
|    mean_reward_advantages    | -0.707188     |
|    n_updates                 | 2310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.98e-09      |
|    reward_explained_variance | -2.48         |
|    reward_value_loss         | 5.24          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.21          |
|    ep_rew_mean               | 5.19          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 233           |
|    time_elapsed              | 65463         |
|    total_timesteps           | 29824         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0263        |
|    cost_value_loss           | 9.28e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000721     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.41          |
|    mean_cost_advantages      | -0.0010069975 |
|    mean_reward_advantages    | 0.74668354    |
|    n_updates                 | 2320          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.3e-09      |
|    reward_explained_variance | -1.75         |
|    reward_value_loss         | 9.25          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.39         |
|    ep_rew_mean               | 5.37         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 234          |
|    time_elapsed              | 65570        |
|    total_timesteps           | 29952        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0258       |
|    cost_value_loss           | 0.000147     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000707    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.17         |
|    mean_cost_advantages      | 0.0014650174 |
|    mean_reward_advantages    | -0.34492606  |
|    n_updates                 | 2330         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.96e-09     |
|    reward_explained_variance | -0.496       |
|    reward_value_loss         | 5.76         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.58          |
|    ep_rew_mean               | 5.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 235           |
|    time_elapsed              | 65675         |
|    total_timesteps           | 30080         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0375        |
|    cost_value_loss           | 0.000108      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000707     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.35          |
|    mean_cost_advantages      | -0.0022785026 |
|    mean_reward_advantages    | 0.5435941     |
|    n_updates                 | 2340          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.01e-09      |
|    reward_explained_variance | -3.13         |
|    reward_value_loss         | 12.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.9           |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 236           |
|    time_elapsed              | 65785         |
|    total_timesteps           | 30208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0484        |
|    cost_value_loss           | 0.000197      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.28          |
|    mean_cost_advantages      | -0.0016938988 |
|    mean_reward_advantages    | 0.5591141     |
|    n_updates                 | 2350          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.83e-10     |
|    reward_explained_variance | -2.77         |
|    reward_value_loss         | 13.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.9          |
|    ep_rew_mean               | 5.87         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 237          |
|    time_elapsed              | 65890        |
|    total_timesteps           | 30336        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.000898     |
|    cost_value_loss           | 0.000152     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00071     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.18         |
|    mean_cost_advantages      | 0.0027329107 |
|    mean_reward_advantages    | -0.7026824   |
|    n_updates                 | 2360         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.23e-09    |
|    reward_explained_variance | -1.17        |
|    reward_value_loss         | 8.37         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 238           |
|    time_elapsed              | 66005         |
|    total_timesteps           | 30464         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0954       |
|    cost_value_loss           | 0.000114      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000721     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.44          |
|    mean_cost_advantages      | -0.0013728463 |
|    mean_reward_advantages    | 0.58873695    |
|    n_updates                 | 2370          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.86e-10      |
|    reward_explained_variance | -4.2          |
|    reward_value_loss         | 13            |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.32         |
|    ep_rew_mean               | 6.29         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 239          |
|    time_elapsed              | 66117        |
|    total_timesteps           | 30592        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00493      |
|    cost_value_loss           | 0.000168     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000716    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.05         |
|    mean_cost_advantages      | 0.0022562975 |
|    mean_reward_advantages    | 0.21843828   |
|    n_updates                 | 2380         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.68e-09    |
|    reward_explained_variance | -0.896       |
|    reward_value_loss         | 9.95         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 240           |
|    time_elapsed              | 66221         |
|    total_timesteps           | 30720         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.047         |
|    cost_value_loss           | 0.000123      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000725     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.64          |
|    mean_cost_advantages      | 2.4881912e-05 |
|    mean_reward_advantages    | 0.036702085   |
|    n_updates                 | 2390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.89e-09     |
|    reward_explained_variance | -1.33         |
|    reward_value_loss         | 9.22          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.61          |
|    ep_rew_mean               | 6.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 241           |
|    time_elapsed              | 66329         |
|    total_timesteps           | 30848         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0243        |
|    cost_value_loss           | 0.000171      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000727     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.49          |
|    mean_cost_advantages      | -0.0012346267 |
|    mean_reward_advantages    | 1.7846867     |
|    n_updates                 | 2400          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.04e-09      |
|    reward_explained_variance | -2.04         |
|    reward_value_loss         | 13.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.69           |
|    ep_rew_mean               | 6.65           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 242            |
|    time_elapsed              | 66435          |
|    total_timesteps           | 30976          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0274         |
|    cost_value_loss           | 0.000102       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000731      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.6            |
|    mean_cost_advantages      | -0.00081011676 |
|    mean_reward_advantages    | -0.92902356    |
|    n_updates                 | 2410           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.02e-09      |
|    reward_explained_variance | -0.551         |
|    reward_value_loss         | 7.74           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.59         |
|    ep_rew_mean               | 6.55         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 243          |
|    time_elapsed              | 66538        |
|    total_timesteps           | 31104        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0163       |
|    cost_value_loss           | 8.32e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.02         |
|    mean_cost_advantages      | 0.0004438334 |
|    mean_reward_advantages    | -0.33547014  |
|    n_updates                 | 2420         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.42e-09    |
|    reward_explained_variance | -0.777       |
|    reward_value_loss         | 10.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.56         |
|    ep_rew_mean               | 6.53         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 244          |
|    time_elapsed              | 66642        |
|    total_timesteps           | 31232        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000911    |
|    cost_value_loss           | 0.000123     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.01         |
|    mean_cost_advantages      | 0.0006808929 |
|    mean_reward_advantages    | -0.23407176  |
|    n_updates                 | 2430         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.46e-09     |
|    reward_explained_variance | -1.64        |
|    reward_value_loss         | 8.73         |
|    total_cost                | 0.0          |
-----------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 7.55            |
|    ep_rew_mean               | 6.51            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 245             |
|    time_elapsed              | 66744           |
|    total_timesteps           | 31360           |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | -0.000679       |
|    cost_value_loss           | 8.49e-05        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -0.000716       |
|    learning_rate             | 0.0005          |
|    loss                      | 2.56            |
|    mean_cost_advantages      | -0.000116324285 |
|    mean_reward_advantages    | -0.58693314     |
|    n_updates                 | 2440            |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | -9.24e-10       |
|    reward_explained_variance | -0.819          |
|    reward_value_loss         | 6.26            |
|    total_cost                | 0.0             |
--------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.19          |
|    ep_rew_mean               | 6.16          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 246           |
|    time_elapsed              | 66851         |
|    total_timesteps           | 31488         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00274      |
|    cost_value_loss           | 0.000126      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000728     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.64          |
|    mean_cost_advantages      | 0.00043294978 |
|    mean_reward_advantages    | 1.1217011     |
|    n_updates                 | 2450          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.36e-09      |
|    reward_explained_variance | -1.33         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.56         |
|    ep_rew_mean               | 6.52         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 247          |
|    time_elapsed              | 66952        |
|    total_timesteps           | 31616        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0284       |
|    cost_value_loss           | 0.000151     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000722    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.24         |
|    mean_cost_advantages      | 0.0021743425 |
|    mean_reward_advantages    | 0.36642542   |
|    n_updates                 | 2460         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.56e-09     |
|    reward_explained_variance | -0.696       |
|    reward_value_loss         | 9.03         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.4           |
|    ep_rew_mean               | 6.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 248           |
|    time_elapsed              | 67055         |
|    total_timesteps           | 31744         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0057        |
|    cost_value_loss           | 0.00011       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.39          |
|    mean_cost_advantages      | -0.0014566268 |
|    mean_reward_advantages    | -0.08033084   |
|    n_updates                 | 2470          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.07e-09      |
|    reward_explained_variance | -0.547        |
|    reward_value_loss         | 6.97          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.62          |
|    ep_rew_mean               | 6.58          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 249           |
|    time_elapsed              | 67161         |
|    total_timesteps           | 31872         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00189      |
|    cost_value_loss           | 0.000106      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.06          |
|    mean_cost_advantages      | 0.00032277382 |
|    mean_reward_advantages    | 0.2522471     |
|    n_updates                 | 2480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.6e-09      |
|    reward_explained_variance | -1.62         |
|    reward_value_loss         | 8.56          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.06           |
|    ep_rew_mean               | 7.01           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 250            |
|    time_elapsed              | 67266          |
|    total_timesteps           | 32000          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0399        |
|    cost_value_loss           | 9.53e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000731      |
|    learning_rate             | 0.0005         |
|    loss                      | 3              |
|    mean_cost_advantages      | -4.1493797e-05 |
|    mean_reward_advantages    | 0.25588962     |
|    n_updates                 | 2490           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.8e-11       |
|    reward_explained_variance | -1.53          |
|    reward_value_loss         | 9.68           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.67          |
|    ep_rew_mean               | 6.63          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 251           |
|    time_elapsed              | 67370         |
|    total_timesteps           | 32128         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0174        |
|    cost_value_loss           | 0.000115      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00071      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.48          |
|    mean_cost_advantages      | -0.0026671097 |
|    mean_reward_advantages    | 0.99137974    |
|    n_updates                 | 2500          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.41e-09      |
|    reward_explained_variance | -0.986        |
|    reward_value_loss         | 8.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.14         |
|    ep_rew_mean               | 6.11         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 252          |
|    time_elapsed              | 67477        |
|    total_timesteps           | 32256        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0186       |
|    cost_value_loss           | 7.55e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000722    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.31         |
|    mean_cost_advantages      | 0.0031842212 |
|    mean_reward_advantages    | -0.7233733   |
|    n_updates                 | 2510         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.77e-09    |
|    reward_explained_variance | -0.877       |
|    reward_value_loss         | 9.18         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.3          |
|    ep_rew_mean               | 6.27         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 253          |
|    time_elapsed              | 67581        |
|    total_timesteps           | 32384        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0256      |
|    cost_value_loss           | 0.000105     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000699    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.51         |
|    mean_cost_advantages      | -0.001544125 |
|    mean_reward_advantages    | 0.10414855   |
|    n_updates                 | 2520         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.16e-10    |
|    reward_explained_variance | -1.2         |
|    reward_value_loss         | 12.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.61          |
|    ep_rew_mean               | 6.58          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 254           |
|    time_elapsed              | 67688         |
|    total_timesteps           | 32512         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0269        |
|    cost_value_loss           | 0.00016       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000711     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.75          |
|    mean_cost_advantages      | -0.0028357178 |
|    mean_reward_advantages    | 1.4805458     |
|    n_updates                 | 2530          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.76e-10      |
|    reward_explained_variance | -1.81         |
|    reward_value_loss         | 14.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.39         |
|    ep_rew_mean               | 6.36         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 255          |
|    time_elapsed              | 67793        |
|    total_timesteps           | 32640        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0287       |
|    cost_value_loss           | 0.000111     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000706    |
|    learning_rate             | 0.0005       |
|    loss                      | 8.23         |
|    mean_cost_advantages      | 0.0014710207 |
|    mean_reward_advantages    | 0.13674036   |
|    n_updates                 | 2540         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.78e-09     |
|    reward_explained_variance | -8.97        |
|    reward_value_loss         | 16.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.16         |
|    ep_rew_mean               | 6.13         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 256          |
|    time_elapsed              | 67900        |
|    total_timesteps           | 32768        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00631     |
|    cost_value_loss           | 9.89e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000729    |
|    learning_rate             | 0.0005       |
|    loss                      | 7.59         |
|    mean_cost_advantages      | 0.0016795776 |
|    mean_reward_advantages    | -1.092109    |
|    n_updates                 | 2550         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.12e-09     |
|    reward_explained_variance | -3.66        |
|    reward_value_loss         | 15.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.09          |
|    ep_rew_mean               | 6.06          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 257           |
|    time_elapsed              | 68005         |
|    total_timesteps           | 32896         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0155       |
|    cost_value_loss           | 0.000136      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000719     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.64          |
|    mean_cost_advantages      | -0.0006017019 |
|    mean_reward_advantages    | -0.16529666   |
|    n_updates                 | 2560          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4e-09        |
|    reward_explained_variance | -0.9          |
|    reward_value_loss         | 9.31          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.3          |
|    ep_rew_mean               | 6.27         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 258          |
|    time_elapsed              | 68110        |
|    total_timesteps           | 33024        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0086       |
|    cost_value_loss           | 0.000112     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000737    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.32         |
|    mean_cost_advantages      | 0.0014061603 |
|    mean_reward_advantages    | -0.288943    |
|    n_updates                 | 2570         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.18e-09    |
|    reward_explained_variance | -4.56        |
|    reward_value_loss         | 13           |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.45         |
|    ep_rew_mean               | 6.42         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 259          |
|    time_elapsed              | 68212        |
|    total_timesteps           | 33152        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0242       |
|    cost_value_loss           | 0.000139     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00071     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.27         |
|    mean_cost_advantages      | 0.0006418741 |
|    mean_reward_advantages    | 0.6592007    |
|    n_updates                 | 2580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.49e-09    |
|    reward_explained_variance | -2.66        |
|    reward_value_loss         | 12.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.88          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 260           |
|    time_elapsed              | 68312         |
|    total_timesteps           | 33280         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0387        |
|    cost_value_loss           | 0.000102      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000701     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.37          |
|    mean_cost_advantages      | -0.0013159332 |
|    mean_reward_advantages    | -0.10299274   |
|    n_updates                 | 2590          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.27e-10      |
|    reward_explained_variance | -3.59         |
|    reward_value_loss         | 13.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.88         |
|    ep_rew_mean               | 5.86         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 261          |
|    time_elapsed              | 68411        |
|    total_timesteps           | 33408        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.333       |
|    cost_value_loss           | 0.000163     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000731    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.43         |
|    mean_cost_advantages      | 0.0021662437 |
|    mean_reward_advantages    | 0.17506471   |
|    n_updates                 | 2600         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.41e-10    |
|    reward_explained_variance | -0.714       |
|    reward_value_loss         | 11.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.21         |
|    ep_rew_mean               | 6.18         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 262          |
|    time_elapsed              | 68511        |
|    total_timesteps           | 33536        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0778      |
|    cost_value_loss           | 0.000108     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000715    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.86         |
|    mean_cost_advantages      | -0.001744786 |
|    mean_reward_advantages    | -0.9044289   |
|    n_updates                 | 2610         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.11e-09     |
|    reward_explained_variance | 0.269        |
|    reward_value_loss         | 5.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.21          |
|    ep_rew_mean               | 6.18          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 263           |
|    time_elapsed              | 68608         |
|    total_timesteps           | 33664         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0153        |
|    cost_value_loss           | 0.000159      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.3           |
|    mean_cost_advantages      | 0.00024638372 |
|    mean_reward_advantages    | 0.82654035    |
|    n_updates                 | 2620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.4e-09       |
|    reward_explained_variance | -0.616        |
|    reward_value_loss         | 9.59          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.58         |
|    ep_rew_mean               | 6.54         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 264          |
|    time_elapsed              | 68711        |
|    total_timesteps           | 33792        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0139      |
|    cost_value_loss           | 0.000141     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000726    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.43         |
|    mean_cost_advantages      | 0.0006628597 |
|    mean_reward_advantages    | -0.34031522  |
|    n_updates                 | 2630         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.99e-10     |
|    reward_explained_variance | -0.354       |
|    reward_value_loss         | 6.49         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.59        |
|    ep_rew_mean               | 6.56        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 265         |
|    time_elapsed              | 68810       |
|    total_timesteps           | 33920       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0888     |
|    cost_value_loss           | 0.000142    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000734   |
|    learning_rate             | 0.0005      |
|    loss                      | 3.67        |
|    mean_cost_advantages      | 0.001511477 |
|    mean_reward_advantages    | 0.6643032   |
|    n_updates                 | 2640        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 6.7e-10     |
|    reward_explained_variance | -1.07       |
|    reward_value_loss         | 8.21        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.81          |
|    ep_rew_mean               | 6.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 266           |
|    time_elapsed              | 68906         |
|    total_timesteps           | 34048         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0045       |
|    cost_value_loss           | 0.000107      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000717     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.93          |
|    mean_cost_advantages      | 0.00061534037 |
|    mean_reward_advantages    | 0.41391686    |
|    n_updates                 | 2650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.58e-10      |
|    reward_explained_variance | -0.637        |
|    reward_value_loss         | 11.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.95          |
|    ep_rew_mean               | 6.92          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 267           |
|    time_elapsed              | 69005         |
|    total_timesteps           | 34176         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00709       |
|    cost_value_loss           | 7.71e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.51          |
|    mean_cost_advantages      | -0.0015259614 |
|    mean_reward_advantages    | 0.21592416    |
|    n_updates                 | 2660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.87e-09     |
|    reward_explained_variance | -0.813        |
|    reward_value_loss         | 13.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.78          |
|    ep_rew_mean               | 6.74          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 268           |
|    time_elapsed              | 69103         |
|    total_timesteps           | 34304         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0209        |
|    cost_value_loss           | 0.000151      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000699     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.43          |
|    mean_cost_advantages      | -0.0014136499 |
|    mean_reward_advantages    | 0.23967811    |
|    n_updates                 | 2670          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.59e-09     |
|    reward_explained_variance | -2.2          |
|    reward_value_loss         | 14.6          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.82         |
|    ep_rew_mean               | 6.78         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 269          |
|    time_elapsed              | 69199        |
|    total_timesteps           | 34432        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.016       |
|    cost_value_loss           | 0.000178     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000747    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.2          |
|    mean_cost_advantages      | 0.0031638579 |
|    mean_reward_advantages    | 0.67527586   |
|    n_updates                 | 2680         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.29e-09     |
|    reward_explained_variance | -1.21        |
|    reward_value_loss         | 8.48         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.73          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 270           |
|    time_elapsed              | 69296         |
|    total_timesteps           | 34560         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0246        |
|    cost_value_loss           | 0.000132      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000713     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.13          |
|    mean_cost_advantages      | -0.0038927523 |
|    mean_reward_advantages    | 1.689137      |
|    n_updates                 | 2690          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.02e-10      |
|    reward_explained_variance | -1.59         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.61         |
|    ep_rew_mean               | 6.58         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 271          |
|    time_elapsed              | 69392        |
|    total_timesteps           | 34688        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0614       |
|    cost_value_loss           | 0.000129     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000708    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.45         |
|    mean_cost_advantages      | 0.0019300579 |
|    mean_reward_advantages    | 0.8665325    |
|    n_updates                 | 2700         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.69e-10    |
|    reward_explained_variance | -1.69        |
|    reward_value_loss         | 12.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.49         |
|    ep_rew_mean               | 6.46         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 272          |
|    time_elapsed              | 69491        |
|    total_timesteps           | 34816        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0216      |
|    cost_value_loss           | 0.000158     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000705    |
|    learning_rate             | 0.0005       |
|    loss                      | 12.3         |
|    mean_cost_advantages      | 0.0001363603 |
|    mean_reward_advantages    | 1.3110162    |
|    n_updates                 | 2710         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.98e-11    |
|    reward_explained_variance | -6.36        |
|    reward_value_loss         | 25.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.3          |
|    ep_rew_mean               | 6.26         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 273          |
|    time_elapsed              | 69589        |
|    total_timesteps           | 34944        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0469      |
|    cost_value_loss           | 0.000117     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000735    |
|    learning_rate             | 0.0005       |
|    loss                      | 7.38         |
|    mean_cost_advantages      | 0.0009891916 |
|    mean_reward_advantages    | 0.08099744   |
|    n_updates                 | 2720         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.06e-09    |
|    reward_explained_variance | -0.662       |
|    reward_value_loss         | 14.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.95         |
|    ep_rew_mean               | 5.92         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 274          |
|    time_elapsed              | 69687        |
|    total_timesteps           | 35072        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0351      |
|    cost_value_loss           | 9.58e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000725    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.36         |
|    mean_cost_advantages      | -0.001599217 |
|    mean_reward_advantages    | -0.33851206  |
|    n_updates                 | 2730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.05e-08     |
|    reward_explained_variance | -0.382       |
|    reward_value_loss         | 6.7          |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.17         |
|    ep_rew_mean               | 6.14         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 275          |
|    time_elapsed              | 69786        |
|    total_timesteps           | 35200        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0241       |
|    cost_value_loss           | 0.00011      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.26         |
|    mean_cost_advantages      | 0.0016179888 |
|    mean_reward_advantages    | -0.562473    |
|    n_updates                 | 2740         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.81e-09    |
|    reward_explained_variance | -0.179       |
|    reward_value_loss         | 7.66         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.65          |
|    ep_rew_mean               | 5.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 276           |
|    time_elapsed              | 69883         |
|    total_timesteps           | 35328         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0214        |
|    cost_value_loss           | 0.000117      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000719     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.73          |
|    mean_cost_advantages      | 0.00021054363 |
|    mean_reward_advantages    | 1.0961761     |
|    n_updates                 | 2750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.76e-10     |
|    reward_explained_variance | -2.44         |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.68          |
|    ep_rew_mean               | 5.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 277           |
|    time_elapsed              | 69982         |
|    total_timesteps           | 35456         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00586       |
|    cost_value_loss           | 0.000111      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000711     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.28          |
|    mean_cost_advantages      | -0.0012796568 |
|    mean_reward_advantages    | -0.07443558   |
|    n_updates                 | 2760          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.1e-09       |
|    reward_explained_variance | -2.51         |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.52         |
|    ep_rew_mean               | 5.49         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 278          |
|    time_elapsed              | 70079        |
|    total_timesteps           | 35584        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0812       |
|    cost_value_loss           | 0.000114     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000706    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.97         |
|    mean_cost_advantages      | -0.000761808 |
|    mean_reward_advantages    | -0.36409438  |
|    n_updates                 | 2770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.29e-09     |
|    reward_explained_variance | -0.0992      |
|    reward_value_loss         | 9.24         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.77        |
|    ep_rew_mean               | 5.74        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 279         |
|    time_elapsed              | 70178       |
|    total_timesteps           | 35712       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.012       |
|    cost_value_loss           | 7.25e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000724   |
|    learning_rate             | 0.0005      |
|    loss                      | 2.75        |
|    mean_cost_advantages      | 0.002254323 |
|    mean_reward_advantages    | -0.4986279  |
|    n_updates                 | 2780        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 2.9e-09     |
|    reward_explained_variance | -1.07       |
|    reward_value_loss         | 6.29        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.68           |
|    ep_rew_mean               | 5.66           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 280            |
|    time_elapsed              | 70276          |
|    total_timesteps           | 35840          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0623        |
|    cost_value_loss           | 0.000127       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00071       |
|    learning_rate             | 0.0005         |
|    loss                      | 7.89           |
|    mean_cost_advantages      | -0.00022292713 |
|    mean_reward_advantages    | 1.6551564      |
|    n_updates                 | 2790           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.8e-09        |
|    reward_explained_variance | -5.34          |
|    reward_value_loss         | 18.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.46          |
|    ep_rew_mean               | 5.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 281           |
|    time_elapsed              | 70378         |
|    total_timesteps           | 35968         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0058        |
|    cost_value_loss           | 0.000116      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000703     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.29          |
|    mean_cost_advantages      | -0.0013421986 |
|    mean_reward_advantages    | -1.6615951    |
|    n_updates                 | 2800          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.15e-10      |
|    reward_explained_variance | -0.282        |
|    reward_value_loss         | 10.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.3          |
|    ep_rew_mean               | 5.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 282          |
|    time_elapsed              | 70475        |
|    total_timesteps           | 36096        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00761     |
|    cost_value_loss           | 9.52e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000722    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.74         |
|    mean_cost_advantages      | 0.0013291053 |
|    mean_reward_advantages    | 0.3299499    |
|    n_updates                 | 2810         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.19e-09     |
|    reward_explained_variance | -1.01        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.47          |
|    ep_rew_mean               | 5.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 283           |
|    time_elapsed              | 70575         |
|    total_timesteps           | 36224         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00577       |
|    cost_value_loss           | 0.000105      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.32          |
|    mean_cost_advantages      | -0.0007583052 |
|    mean_reward_advantages    | -0.6801245    |
|    n_updates                 | 2820          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.22e-09      |
|    reward_explained_variance | -0.323        |
|    reward_value_loss         | 5.3           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.47           |
|    ep_rew_mean               | 5.45           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 284            |
|    time_elapsed              | 70672          |
|    total_timesteps           | 36352          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.05           |
|    cost_value_loss           | 0.000109       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000733      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.02           |
|    mean_cost_advantages      | -0.00079471536 |
|    mean_reward_advantages    | 1.6237031      |
|    n_updates                 | 2830           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.71e-09      |
|    reward_explained_variance | -5.14          |
|    reward_value_loss         | 16.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.98          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 285           |
|    time_elapsed              | 70741         |
|    total_timesteps           | 36480         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0291        |
|    cost_value_loss           | 0.000119      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000703     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.91          |
|    mean_cost_advantages      | 0.00012283199 |
|    mean_reward_advantages    | -0.014491335  |
|    n_updates                 | 2840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.54e-10      |
|    reward_explained_variance | -0.284        |
|    reward_value_loss         | 6.24          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 286           |
|    time_elapsed              | 70807         |
|    total_timesteps           | 36608         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00153       |
|    cost_value_loss           | 0.000119      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.62          |
|    mean_cost_advantages      | -0.0010501994 |
|    mean_reward_advantages    | 1.3864855     |
|    n_updates                 | 2850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.16e-09     |
|    reward_explained_variance | -2.83         |
|    reward_value_loss         | 13.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.14          |
|    ep_rew_mean               | 7.11          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 287           |
|    time_elapsed              | 70868         |
|    total_timesteps           | 36736         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.189        |
|    cost_value_loss           | 0.000126      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000707     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.68          |
|    mean_cost_advantages      | 0.00033248294 |
|    mean_reward_advantages    | 1.8568273     |
|    n_updates                 | 2860          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.01e-09      |
|    reward_explained_variance | -3.27         |
|    reward_value_loss         | 14.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.33          |
|    ep_rew_mean               | 7.29          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 288           |
|    time_elapsed              | 70930         |
|    total_timesteps           | 36864         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0434        |
|    cost_value_loss           | 0.00014       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000737     |
|    learning_rate             | 0.0005        |
|    loss                      | 9.1           |
|    mean_cost_advantages      | -0.0011298505 |
|    mean_reward_advantages    | 0.39024985    |
|    n_updates                 | 2870          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.59e-09      |
|    reward_explained_variance | -9.01         |
|    reward_value_loss         | 20.2          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 8.47        |
|    ep_rew_mean               | 7.42        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 289         |
|    time_elapsed              | 70991       |
|    total_timesteps           | 36992       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0203     |
|    cost_value_loss           | 0.000129    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000738   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.45        |
|    mean_cost_advantages      | 0.002297804 |
|    mean_reward_advantages    | -2.1654289  |
|    n_updates                 | 2880        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -6.64e-09   |
|    reward_explained_variance | -0.907      |
|    reward_value_loss         | 10.9        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.61         |
|    ep_rew_mean               | 7.56         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 290          |
|    time_elapsed              | 71052        |
|    total_timesteps           | 37120        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0434      |
|    cost_value_loss           | 0.000106     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00074     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.25         |
|    mean_cost_advantages      | 0.0011824935 |
|    mean_reward_advantages    | 0.9836723    |
|    n_updates                 | 2890         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.77e-10    |
|    reward_explained_variance | -1.41        |
|    reward_value_loss         | 11           |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.25           |
|    ep_rew_mean               | 7.21           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 291            |
|    time_elapsed              | 71114          |
|    total_timesteps           | 37248          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.101          |
|    cost_value_loss           | 0.000102       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000725      |
|    learning_rate             | 0.0005         |
|    loss                      | 8.57           |
|    mean_cost_advantages      | -0.00093856105 |
|    mean_reward_advantages    | 0.03563639     |
|    n_updates                 | 2900           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.44e-10      |
|    reward_explained_variance | -1.64          |
|    reward_value_loss         | 19.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.59          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 292           |
|    time_elapsed              | 71175         |
|    total_timesteps           | 37376         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0931       |
|    cost_value_loss           | 7.91e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000707     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.9           |
|    mean_cost_advantages      | -0.0012354976 |
|    mean_reward_advantages    | -3.0279326    |
|    n_updates                 | 2910          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.13e-09      |
|    reward_explained_variance | -0.11         |
|    reward_value_loss         | 11            |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.22         |
|    ep_rew_mean               | 6.18         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 293          |
|    time_elapsed              | 71236        |
|    total_timesteps           | 37504        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0352       |
|    cost_value_loss           | 9.2e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000714    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.39         |
|    mean_cost_advantages      | 0.0014497861 |
|    mean_reward_advantages    | 1.6263207    |
|    n_updates                 | 2920         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.43e-09    |
|    reward_explained_variance | -2.99        |
|    reward_value_loss         | 14.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.45          |
|    ep_rew_mean               | 6.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 294           |
|    time_elapsed              | 71297         |
|    total_timesteps           | 37632         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00149       |
|    cost_value_loss           | 0.000113      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000733     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.12          |
|    mean_cost_advantages      | 0.00017305481 |
|    mean_reward_advantages    | 0.76838076    |
|    n_updates                 | 2930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.28e-09      |
|    reward_explained_variance | -1.82         |
|    reward_value_loss         | 12.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.16          |
|    ep_rew_mean               | 6.12          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 295           |
|    time_elapsed              | 71358         |
|    total_timesteps           | 37760         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000884     |
|    cost_value_loss           | 0.00012       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000717     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.37          |
|    mean_cost_advantages      | -0.0016504404 |
|    mean_reward_advantages    | -0.8842449    |
|    n_updates                 | 2940          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.78e-09      |
|    reward_explained_variance | -2.56         |
|    reward_value_loss         | 11.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.13         |
|    ep_rew_mean               | 6.09         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 296          |
|    time_elapsed              | 71420        |
|    total_timesteps           | 37888        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0785       |
|    cost_value_loss           | 0.000141     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000731    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.47         |
|    mean_cost_advantages      | 0.0027249916 |
|    mean_reward_advantages    | -0.068540044 |
|    n_updates                 | 2950         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.45e-09    |
|    reward_explained_variance | -1.34        |
|    reward_value_loss         | 9.56         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 297           |
|    time_elapsed              | 71482         |
|    total_timesteps           | 38016         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0217        |
|    cost_value_loss           | 9.06e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000727     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.73          |
|    mean_cost_advantages      | -0.0010463782 |
|    mean_reward_advantages    | -0.4033473    |
|    n_updates                 | 2960          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.22e-10      |
|    reward_explained_variance | -1.08         |
|    reward_value_loss         | 6.33          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.02           |
|    ep_rew_mean               | 5.97           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 298            |
|    time_elapsed              | 71543          |
|    total_timesteps           | 38144          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00936       |
|    cost_value_loss           | 0.000103       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000711      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.07           |
|    mean_cost_advantages      | -0.00083224045 |
|    mean_reward_advantages    | 0.91533774     |
|    n_updates                 | 2970           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.48e-09      |
|    reward_explained_variance | -2.68          |
|    reward_value_loss         | 9.76           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.94         |
|    ep_rew_mean               | 5.9          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 299          |
|    time_elapsed              | 71605        |
|    total_timesteps           | 38272        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00719      |
|    cost_value_loss           | 7.92e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000731    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.95         |
|    mean_cost_advantages      | 0.0020250995 |
|    mean_reward_advantages    | -1.0001338   |
|    n_updates                 | 2980         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.8e-09      |
|    reward_explained_variance | -0.0642      |
|    reward_value_loss         | 5.21         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.6           |
|    ep_rew_mean               | 5.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 300           |
|    time_elapsed              | 71666         |
|    total_timesteps           | 38400         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.319        |
|    cost_value_loss           | 0.000135      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.58          |
|    mean_cost_advantages      | -0.0037246319 |
|    mean_reward_advantages    | 0.96062595    |
|    n_updates                 | 2990          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.03e-10      |
|    reward_explained_variance | -3.77         |
|    reward_value_loss         | 9.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.79         |
|    ep_rew_mean               | 5.75         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 301          |
|    time_elapsed              | 71728        |
|    total_timesteps           | 38528        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00833     |
|    cost_value_loss           | 0.000106     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000721    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.49         |
|    mean_cost_advantages      | 0.0015267308 |
|    mean_reward_advantages    | -0.7971671   |
|    n_updates                 | 3000         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.41e-09    |
|    reward_explained_variance | -0.483       |
|    reward_value_loss         | 6.66         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.39           |
|    ep_rew_mean               | 5.36           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 302            |
|    time_elapsed              | 71789          |
|    total_timesteps           | 38656          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0444         |
|    cost_value_loss           | 0.00013        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000713      |
|    learning_rate             | 0.0005         |
|    loss                      | 8.93           |
|    mean_cost_advantages      | -0.00023364808 |
|    mean_reward_advantages    | 1.4917026      |
|    n_updates                 | 3010           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.68e-09      |
|    reward_explained_variance | -4.87          |
|    reward_value_loss         | 19.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.84          |
|    ep_rew_mean               | 5.8           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 303           |
|    time_elapsed              | 71851         |
|    total_timesteps           | 38784         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00464      |
|    cost_value_loss           | 0.000106      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.44          |
|    mean_cost_advantages      | 3.3404896e-05 |
|    mean_reward_advantages    | -0.7977171    |
|    n_updates                 | 3020          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.46e-09      |
|    reward_explained_variance | -2            |
|    reward_value_loss         | 20            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.55          |
|    ep_rew_mean               | 5.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 304           |
|    time_elapsed              | 71912         |
|    total_timesteps           | 38912         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0256        |
|    cost_value_loss           | 0.000202      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000726     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.64          |
|    mean_cost_advantages      | -0.0022907918 |
|    mean_reward_advantages    | 0.43409547    |
|    n_updates                 | 3030          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.25e-09      |
|    reward_explained_variance | -2.31         |
|    reward_value_loss         | 9.1           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.69         |
|    ep_rew_mean               | 5.66         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 305          |
|    time_elapsed              | 71973        |
|    total_timesteps           | 39040        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0363      |
|    cost_value_loss           | 0.000121     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000721    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.69         |
|    mean_cost_advantages      | 0.0036471332 |
|    mean_reward_advantages    | -0.40398717  |
|    n_updates                 | 3040         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.03e-09     |
|    reward_explained_variance | -1.47        |
|    reward_value_loss         | 8.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.02          |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 306           |
|    time_elapsed              | 72035         |
|    total_timesteps           | 39168         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0217       |
|    cost_value_loss           | 9.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000708     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.97          |
|    mean_cost_advantages      | -0.0009342872 |
|    mean_reward_advantages    | 0.39224148    |
|    n_updates                 | 3050          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.68e-09     |
|    reward_explained_variance | -1.27         |
|    reward_value_loss         | 8.49          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.37           |
|    ep_rew_mean               | 6.33           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 307            |
|    time_elapsed              | 72096          |
|    total_timesteps           | 39296          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00772       |
|    cost_value_loss           | 0.000134       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000727      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.58           |
|    mean_cost_advantages      | -0.00041061235 |
|    mean_reward_advantages    | 1.3081588      |
|    n_updates                 | 3060           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.2e-09        |
|    reward_explained_variance | -1.32          |
|    reward_value_loss         | 8.86           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.49          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 308           |
|    time_elapsed              | 72157         |
|    total_timesteps           | 39424         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.018        |
|    cost_value_loss           | 0.000121      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.51          |
|    mean_cost_advantages      | 0.00011923496 |
|    mean_reward_advantages    | -0.13928261   |
|    n_updates                 | 3070          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.17e-09      |
|    reward_explained_variance | -1.45         |
|    reward_value_loss         | 7.65          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.35           |
|    ep_rew_mean               | 6.31           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 309            |
|    time_elapsed              | 72219          |
|    total_timesteps           | 39552          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.000436      |
|    cost_value_loss           | 0.00012        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000743      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.54           |
|    mean_cost_advantages      | -0.00022968436 |
|    mean_reward_advantages    | 0.27360332     |
|    n_updates                 | 3080           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.17e-09       |
|    reward_explained_variance | -1.54          |
|    reward_value_loss         | 10.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.28           |
|    ep_rew_mean               | 6.25           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 310            |
|    time_elapsed              | 72280          |
|    total_timesteps           | 39680          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0581         |
|    cost_value_loss           | 8.92e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000712      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.59           |
|    mean_cost_advantages      | -0.00083862233 |
|    mean_reward_advantages    | -0.8405478     |
|    n_updates                 | 3090           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.48e-09       |
|    reward_explained_variance | -2.09          |
|    reward_value_loss         | 11.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.86         |
|    ep_rew_mean               | 6.82         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 311          |
|    time_elapsed              | 72341        |
|    total_timesteps           | 39808        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0337       |
|    cost_value_loss           | 0.000106     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000711    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.01         |
|    mean_cost_advantages      | 0.0014091823 |
|    mean_reward_advantages    | 0.53761405   |
|    n_updates                 | 3100         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 9.15e-10     |
|    reward_explained_variance | -1.98        |
|    reward_value_loss         | 9.18         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.19          |
|    ep_rew_mean               | 7.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 312           |
|    time_elapsed              | 72402         |
|    total_timesteps           | 39936         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0545       |
|    cost_value_loss           | 0.000149      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000739     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.97          |
|    mean_cost_advantages      | -0.0014385389 |
|    mean_reward_advantages    | 0.405251      |
|    n_updates                 | 3110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.8e-09       |
|    reward_explained_variance | -1.7          |
|    reward_value_loss         | 6.8           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.34         |
|    ep_rew_mean               | 7.29         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 313          |
|    time_elapsed              | 72463        |
|    total_timesteps           | 40064        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000463    |
|    cost_value_loss           | 0.000118     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000728    |
|    learning_rate             | 0.0005       |
|    loss                      | 7.67         |
|    mean_cost_advantages      | 0.0004037828 |
|    mean_reward_advantages    | 1.4726441    |
|    n_updates                 | 3120         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.96e-09     |
|    reward_explained_variance | -5.13        |
|    reward_value_loss         | 16.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.93          |
|    ep_rew_mean               | 6.89          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 314           |
|    time_elapsed              | 72524         |
|    total_timesteps           | 40192         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0337        |
|    cost_value_loss           | 0.000101      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000699     |
|    learning_rate             | 0.0005        |
|    loss                      | 9.37          |
|    mean_cost_advantages      | -0.0014211049 |
|    mean_reward_advantages    | 1.3886367     |
|    n_updates                 | 3130          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.48e-10      |
|    reward_explained_variance | -6.84         |
|    reward_value_loss         | 21.1          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 8.07        |
|    ep_rew_mean               | 7.03        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 315         |
|    time_elapsed              | 72586       |
|    total_timesteps           | 40320       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0163      |
|    cost_value_loss           | 9.96e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000704   |
|    learning_rate             | 0.0005      |
|    loss                      | 5.65        |
|    mean_cost_advantages      | 0.001248149 |
|    mean_reward_advantages    | -2.5103295  |
|    n_updates                 | 3140        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 4.9e-09     |
|    reward_explained_variance | -1.67       |
|    reward_value_loss         | 13.9        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.66          |
|    ep_rew_mean               | 6.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 316           |
|    time_elapsed              | 72647         |
|    total_timesteps           | 40448         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0169        |
|    cost_value_loss           | 0.000131      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000718     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.48          |
|    mean_cost_advantages      | 0.00034393248 |
|    mean_reward_advantages    | 0.35438794    |
|    n_updates                 | 3150          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.16e-09     |
|    reward_explained_variance | -0.645        |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.92         |
|    ep_rew_mean               | 5.9          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 317          |
|    time_elapsed              | 72709        |
|    total_timesteps           | 40576        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0273       |
|    cost_value_loss           | 0.00012      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000719    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.88         |
|    mean_cost_advantages      | 0.0013385995 |
|    mean_reward_advantages    | -1.156575    |
|    n_updates                 | 3160         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.21e-10     |
|    reward_explained_variance | -0.742       |
|    reward_value_loss         | 10.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.03          |
|    ep_rew_mean               | 5.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 318           |
|    time_elapsed              | 72771         |
|    total_timesteps           | 40704         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.033         |
|    cost_value_loss           | 9.45e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.28          |
|    mean_cost_advantages      | 0.00013064704 |
|    mean_reward_advantages    | 0.4184628     |
|    n_updates                 | 3170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.73e-09      |
|    reward_explained_variance | -0.277        |
|    reward_value_loss         | 4.57          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.16         |
|    ep_rew_mean               | 5.14         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 319          |
|    time_elapsed              | 72836        |
|    total_timesteps           | 40832        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0509       |
|    cost_value_loss           | 9.06e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000712    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.64         |
|    mean_cost_advantages      | 0.0006395681 |
|    mean_reward_advantages    | -0.06922048  |
|    n_updates                 | 3180         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.19e-09     |
|    reward_explained_variance | -1.52        |
|    reward_value_loss         | 8.51         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.22         |
|    ep_rew_mean               | 5.21         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 320          |
|    time_elapsed              | 72899        |
|    total_timesteps           | 40960        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0095       |
|    cost_value_loss           | 0.000109     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.28         |
|    mean_cost_advantages      | -0.001936036 |
|    mean_reward_advantages    | 0.8628415    |
|    n_updates                 | 3190         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.75e-10    |
|    reward_explained_variance | -0.481       |
|    reward_value_loss         | 9.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.47        |
|    ep_rew_mean               | 5.46        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 321         |
|    time_elapsed              | 72962       |
|    total_timesteps           | 41088       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.00138    |
|    cost_value_loss           | 0.000122    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000706   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.05        |
|    mean_cost_advantages      | 0.001640473 |
|    mean_reward_advantages    | -0.6849776  |
|    n_updates                 | 3200        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -5.05e-10   |
|    reward_explained_variance | -1.43       |
|    reward_value_loss         | 7.99        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.86          |
|    ep_rew_mean               | 5.84          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 322           |
|    time_elapsed              | 73024         |
|    total_timesteps           | 41216         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00693      |
|    cost_value_loss           | 7.6e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000708     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.11          |
|    mean_cost_advantages      | -0.0012714886 |
|    mean_reward_advantages    | -0.6618563    |
|    n_updates                 | 3210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.2e-09      |
|    reward_explained_variance | -0.46         |
|    reward_value_loss         | 5.31          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.8           |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 323           |
|    time_elapsed              | 73087         |
|    total_timesteps           | 41344         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0103        |
|    cost_value_loss           | 9e-05         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000714     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.16          |
|    mean_cost_advantages      | -8.277524e-05 |
|    mean_reward_advantages    | 0.9020889     |
|    n_updates                 | 3220          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.83e-09      |
|    reward_explained_variance | -3.13         |
|    reward_value_loss         | 13.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.12         |
|    ep_rew_mean               | 6.1          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 324          |
|    time_elapsed              | 73151        |
|    total_timesteps           | 41472        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00468      |
|    cost_value_loss           | 6.5e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00071     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.54         |
|    mean_cost_advantages      | 0.0016615015 |
|    mean_reward_advantages    | -0.4237947   |
|    n_updates                 | 3230         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.44e-09    |
|    reward_explained_variance | -4.03        |
|    reward_value_loss         | 14.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.41          |
|    ep_rew_mean               | 6.39          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 325           |
|    time_elapsed              | 73214         |
|    total_timesteps           | 41600         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0561        |
|    cost_value_loss           | 9.83e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000716     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.37          |
|    mean_cost_advantages      | -0.0012282842 |
|    mean_reward_advantages    | -0.20546792   |
|    n_updates                 | 3240          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.69e-09     |
|    reward_explained_variance | -1.48         |
|    reward_value_loss         | 9.43          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.48           |
|    ep_rew_mean               | 6.46           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 326            |
|    time_elapsed              | 73276          |
|    total_timesteps           | 41728          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0264         |
|    cost_value_loss           | 0.000141       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000697      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.61           |
|    mean_cost_advantages      | -0.00078843324 |
|    mean_reward_advantages    | 0.98915845     |
|    n_updates                 | 3250           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.28e-09      |
|    reward_explained_variance | -2.47          |
|    reward_value_loss         | 8.96           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 327           |
|    time_elapsed              | 73339         |
|    total_timesteps           | 41856         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.256        |
|    cost_value_loss           | 0.000101      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000701     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.08          |
|    mean_cost_advantages      | -0.0008299189 |
|    mean_reward_advantages    | 0.017566942   |
|    n_updates                 | 3260          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.36e-09     |
|    reward_explained_variance | -3.11         |
|    reward_value_loss         | 11.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.53         |
|    ep_rew_mean               | 6.51         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 328          |
|    time_elapsed              | 73402        |
|    total_timesteps           | 41984        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0362       |
|    cost_value_loss           | 0.000127     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.24         |
|    mean_cost_advantages      | 0.0025312933 |
|    mean_reward_advantages    | 0.46218288   |
|    n_updates                 | 3270         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.07e-09    |
|    reward_explained_variance | -0.132       |
|    reward_value_loss         | 9.32         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.28           |
|    ep_rew_mean               | 6.26           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 329            |
|    time_elapsed              | 73464          |
|    total_timesteps           | 42112          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0256         |
|    cost_value_loss           | 0.000101       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000717      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.95           |
|    mean_cost_advantages      | -0.00030114243 |
|    mean_reward_advantages    | 1.5866085      |
|    n_updates                 | 3280           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.56e-09       |
|    reward_explained_variance | -1.21          |
|    reward_value_loss         | 16.1           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.78          |
|    ep_rew_mean               | 5.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 330           |
|    time_elapsed              | 73527         |
|    total_timesteps           | 42240         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00416       |
|    cost_value_loss           | 5.93e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0007       |
|    learning_rate             | 0.0005        |
|    loss                      | 3.91          |
|    mean_cost_advantages      | 0.00025789684 |
|    mean_reward_advantages    | -2.632698     |
|    n_updates                 | 3290          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.44e-09     |
|    reward_explained_variance | 0.106         |
|    reward_value_loss         | 9.76          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.74        |
|    ep_rew_mean               | 5.72        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 331         |
|    time_elapsed              | 73590       |
|    total_timesteps           | 42368       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0408      |
|    cost_value_loss           | 9.99e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000715   |
|    learning_rate             | 0.0005      |
|    loss                      | 2.67        |
|    mean_cost_advantages      | 0.001532818 |
|    mean_reward_advantages    | 0.36868784  |
|    n_updates                 | 3300        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -5.14e-10   |
|    reward_explained_variance | -0.887      |
|    reward_value_loss         | 9.88        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.78           |
|    ep_rew_mean               | 5.75           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 332            |
|    time_elapsed              | 73651          |
|    total_timesteps           | 42496          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0309         |
|    cost_value_loss           | 0.000157       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00071       |
|    learning_rate             | 0.0005         |
|    loss                      | 7.95           |
|    mean_cost_advantages      | -2.7898437e-05 |
|    mean_reward_advantages    | 1.3750076      |
|    n_updates                 | 3310           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.09e-09       |
|    reward_explained_variance | -3.35          |
|    reward_value_loss         | 16.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.43          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 333           |
|    time_elapsed              | 73713         |
|    total_timesteps           | 42624         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0218       |
|    cost_value_loss           | 0.000119      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000709     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.02          |
|    mean_cost_advantages      | -0.0008036324 |
|    mean_reward_advantages    | -0.26566082   |
|    n_updates                 | 3320          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.16e-09      |
|    reward_explained_variance | -2.37         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.69          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 334           |
|    time_elapsed              | 73776         |
|    total_timesteps           | 42752         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00971      |
|    cost_value_loss           | 9.91e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.03          |
|    mean_cost_advantages      | 0.00092530815 |
|    mean_reward_advantages    | -2.0924191    |
|    n_updates                 | 3330          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.35e-09     |
|    reward_explained_variance | -0.0606       |
|    reward_value_loss         | 6.54          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.75           |
|    ep_rew_mean               | 5.73           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 335            |
|    time_elapsed              | 73839          |
|    total_timesteps           | 42880          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.074          |
|    cost_value_loss           | 9.41e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000713      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.82           |
|    mean_cost_advantages      | -0.00029279856 |
|    mean_reward_advantages    | 1.8154496      |
|    n_updates                 | 3340           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.01e-09       |
|    reward_explained_variance | -2.93          |
|    reward_value_loss         | 9.84           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.77          |
|    ep_rew_mean               | 5.75          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 336           |
|    time_elapsed              | 73901         |
|    total_timesteps           | 43008         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00893       |
|    cost_value_loss           | 8.59e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000694     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.66          |
|    mean_cost_advantages      | -0.0016237574 |
|    mean_reward_advantages    | 0.45345253    |
|    n_updates                 | 3350          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.16e-10      |
|    reward_explained_variance | -4.23         |
|    reward_value_loss         | 13.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.48          |
|    ep_rew_mean               | 5.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 337           |
|    time_elapsed              | 73964         |
|    total_timesteps           | 43136         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0622       |
|    cost_value_loss           | 8.48e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00072      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.52          |
|    mean_cost_advantages      | 0.00038728386 |
|    mean_reward_advantages    | -0.2488465    |
|    n_updates                 | 3360          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.77e-09      |
|    reward_explained_variance | -1.54         |
|    reward_value_loss         | 7.01          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.78           |
|    ep_rew_mean               | 5.76           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 338            |
|    time_elapsed              | 74026          |
|    total_timesteps           | 43264          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0443         |
|    cost_value_loss           | 8.45e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000703      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.93           |
|    mean_cost_advantages      | -0.00025586822 |
|    mean_reward_advantages    | -0.2568152     |
|    n_updates                 | 3370           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.27e-09       |
|    reward_explained_variance | -0.507         |
|    reward_value_loss         | 6.72           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.74           |
|    ep_rew_mean               | 5.72           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 339            |
|    time_elapsed              | 74089          |
|    total_timesteps           | 43392          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0155         |
|    cost_value_loss           | 0.000114       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000716      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.46           |
|    mean_cost_advantages      | -0.00033642707 |
|    mean_reward_advantages    | 2.4537888      |
|    n_updates                 | 3380           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.08e-09      |
|    reward_explained_variance | -4.29          |
|    reward_value_loss         | 16.9           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.7           |
|    ep_rew_mean               | 5.68          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 340           |
|    time_elapsed              | 74152         |
|    total_timesteps           | 43520         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0584        |
|    cost_value_loss           | 9.3e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000708     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.14          |
|    mean_cost_advantages      | 0.00047865455 |
|    mean_reward_advantages    | -1.3237274    |
|    n_updates                 | 3390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.49e-09      |
|    reward_explained_variance | -2.13         |
|    reward_value_loss         | 9.61          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.99        |
|    ep_rew_mean               | 5.97        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 341         |
|    time_elapsed              | 74214       |
|    total_timesteps           | 43648       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.00548    |
|    cost_value_loss           | 0.000109    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000706   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.12        |
|    mean_cost_advantages      | 0.00070203  |
|    mean_reward_advantages    | -0.15315688 |
|    n_updates                 | 3400        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.55e-09   |
|    reward_explained_variance | -1.82       |
|    reward_value_loss         | 6.93        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.98         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 342          |
|    time_elapsed              | 74277        |
|    total_timesteps           | 43776        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0412      |
|    cost_value_loss           | 0.00011      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000714    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.26         |
|    mean_cost_advantages      | 0.0008532299 |
|    mean_reward_advantages    | 0.94208175   |
|    n_updates                 | 3410         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.66e-09    |
|    reward_explained_variance | -1.95        |
|    reward_value_loss         | 9.12         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.68          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 343           |
|    time_elapsed              | 74339         |
|    total_timesteps           | 43904         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00548      |
|    cost_value_loss           | 0.000117      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000713     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.86          |
|    mean_cost_advantages      | -0.0027398716 |
|    mean_reward_advantages    | -0.6970256    |
|    n_updates                 | 3420          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.48e-10      |
|    reward_explained_variance | -1.73         |
|    reward_value_loss         | 9.37          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.94         |
|    ep_rew_mean               | 5.91         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 344          |
|    time_elapsed              | 74402        |
|    total_timesteps           | 44032        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0292      |
|    cost_value_loss           | 9.2e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.0007      |
|    learning_rate             | 0.0005       |
|    loss                      | 4.09         |
|    mean_cost_advantages      | 0.0036661713 |
|    mean_reward_advantages    | -0.15132338  |
|    n_updates                 | 3430         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.76e-09    |
|    reward_explained_variance | -0.825       |
|    reward_value_loss         | 8.86         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 345           |
|    time_elapsed              | 74464         |
|    total_timesteps           | 44160         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.029        |
|    cost_value_loss           | 0.000163      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000745     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.2           |
|    mean_cost_advantages      | -0.0008225208 |
|    mean_reward_advantages    | 1.5519586     |
|    n_updates                 | 3440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.45e-11      |
|    reward_explained_variance | -4.36         |
|    reward_value_loss         | 13            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 346           |
|    time_elapsed              | 74527         |
|    total_timesteps           | 44288         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00193       |
|    cost_value_loss           | 8.83e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000705     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.6           |
|    mean_cost_advantages      | -0.0015785873 |
|    mean_reward_advantages    | 0.0018937327  |
|    n_updates                 | 3450          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.4e-09       |
|    reward_explained_variance | -0.785        |
|    reward_value_loss         | 10.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.28          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 347           |
|    time_elapsed              | 74589         |
|    total_timesteps           | 44416         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00765       |
|    cost_value_loss           | 5.59e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000706     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.14          |
|    mean_cost_advantages      | 0.00044958302 |
|    mean_reward_advantages    | -0.67968893   |
|    n_updates                 | 3460          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.18e-09     |
|    reward_explained_variance | -0.425        |
|    reward_value_loss         | 5.1           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.6          |
|    ep_rew_mean               | 6.57         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 348          |
|    time_elapsed              | 74652        |
|    total_timesteps           | 44544        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.024        |
|    cost_value_loss           | 8.28e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00072     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.14         |
|    mean_cost_advantages      | 0.0006244739 |
|    mean_reward_advantages    | 1.8035597    |
|    n_updates                 | 3470         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.28e-09     |
|    reward_explained_variance | -4.85        |
|    reward_value_loss         | 14.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.6           |
|    ep_rew_mean               | 6.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 349           |
|    time_elapsed              | 74715         |
|    total_timesteps           | 44672         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00858      |
|    cost_value_loss           | 9.49e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000686     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.15          |
|    mean_cost_advantages      | -0.0013354605 |
|    mean_reward_advantages    | 0.94321704    |
|    n_updates                 | 3480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.05e-09      |
|    reward_explained_variance | -1.1          |
|    reward_value_loss         | 8.21          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.02          |
|    ep_rew_mean               | 6             |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 350           |
|    time_elapsed              | 74778         |
|    total_timesteps           | 44800         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0182        |
|    cost_value_loss           | 0.000106      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000705     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.86          |
|    mean_cost_advantages      | 0.00068525097 |
|    mean_reward_advantages    | -0.7041825    |
|    n_updates                 | 3490          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.2e-09      |
|    reward_explained_variance | -1.98         |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.06        |
|    ep_rew_mean               | 6.03        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 351         |
|    time_elapsed              | 74840       |
|    total_timesteps           | 44928       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0597     |
|    cost_value_loss           | 7.09e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000715   |
|    learning_rate             | 0.0005      |
|    loss                      | 3.03        |
|    mean_cost_advantages      | 0.001417277 |
|    mean_reward_advantages    | -0.5779809  |
|    n_updates                 | 3500        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -6e-09      |
|    reward_explained_variance | -0.253      |
|    reward_value_loss         | 6.43        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.5           |
|    ep_rew_mean               | 6.47          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 352           |
|    time_elapsed              | 74903         |
|    total_timesteps           | 45056         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0184       |
|    cost_value_loss           | 7.13e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000712     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.05          |
|    mean_cost_advantages      | -0.0010888948 |
|    mean_reward_advantages    | 0.36073858    |
|    n_updates                 | 3510          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.71e-09      |
|    reward_explained_variance | -1.27         |
|    reward_value_loss         | 8.49          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.29         |
|    ep_rew_mean               | 6.26         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 353          |
|    time_elapsed              | 74965        |
|    total_timesteps           | 45184        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0667      |
|    cost_value_loss           | 5.72e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000718    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.89         |
|    mean_cost_advantages      | 0.0005482865 |
|    mean_reward_advantages    | 1.2860558    |
|    n_updates                 | 3520         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.94e-09    |
|    reward_explained_variance | -0.782       |
|    reward_value_loss         | 10.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.16           |
|    ep_rew_mean               | 6.12           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 354            |
|    time_elapsed              | 75029          |
|    total_timesteps           | 45312          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0435         |
|    cost_value_loss           | 8.33e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00073       |
|    learning_rate             | 0.0005         |
|    loss                      | 5.99           |
|    mean_cost_advantages      | -0.00017208405 |
|    mean_reward_advantages    | -0.99600863    |
|    n_updates                 | 3530           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.92e-09      |
|    reward_explained_variance | -1.45          |
|    reward_value_loss         | 9.9            |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.32           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 355            |
|    time_elapsed              | 75095          |
|    total_timesteps           | 45440          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0239        |
|    cost_value_loss           | 0.0001         |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00071       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.56           |
|    mean_cost_advantages      | -0.00015058143 |
|    mean_reward_advantages    | -0.15215418    |
|    n_updates                 | 3540           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.68e-10      |
|    reward_explained_variance | -0.985         |
|    reward_value_loss         | 5.81           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.64         |
|    ep_rew_mean               | 6.6          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 356          |
|    time_elapsed              | 75166        |
|    total_timesteps           | 45568        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.15        |
|    cost_value_loss           | 9.29e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000698    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.43         |
|    mean_cost_advantages      | 0.0006292374 |
|    mean_reward_advantages    | 0.5465228    |
|    n_updates                 | 3550         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.58e-09    |
|    reward_explained_variance | -1.61        |
|    reward_value_loss         | 7.77         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 357           |
|    time_elapsed              | 75232         |
|    total_timesteps           | 45696         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00511       |
|    cost_value_loss           | 7.73e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000708     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.74          |
|    mean_cost_advantages      | -0.0011345616 |
|    mean_reward_advantages    | 0.99748355    |
|    n_updates                 | 3560          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.66e-09      |
|    reward_explained_variance | -1.87         |
|    reward_value_loss         | 11.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.16          |
|    ep_rew_mean               | 6.13          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 358           |
|    time_elapsed              | 75300         |
|    total_timesteps           | 45824         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00879      |
|    cost_value_loss           | 8.4e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000704     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.01          |
|    mean_cost_advantages      | 0.00071992667 |
|    mean_reward_advantages    | -0.32635057   |
|    n_updates                 | 3570          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.46e-09     |
|    reward_explained_variance | -1.31         |
|    reward_value_loss         | 8.62          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.65         |
|    ep_rew_mean               | 5.62         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 359          |
|    time_elapsed              | 75369        |
|    total_timesteps           | 45952        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00569      |
|    cost_value_loss           | 0.000101     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000724    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.25         |
|    mean_cost_advantages      | 0.0009845388 |
|    mean_reward_advantages    | 1.1684613    |
|    n_updates                 | 3580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.99e-09    |
|    reward_explained_variance | -1.92        |
|    reward_value_loss         | 13.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.74           |
|    ep_rew_mean               | 5.72           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 360            |
|    time_elapsed              | 75439          |
|    total_timesteps           | 46080          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00725       |
|    cost_value_loss           | 6.08e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000713      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.9            |
|    mean_cost_advantages      | -0.00026932245 |
|    mean_reward_advantages    | -1.5187478     |
|    n_updates                 | 3590           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.32e-09      |
|    reward_explained_variance | -0.149         |
|    reward_value_loss         | 5.26           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.64          |
|    ep_rew_mean               | 5.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 361           |
|    time_elapsed              | 75509         |
|    total_timesteps           | 46208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00171       |
|    cost_value_loss           | 9.85e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000707     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.19          |
|    mean_cost_advantages      | 0.00011063501 |
|    mean_reward_advantages    | 1.3286959     |
|    n_updates                 | 3600          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.66e-09     |
|    reward_explained_variance | -1.55         |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.71           |
|    ep_rew_mean               | 5.68           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 362            |
|    time_elapsed              | 75576          |
|    total_timesteps           | 46336          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0525         |
|    cost_value_loss           | 0.000129       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000704      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.57           |
|    mean_cost_advantages      | -0.00054821064 |
|    mean_reward_advantages    | -0.6036399     |
|    n_updates                 | 3610           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.42e-09      |
|    reward_explained_variance | -0.836         |
|    reward_value_loss         | 10.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.95          |
|    ep_rew_mean               | 5.92          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 363           |
|    time_elapsed              | 75644         |
|    total_timesteps           | 46464         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.039        |
|    cost_value_loss           | 0.000108      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000701     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.64          |
|    mean_cost_advantages      | -0.0011983602 |
|    mean_reward_advantages    | 0.888604      |
|    n_updates                 | 3620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.42e-09      |
|    reward_explained_variance | -0.189        |
|    reward_value_loss         | 8.94          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.1          |
|    ep_rew_mean               | 6.07         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 364          |
|    time_elapsed              | 75709        |
|    total_timesteps           | 46592        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0265      |
|    cost_value_loss           | 0.000107     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00072     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.19         |
|    mean_cost_advantages      | 0.0033554947 |
|    mean_reward_advantages    | 0.008363426  |
|    n_updates                 | 3630         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.15e-09    |
|    reward_explained_variance | -1.77        |
|    reward_value_loss         | 13.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 365           |
|    time_elapsed              | 75775         |
|    total_timesteps           | 46720         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0305        |
|    cost_value_loss           | 9.4e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000711     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.61          |
|    mean_cost_advantages      | -0.0009799178 |
|    mean_reward_advantages    | -1.1245799    |
|    n_updates                 | 3640          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.49e-09     |
|    reward_explained_variance | -0.591        |
|    reward_value_loss         | 7.6           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 366           |
|    time_elapsed              | 75842         |
|    total_timesteps           | 46848         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0131       |
|    cost_value_loss           | 0.000138      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000722     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.15          |
|    mean_cost_advantages      | 0.00037478912 |
|    mean_reward_advantages    | 0.521377      |
|    n_updates                 | 3650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.26e-09     |
|    reward_explained_variance | -1.65         |
|    reward_value_loss         | 8.79          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 367           |
|    time_elapsed              | 75909         |
|    total_timesteps           | 46976         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.032        |
|    cost_value_loss           | 0.0001        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000707     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.61          |
|    mean_cost_advantages      | -0.0010384934 |
|    mean_reward_advantages    | 0.3808759     |
|    n_updates                 | 3660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.13e-09     |
|    reward_explained_variance | -1.82         |
|    reward_value_loss         | 11.2          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.84        |
|    ep_rew_mean               | 5.81        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 368         |
|    time_elapsed              | 75976       |
|    total_timesteps           | 47104       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0083     |
|    cost_value_loss           | 9.68e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000705   |
|    learning_rate             | 0.0005      |
|    loss                      | 2.71        |
|    mean_cost_advantages      | 0.001965132 |
|    mean_reward_advantages    | 0.10361266  |
|    n_updates                 | 3670        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -2.67e-09   |
|    reward_explained_variance | -1.14       |
|    reward_value_loss         | 5.83        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.8           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 369           |
|    time_elapsed              | 76042         |
|    total_timesteps           | 47232         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0483        |
|    cost_value_loss           | 8.03e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000713     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.99          |
|    mean_cost_advantages      | -0.0005625789 |
|    mean_reward_advantages    | 0.33793044    |
|    n_updates                 | 3680          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.71e-10      |
|    reward_explained_variance | -0.442        |
|    reward_value_loss         | 7.22          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.01        |
|    ep_rew_mean               | 5.98        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 370         |
|    time_elapsed              | 76108       |
|    total_timesteps           | 47360       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0302     |
|    cost_value_loss           | 9.04e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.00071    |
|    learning_rate             | 0.0005      |
|    loss                      | 4.16        |
|    mean_cost_advantages      | 0.000700047 |
|    mean_reward_advantages    | 0.34800154  |
|    n_updates                 | 3690        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -6.17e-10   |
|    reward_explained_variance | -1.1        |
|    reward_value_loss         | 7.91        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.97          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 371           |
|    time_elapsed              | 76173         |
|    total_timesteps           | 47488         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00794      |
|    cost_value_loss           | 8.95e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000693     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.38          |
|    mean_cost_advantages      | -0.0013906416 |
|    mean_reward_advantages    | 0.5667429     |
|    n_updates                 | 3700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.96e-09     |
|    reward_explained_variance | -1.78         |
|    reward_value_loss         | 10.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.25           |
|    ep_rew_mean               | 6.22           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 372            |
|    time_elapsed              | 76239          |
|    total_timesteps           | 47616          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00693        |
|    cost_value_loss           | 6.43e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000722      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.47           |
|    mean_cost_advantages      | -0.00014258584 |
|    mean_reward_advantages    | 0.34695712     |
|    n_updates                 | 3710           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.19e-09       |
|    reward_explained_variance | -0.586         |
|    reward_value_loss         | 8.49           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.18         |
|    ep_rew_mean               | 6.15         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 373          |
|    time_elapsed              | 76305        |
|    total_timesteps           | 47744        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0162      |
|    cost_value_loss           | 0.000105     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000698    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.66         |
|    mean_cost_advantages      | 0.0006684037 |
|    mean_reward_advantages    | 1.0155174    |
|    n_updates                 | 3720         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.58e-09    |
|    reward_explained_variance | -0.914       |
|    reward_value_loss         | 7.76         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.51         |
|    ep_rew_mean               | 6.47         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 374          |
|    time_elapsed              | 76373        |
|    total_timesteps           | 47872        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0258      |
|    cost_value_loss           | 9.91e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000712    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.04         |
|    mean_cost_advantages      | 0.0007529148 |
|    mean_reward_advantages    | -0.14095327  |
|    n_updates                 | 3730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.91e-09     |
|    reward_explained_variance | -0.111       |
|    reward_value_loss         | 5.32         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.7           |
|    ep_rew_mean               | 6.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 375           |
|    time_elapsed              | 76440         |
|    total_timesteps           | 48000         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.019         |
|    cost_value_loss           | 0.00013       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0007       |
|    learning_rate             | 0.0005        |
|    loss                      | 5.04          |
|    mean_cost_advantages      | -0.0012826074 |
|    mean_reward_advantages    | 1.9421674     |
|    n_updates                 | 3740          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.96e-10     |
|    reward_explained_variance | -1.7          |
|    reward_value_loss         | 11.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.49         |
|    ep_rew_mean               | 6.45         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 376          |
|    time_elapsed              | 76509        |
|    total_timesteps           | 48128        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0171       |
|    cost_value_loss           | 9.58e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000707    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.89         |
|    mean_cost_advantages      | 0.0019537662 |
|    mean_reward_advantages    | 0.53640187   |
|    n_updates                 | 3750         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.98e-10     |
|    reward_explained_variance | -2.07        |
|    reward_value_loss         | 14.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.74          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 377           |
|    time_elapsed              | 76575         |
|    total_timesteps           | 48256         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0261       |
|    cost_value_loss           | 0.000137      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000703     |
|    learning_rate             | 0.0005        |
|    loss                      | 10.3          |
|    mean_cost_advantages      | -0.0026167221 |
|    mean_reward_advantages    | 1.6458771     |
|    n_updates                 | 3760          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.95e-09      |
|    reward_explained_variance | -3.78         |
|    reward_value_loss         | 21.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.05         |
|    ep_rew_mean               | 7            |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 378          |
|    time_elapsed              | 76642        |
|    total_timesteps           | 48384        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0631      |
|    cost_value_loss           | 0.000138     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000719    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.88         |
|    mean_cost_advantages      | 0.0022291369 |
|    mean_reward_advantages    | -1.5146501   |
|    n_updates                 | 3770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.77e-09    |
|    reward_explained_variance | -4.12        |
|    reward_value_loss         | 13.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.95          |
|    ep_rew_mean               | 6.91          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 379           |
|    time_elapsed              | 76709         |
|    total_timesteps           | 48512         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0142        |
|    cost_value_loss           | 9.33e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000703     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.5           |
|    mean_cost_advantages      | -0.0008208717 |
|    mean_reward_advantages    | -0.29642183   |
|    n_updates                 | 3780          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.92e-09     |
|    reward_explained_variance | -1.52         |
|    reward_value_loss         | 8.9           |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.89        |
|    ep_rew_mean               | 6.85        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 380         |
|    time_elapsed              | 76776       |
|    total_timesteps           | 48640       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0103     |
|    cost_value_loss           | 7.44e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000713   |
|    learning_rate             | 0.0005      |
|    loss                      | 3.81        |
|    mean_cost_advantages      | 0.003015696 |
|    mean_reward_advantages    | -0.9653661  |
|    n_updates                 | 3790        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -3.63e-09   |
|    reward_explained_variance | -0.292      |
|    reward_value_loss         | 7.14        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 381           |
|    time_elapsed              | 76844         |
|    total_timesteps           | 48768         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0073        |
|    cost_value_loss           | 0.0001        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000699     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.75          |
|    mean_cost_advantages      | -0.0018933824 |
|    mean_reward_advantages    | 0.95283663    |
|    n_updates                 | 3800          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.4e-10       |
|    reward_explained_variance | -3.45         |
|    reward_value_loss         | 16            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 382           |
|    time_elapsed              | 76910         |
|    total_timesteps           | 48896         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.18         |
|    cost_value_loss           | 0.00012       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000715     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.67          |
|    mean_cost_advantages      | -0.0027629328 |
|    mean_reward_advantages    | -0.76939994   |
|    n_updates                 | 3810          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.68e-09      |
|    reward_explained_variance | -2.2          |
|    reward_value_loss         | 11.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.57         |
|    ep_rew_mean               | 5.54         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 383          |
|    time_elapsed              | 76983        |
|    total_timesteps           | 49024        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0164      |
|    cost_value_loss           | 9.33e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000698    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.05         |
|    mean_cost_advantages      | 0.0033283627 |
|    mean_reward_advantages    | 1.5360272    |
|    n_updates                 | 3820         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.05e-08    |
|    reward_explained_variance | -4.81        |
|    reward_value_loss         | 9.5          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.69          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 384           |
|    time_elapsed              | 77055         |
|    total_timesteps           | 49152         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.014         |
|    cost_value_loss           | 6.92e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000706     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.92          |
|    mean_cost_advantages      | -0.0012980062 |
|    mean_reward_advantages    | -0.7958104    |
|    n_updates                 | 3830          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.61e-09     |
|    reward_explained_variance | -1.57         |
|    reward_value_loss         | 9.94          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.65          |
|    ep_rew_mean               | 5.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 385           |
|    time_elapsed              | 77125         |
|    total_timesteps           | 49280         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0125        |
|    cost_value_loss           | 0.00013       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000709     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.49          |
|    mean_cost_advantages      | -0.0002931922 |
|    mean_reward_advantages    | 0.32722476    |
|    n_updates                 | 3840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.52e-10     |
|    reward_explained_variance | -3.28         |
|    reward_value_loss         | 17.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.15           |
|    ep_rew_mean               | 6.11           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 386            |
|    time_elapsed              | 77193          |
|    total_timesteps           | 49408          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0018         |
|    cost_value_loss           | 8.33e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000699      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.12           |
|    mean_cost_advantages      | -0.00042934663 |
|    mean_reward_advantages    | 0.30646572     |
|    n_updates                 | 3850           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.42e-10       |
|    reward_explained_variance | -1.83          |
|    reward_value_loss         | 7.72           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 387           |
|    time_elapsed              | 77259         |
|    total_timesteps           | 49536         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0271       |
|    cost_value_loss           | 9.36e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000693     |
|    learning_rate             | 0.0005        |
|    loss                      | 12            |
|    mean_cost_advantages      | 0.00017917354 |
|    mean_reward_advantages    | 2.61748       |
|    n_updates                 | 3860          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.3e-09      |
|    reward_explained_variance | -6.18         |
|    reward_value_loss         | 24.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.25         |
|    ep_rew_mean               | 6.22         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 388          |
|    time_elapsed              | 77327        |
|    total_timesteps           | 49664        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0988      |
|    cost_value_loss           | 0.000125     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000714    |
|    learning_rate             | 0.0005       |
|    loss                      | 7.44         |
|    mean_cost_advantages      | 0.0010918654 |
|    mean_reward_advantages    | -1.4547282   |
|    n_updates                 | 3870         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.66e-09     |
|    reward_explained_variance | -3           |
|    reward_value_loss         | 18.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.28          |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 389           |
|    time_elapsed              | 77395         |
|    total_timesteps           | 49792         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0883       |
|    cost_value_loss           | 0.000137      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000699     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.39          |
|    mean_cost_advantages      | -0.0018385232 |
|    mean_reward_advantages    | -1.3947482    |
|    n_updates                 | 3880          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.76e-09     |
|    reward_explained_variance | -1.19         |
|    reward_value_loss         | 11.8          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.35        |
|    ep_rew_mean               | 6.32        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 390         |
|    time_elapsed              | 77462       |
|    total_timesteps           | 49920       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0306     |
|    cost_value_loss           | 4.42e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000705   |
|    learning_rate             | 0.0005      |
|    loss                      | 2.45        |
|    mean_cost_advantages      | 0.002636662 |
|    mean_reward_advantages    | -0.8560346  |
|    n_updates                 | 3890        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 4.4e-09     |
|    reward_explained_variance | 0.285       |
|    reward_value_loss         | 4.88        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.08         |
|    ep_rew_mean               | 6.06         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 391          |
|    time_elapsed              | 77527        |
|    total_timesteps           | 50048        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0156       |
|    cost_value_loss           | 0.000114     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00071     |
|    learning_rate             | 0.0005       |
|    loss                      | 7.07         |
|    mean_cost_advantages      | -0.002212589 |
|    mean_reward_advantages    | 1.8028723    |
|    n_updates                 | 3900         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 6.37e-09     |
|    reward_explained_variance | -2.75        |
|    reward_value_loss         | 12.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.11         |
|    ep_rew_mean               | 6.09         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 392          |
|    time_elapsed              | 77596        |
|    total_timesteps           | 50176        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0138       |
|    cost_value_loss           | 8.44e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000694    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.65         |
|    mean_cost_advantages      | 0.0025429395 |
|    mean_reward_advantages    | -0.94473433  |
|    n_updates                 | 3910         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.44e-09     |
|    reward_explained_variance | -1.52        |
|    reward_value_loss         | 9.41         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.98          |
|    ep_rew_mean               | 5.97          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 393           |
|    time_elapsed              | 77663         |
|    total_timesteps           | 50304         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0453        |
|    cost_value_loss           | 7.97e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000688     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.19          |
|    mean_cost_advantages      | -0.0017431958 |
|    mean_reward_advantages    | -0.062348157  |
|    n_updates                 | 3920          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.56e-09      |
|    reward_explained_variance | -4.54         |
|    reward_value_loss         | 9.37          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.03         |
|    ep_rew_mean               | 6.01         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 394          |
|    time_elapsed              | 77744        |
|    total_timesteps           | 50432        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0397      |
|    cost_value_loss           | 9e-05        |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000695    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.24         |
|    mean_cost_advantages      | 0.0016472916 |
|    mean_reward_advantages    | 0.5931928    |
|    n_updates                 | 3930         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.62e-09    |
|    reward_explained_variance | -2.69        |
|    reward_value_loss         | 9.25         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.28          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 395           |
|    time_elapsed              | 77856         |
|    total_timesteps           | 50560         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00523      |
|    cost_value_loss           | 9.39e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000697     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.85          |
|    mean_cost_advantages      | -0.0013094703 |
|    mean_reward_advantages    | 1.3097585     |
|    n_updates                 | 3940          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.93e-09      |
|    reward_explained_variance | -1.34         |
|    reward_value_loss         | 15.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.06           |
|    ep_rew_mean               | 6.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 396            |
|    time_elapsed              | 77963          |
|    total_timesteps           | 50688          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0858        |
|    cost_value_loss           | 6.45e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000712      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.94           |
|    mean_cost_advantages      | -0.00013176518 |
|    mean_reward_advantages    | -0.5943558     |
|    n_updates                 | 3950           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.32e-09       |
|    reward_explained_variance | -0.95          |
|    reward_value_loss         | 9.58           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.4           |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 397           |
|    time_elapsed              | 78065         |
|    total_timesteps           | 50816         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0413        |
|    cost_value_loss           | 9.63e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.0007       |
|    learning_rate             | 0.0005        |
|    loss                      | 3             |
|    mean_cost_advantages      | 0.00041007635 |
|    mean_reward_advantages    | 0.32115686    |
|    n_updates                 | 3960          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.57e-10     |
|    reward_explained_variance | -1.5          |
|    reward_value_loss         | 8.72          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.57          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 398           |
|    time_elapsed              | 78167         |
|    total_timesteps           | 50944         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00326       |
|    cost_value_loss           | 8.86e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00071      |
|    learning_rate             | 0.0005        |
|    loss                      | 5             |
|    mean_cost_advantages      | 0.00025584543 |
|    mean_reward_advantages    | 0.6207933     |
|    n_updates                 | 3970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.74e-09      |
|    reward_explained_variance | -1.77         |
|    reward_value_loss         | 9.56          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.23           |
|    ep_rew_mean               | 6.19           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 399            |
|    time_elapsed              | 78267          |
|    total_timesteps           | 51072          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0198        |
|    cost_value_loss           | 8.48e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000705      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.38           |
|    mean_cost_advantages      | -0.00035211106 |
|    mean_reward_advantages    | -0.14794445    |
|    n_updates                 | 3980           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.12e-09      |
|    reward_explained_variance | -1.51          |
|    reward_value_loss         | 9.86           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 400           |
|    time_elapsed              | 78367         |
|    total_timesteps           | 51200         |
| train/                       |               |
|    approx_kl                 | 0.0038641184  |
|    average_cost              | 0.0           |
|    clip_fraction             | 0.00703       |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.317        |
|    cost_value_loss           | 7.59e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00106      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.81          |
|    mean_cost_advantages      | -0.0007834517 |
|    mean_reward_advantages    | -0.56343925   |
|    n_updates                 | 3990          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -0.000324     |
|    reward_explained_variance | -0.395        |
|    reward_value_loss         | 5.23          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.35         |
|    ep_rew_mean               | 6.32         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 401          |
|    time_elapsed              | 78469        |
|    total_timesteps           | 51328        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0322       |
|    cost_value_loss           | 9.73e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00108     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.35         |
|    mean_cost_advantages      | 0.0006668541 |
|    mean_reward_advantages    | 0.21976683   |
|    n_updates                 | 4000         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 9.44e-10     |
|    reward_explained_variance | -1.08        |
|    reward_value_loss         | 10.7         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.82         |
|    ep_rew_mean               | 5.79         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 402          |
|    time_elapsed              | 78571        |
|    total_timesteps           | 51456        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.272       |
|    cost_value_loss           | 7.03e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00097     |
|    learning_rate             | 0.0005       |
|    loss                      | 6.45         |
|    mean_cost_advantages      | 0.0018688088 |
|    mean_reward_advantages    | 0.50092953   |
|    n_updates                 | 4010         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.57e-09    |
|    reward_explained_variance | -2.15        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.73          |
|    ep_rew_mean               | 5.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 403           |
|    time_elapsed              | 78671         |
|    total_timesteps           | 51584         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0358       |
|    cost_value_loss           | 5.64e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000988     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.36          |
|    mean_cost_advantages      | -0.0019485286 |
|    mean_reward_advantages    | -0.20222925   |
|    n_updates                 | 4020          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.77e-10      |
|    reward_explained_variance | -2.12         |
|    reward_value_loss         | 8.66          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.97        |
|    ep_rew_mean               | 5.94        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 404         |
|    time_elapsed              | 78780       |
|    total_timesteps           | 51712       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0252      |
|    cost_value_loss           | 6.61e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000979   |
|    learning_rate             | 0.0005      |
|    loss                      | 6.55        |
|    mean_cost_advantages      | -0.00060572 |
|    mean_reward_advantages    | 0.3121962   |
|    n_updates                 | 4030        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 9.91e-09    |
|    reward_explained_variance | -2.08       |
|    reward_value_loss         | 14.3        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 405           |
|    time_elapsed              | 78893         |
|    total_timesteps           | 51840         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.29         |
|    cost_value_loss           | 9.71e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000989     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.98          |
|    mean_cost_advantages      | -0.0018564174 |
|    mean_reward_advantages    | 0.49686623    |
|    n_updates                 | 4040          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.24e-09      |
|    reward_explained_variance | -3.92         |
|    reward_value_loss         | 9.77          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.19         |
|    ep_rew_mean               | 6.16         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 406          |
|    time_elapsed              | 79007        |
|    total_timesteps           | 51968        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.117       |
|    cost_value_loss           | 0.00011      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000995    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.16         |
|    mean_cost_advantages      | 0.0023000627 |
|    mean_reward_advantages    | -0.083618686 |
|    n_updates                 | 4050         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.25e-09     |
|    reward_explained_variance | -1.81        |
|    reward_value_loss         | 10.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.4           |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 407           |
|    time_elapsed              | 79130         |
|    total_timesteps           | 52096         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.035         |
|    cost_value_loss           | 7.97e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000973     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.9           |
|    mean_cost_advantages      | 0.00035372147 |
|    mean_reward_advantages    | 1.0029744     |
|    n_updates                 | 4060          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.73e-09      |
|    reward_explained_variance | -3.44         |
|    reward_value_loss         | 14.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 408           |
|    time_elapsed              | 79246         |
|    total_timesteps           | 52224         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00511       |
|    cost_value_loss           | 6.91e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000967     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.64          |
|    mean_cost_advantages      | 0.00018266944 |
|    mean_reward_advantages    | 0.33468378    |
|    n_updates                 | 4070          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.7e-09      |
|    reward_explained_variance | -0.724        |
|    reward_value_loss         | 8.13          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.21          |
|    ep_rew_mean               | 6.18          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 409           |
|    time_elapsed              | 79358         |
|    total_timesteps           | 52352         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.105        |
|    cost_value_loss           | 6.35e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00095      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.71          |
|    mean_cost_advantages      | -0.0022301914 |
|    mean_reward_advantages    | -0.24836525   |
|    n_updates                 | 4080          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.63e-10      |
|    reward_explained_variance | -1.46         |
|    reward_value_loss         | 9.37          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.04         |
|    ep_rew_mean               | 6.01         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 410          |
|    time_elapsed              | 79470        |
|    total_timesteps           | 52480        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00625      |
|    cost_value_loss           | 6.38e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000981    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.72         |
|    mean_cost_advantages      | 0.0015922324 |
|    mean_reward_advantages    | -0.49850422  |
|    n_updates                 | 4090         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.3e-09     |
|    reward_explained_variance | -1.78        |
|    reward_value_loss         | 6.49         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.64          |
|    ep_rew_mean               | 5.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 411           |
|    time_elapsed              | 79582         |
|    total_timesteps           | 52608         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00908       |
|    cost_value_loss           | 7.92e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000962     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.58          |
|    mean_cost_advantages      | 0.00025502895 |
|    mean_reward_advantages    | 1.3500096     |
|    n_updates                 | 4100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.27e-11      |
|    reward_explained_variance | -1.95         |
|    reward_value_loss         | 12            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.68          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 412           |
|    time_elapsed              | 79691         |
|    total_timesteps           | 52736         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0183       |
|    cost_value_loss           | 4.59e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000973     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.16          |
|    mean_cost_advantages      | -0.0006252602 |
|    mean_reward_advantages    | -0.62944126   |
|    n_updates                 | 4110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.93e-09      |
|    reward_explained_variance | -0.465        |
|    reward_value_loss         | 5.71          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.98          |
|    ep_rew_mean               | 5.95          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 413           |
|    time_elapsed              | 79805         |
|    total_timesteps           | 52864         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00751      |
|    cost_value_loss           | 7.66e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000962     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.52          |
|    mean_cost_advantages      | 0.00010158974 |
|    mean_reward_advantages    | 1.6325805     |
|    n_updates                 | 4120          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.49e-08     |
|    reward_explained_variance | -9.11         |
|    reward_value_loss         | 18            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.29          |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 414           |
|    time_elapsed              | 79928         |
|    total_timesteps           | 52992         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0183       |
|    cost_value_loss           | 8.29e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00098      |
|    learning_rate             | 0.0005        |
|    loss                      | 6.68          |
|    mean_cost_advantages      | 0.00029913354 |
|    mean_reward_advantages    | -0.19664045   |
|    n_updates                 | 4130          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.07e-09      |
|    reward_explained_variance | -3.94         |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.52          |
|    ep_rew_mean               | 6.48          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 415           |
|    time_elapsed              | 80045         |
|    total_timesteps           | 53120         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.186        |
|    cost_value_loss           | 0.0001        |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000984     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.86          |
|    mean_cost_advantages      | -0.0001956565 |
|    mean_reward_advantages    | -0.80274886   |
|    n_updates                 | 4140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.37e-09     |
|    reward_explained_variance | -1.82         |
|    reward_value_loss         | 9.79          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.4            |
|    ep_rew_mean               | 6.35           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 416            |
|    time_elapsed              | 80165          |
|    total_timesteps           | 53248          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0885         |
|    cost_value_loss           | 8.48e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000972      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.2            |
|    mean_cost_advantages      | -6.1066385e-06 |
|    mean_reward_advantages    | 0.4333374      |
|    n_updates                 | 4150           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.69e-09       |
|    reward_explained_variance | -1.03          |
|    reward_value_loss         | 7.75           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.86           |
|    ep_rew_mean               | 6.81           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 417            |
|    time_elapsed              | 80278          |
|    total_timesteps           | 53376          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0206        |
|    cost_value_loss           | 5.74e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000999      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.27           |
|    mean_cost_advantages      | -0.00018265143 |
|    mean_reward_advantages    | -0.2819366     |
|    n_updates                 | 4160           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.95e-10      |
|    reward_explained_variance | -0.492         |
|    reward_value_loss         | 10.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.9          |
|    ep_rew_mean               | 6.85         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 418          |
|    time_elapsed              | 80392        |
|    total_timesteps           | 53504        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0229      |
|    cost_value_loss           | 0.000137     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000966    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.54         |
|    mean_cost_advantages      | 0.0006433121 |
|    mean_reward_advantages    | 1.5438664    |
|    n_updates                 | 4170         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.72e-09    |
|    reward_explained_variance | -1.53        |
|    reward_value_loss         | 17.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.85           |
|    ep_rew_mean               | 6.81           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 419            |
|    time_elapsed              | 80505          |
|    total_timesteps           | 53632          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.054          |
|    cost_value_loss           | 9.75e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00095       |
|    learning_rate             | 0.0005         |
|    loss                      | 10.3           |
|    mean_cost_advantages      | -0.00059751526 |
|    mean_reward_advantages    | -0.5835691     |
|    n_updates                 | 4180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.95e-09       |
|    reward_explained_variance | -1.38          |
|    reward_value_loss         | 17.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.42          |
|    ep_rew_mean               | 6.39          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 420           |
|    time_elapsed              | 80616         |
|    total_timesteps           | 53760         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0082       |
|    cost_value_loss           | 6.79e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000949     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.58          |
|    mean_cost_advantages      | 3.4780824e-06 |
|    mean_reward_advantages    | -0.82692146   |
|    n_updates                 | 4190          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.45e-09     |
|    reward_explained_variance | -1.22         |
|    reward_value_loss         | 9.11          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.97          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 421           |
|    time_elapsed              | 80735         |
|    total_timesteps           | 53888         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0288        |
|    cost_value_loss           | 5.89e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000981     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.69          |
|    mean_cost_advantages      | 0.00042423123 |
|    mean_reward_advantages    | -1.5120659    |
|    n_updates                 | 4200          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.76e-09     |
|    reward_explained_variance | -1.22         |
|    reward_value_loss         | 13.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.93          |
|    ep_rew_mean               | 5.91          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 422           |
|    time_elapsed              | 80847         |
|    total_timesteps           | 54016         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0724       |
|    cost_value_loss           | 7.42e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00096      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.05          |
|    mean_cost_advantages      | -0.0009863961 |
|    mean_reward_advantages    | 1.0212606     |
|    n_updates                 | 4210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.55e-10     |
|    reward_explained_variance | -0.33         |
|    reward_value_loss         | 8.82          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.76         |
|    ep_rew_mean               | 5.73         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 423          |
|    time_elapsed              | 80958        |
|    total_timesteps           | 54144        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0315       |
|    cost_value_loss           | 7.75e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00098     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.33         |
|    mean_cost_advantages      | 0.0006373947 |
|    mean_reward_advantages    | -0.14771023  |
|    n_updates                 | 4220         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.49e-09     |
|    reward_explained_variance | -0.641       |
|    reward_value_loss         | 6.96         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.66          |
|    ep_rew_mean               | 5.63          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 424           |
|    time_elapsed              | 81068         |
|    total_timesteps           | 54272         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0289       |
|    cost_value_loss           | 7.28e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000947     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.59          |
|    mean_cost_advantages      | 0.00083112495 |
|    mean_reward_advantages    | 1.2310395     |
|    n_updates                 | 4230          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.52e-09      |
|    reward_explained_variance | -0.644        |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.7            |
|    ep_rew_mean               | 5.68           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 425            |
|    time_elapsed              | 81180          |
|    total_timesteps           | 54400          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00575        |
|    cost_value_loss           | 8.03e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000959      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.82           |
|    mean_cost_advantages      | -0.00057647115 |
|    mean_reward_advantages    | -0.6749362     |
|    n_updates                 | 4240           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6e-10         |
|    reward_explained_variance | -0.44          |
|    reward_value_loss         | 11.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.69         |
|    ep_rew_mean               | 5.66         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 426          |
|    time_elapsed              | 81283        |
|    total_timesteps           | 54528        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.157       |
|    cost_value_loss           | 5.12e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000978    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.97         |
|    mean_cost_advantages      | 0.0010312656 |
|    mean_reward_advantages    | -0.62767524  |
|    n_updates                 | 4250         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.57e-10     |
|    reward_explained_variance | -0.0923      |
|    reward_value_loss         | 5.34         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.34         |
|    ep_rew_mean               | 5.32         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 427          |
|    time_elapsed              | 81390        |
|    total_timesteps           | 54656        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0123       |
|    cost_value_loss           | 5.64e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000975    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.27         |
|    mean_cost_advantages      | 0.0011034862 |
|    mean_reward_advantages    | -0.82785845  |
|    n_updates                 | 4260         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.77e-11     |
|    reward_explained_variance | -0.189       |
|    reward_value_loss         | 4.48         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.38           |
|    ep_rew_mean               | 5.35           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 428            |
|    time_elapsed              | 81494          |
|    total_timesteps           | 54784          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.109         |
|    cost_value_loss           | 5.06e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000998      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.12           |
|    mean_cost_advantages      | 0.000112311696 |
|    mean_reward_advantages    | 0.13103299     |
|    n_updates                 | 4270           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.22e-09       |
|    reward_explained_variance | -0.83          |
|    reward_value_loss         | 7.05           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.12          |
|    ep_rew_mean               | 5.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 429           |
|    time_elapsed              | 81596         |
|    total_timesteps           | 54912         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0537        |
|    cost_value_loss           | 6.55e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000972     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.29          |
|    mean_cost_advantages      | -0.0010200276 |
|    mean_reward_advantages    | 1.3267226     |
|    n_updates                 | 4280          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.1e-08      |
|    reward_explained_variance | -1.52         |
|    reward_value_loss         | 9.94          |
|    total_cost                | 0.0           |
------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 6.37            |
|    ep_rew_mean               | 5.33            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 430             |
|    time_elapsed              | 81698           |
|    total_timesteps           | 55040           |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | 0.0155          |
|    cost_value_loss           | 6.03e-05        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -0.000979       |
|    learning_rate             | 0.0005          |
|    loss                      | 3.4             |
|    mean_cost_advantages      | -0.000114470255 |
|    mean_reward_advantages    | -0.64294416     |
|    n_updates                 | 4290            |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | 1.84e-09        |
|    reward_explained_variance | -0.972          |
|    reward_value_loss         | 7.83            |
|    total_cost                | 0.0             |
--------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.62        |
|    ep_rew_mean               | 5.59        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 431         |
|    time_elapsed              | 81801       |
|    total_timesteps           | 55168       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0904     |
|    cost_value_loss           | 7.17e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000993   |
|    learning_rate             | 0.0005      |
|    loss                      | 10.1        |
|    mean_cost_advantages      | -0.00028403 |
|    mean_reward_advantages    | 2.1206005   |
|    n_updates                 | 4300        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 7.85e-09    |
|    reward_explained_variance | -2          |
|    reward_value_loss         | 21.8        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.52          |
|    ep_rew_mean               | 5.49          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 432           |
|    time_elapsed              | 81903         |
|    total_timesteps           | 55296         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00557       |
|    cost_value_loss           | 7.61e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000952     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.22          |
|    mean_cost_advantages      | 0.00034013245 |
|    mean_reward_advantages    | -0.43524915   |
|    n_updates                 | 4310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.68e-09      |
|    reward_explained_variance | -5.7          |
|    reward_value_loss         | 18.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.17         |
|    ep_rew_mean               | 6.14         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 433          |
|    time_elapsed              | 82005        |
|    total_timesteps           | 55424        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0402       |
|    cost_value_loss           | 4.14e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00095     |
|    learning_rate             | 0.0005       |
|    loss                      | 5.82         |
|    mean_cost_advantages      | 0.0019226193 |
|    mean_reward_advantages    | -1.0703459   |
|    n_updates                 | 4320         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.23e-09    |
|    reward_explained_variance | -1.22        |
|    reward_value_loss         | 10.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 434           |
|    time_elapsed              | 82111         |
|    total_timesteps           | 55552         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0127       |
|    cost_value_loss           | 6.24e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000961     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.89          |
|    mean_cost_advantages      | -0.0019232461 |
|    mean_reward_advantages    | 1.3225695     |
|    n_updates                 | 4330          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.76e-09      |
|    reward_explained_variance | -1.33         |
|    reward_value_loss         | 11.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.67         |
|    ep_rew_mean               | 6.64         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 435          |
|    time_elapsed              | 82224        |
|    total_timesteps           | 55680        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0189      |
|    cost_value_loss           | 5.95e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000967    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.89         |
|    mean_cost_advantages      | 0.0009997059 |
|    mean_reward_advantages    | -1.1049011   |
|    n_updates                 | 4340         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.42e-09    |
|    reward_explained_variance | -0.166       |
|    reward_value_loss         | 4.07         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.92           |
|    ep_rew_mean               | 6.88           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 436            |
|    time_elapsed              | 82338          |
|    total_timesteps           | 55808          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0128         |
|    cost_value_loss           | 4.89e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000949      |
|    learning_rate             | 0.0005         |
|    loss                      | 9.55           |
|    mean_cost_advantages      | -0.00040406242 |
|    mean_reward_advantages    | 1.6874146      |
|    n_updates                 | 4350           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.11e-09      |
|    reward_explained_variance | -4.33          |
|    reward_value_loss         | 20.7           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.06          |
|    ep_rew_mean               | 7.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 437           |
|    time_elapsed              | 82449         |
|    total_timesteps           | 55936         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.356        |
|    cost_value_loss           | 0.000107      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000996     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.87          |
|    mean_cost_advantages      | -0.0018740956 |
|    mean_reward_advantages    | 0.55237883    |
|    n_updates                 | 4360          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.93e-09     |
|    reward_explained_variance | -2.55         |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.49          |
|    ep_rew_mean               | 7.43          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 438           |
|    time_elapsed              | 82563         |
|    total_timesteps           | 56064         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000454     |
|    cost_value_loss           | 0.000134      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000971     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.89          |
|    mean_cost_advantages      | 0.00051689416 |
|    mean_reward_advantages    | -1.1647693    |
|    n_updates                 | 4370          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.62e-10     |
|    reward_explained_variance | -1.53         |
|    reward_value_loss         | 17.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.89          |
|    ep_rew_mean               | 7.83          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 439           |
|    time_elapsed              | 82680         |
|    total_timesteps           | 56192         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0081        |
|    cost_value_loss           | 7.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000975     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.08          |
|    mean_cost_advantages      | 0.00041527947 |
|    mean_reward_advantages    | 0.27356398    |
|    n_updates                 | 4380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.99e-09      |
|    reward_explained_variance | -1.24         |
|    reward_value_loss         | 11.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.66          |
|    ep_rew_mean               | 7.6           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 440           |
|    time_elapsed              | 82797         |
|    total_timesteps           | 56320         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.017         |
|    cost_value_loss           | 0.000111      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000965     |
|    learning_rate             | 0.0005        |
|    loss                      | 11            |
|    mean_cost_advantages      | -0.0016711645 |
|    mean_reward_advantages    | 0.48714903    |
|    n_updates                 | 4390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.15e-09     |
|    reward_explained_variance | -3.96         |
|    reward_value_loss         | 24.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.8          |
|    ep_rew_mean               | 7.74         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 441          |
|    time_elapsed              | 82903        |
|    total_timesteps           | 56448        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00412     |
|    cost_value_loss           | 6.34e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000972    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.97         |
|    mean_cost_advantages      | 0.0030537061 |
|    mean_reward_advantages    | -0.30416584  |
|    n_updates                 | 4400         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3e-09        |
|    reward_explained_variance | -0.424       |
|    reward_value_loss         | 10.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.57          |
|    ep_rew_mean               | 7.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 442           |
|    time_elapsed              | 83015         |
|    total_timesteps           | 56576         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.193        |
|    cost_value_loss           | 0.000114      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000949     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.09          |
|    mean_cost_advantages      | -0.0007172751 |
|    mean_reward_advantages    | 0.7642536     |
|    n_updates                 | 4410          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.7e-09      |
|    reward_explained_variance | -0.358        |
|    reward_value_loss         | 15.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.92         |
|    ep_rew_mean               | 6.87         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 443          |
|    time_elapsed              | 83145        |
|    total_timesteps           | 56704        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0101      |
|    cost_value_loss           | 8.35e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000975    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.86         |
|    mean_cost_advantages      | 0.0018476108 |
|    mean_reward_advantages    | 0.56424654   |
|    n_updates                 | 4420         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.86e-09     |
|    reward_explained_variance | -0.268       |
|    reward_value_loss         | 13.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.08          |
|    ep_rew_mean               | 7.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 444           |
|    time_elapsed              | 83266         |
|    total_timesteps           | 56832         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.019        |
|    cost_value_loss           | 5.3e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000948     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.74          |
|    mean_cost_advantages      | 0.00021411365 |
|    mean_reward_advantages    | -1.2876155    |
|    n_updates                 | 4430          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.2e-09       |
|    reward_explained_variance | -1.67         |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.8           |
|    ep_rew_mean               | 6.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 445           |
|    time_elapsed              | 83387         |
|    total_timesteps           | 56960         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00406       |
|    cost_value_loss           | 7.68e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000946     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.78          |
|    mean_cost_advantages      | -0.0011834244 |
|    mean_reward_advantages    | 0.31043786    |
|    n_updates                 | 4440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.22e-09      |
|    reward_explained_variance | -0.784        |
|    reward_value_loss         | 11.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.64         |
|    ep_rew_mean               | 6.6          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 446          |
|    time_elapsed              | 83511        |
|    total_timesteps           | 57088        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0237      |
|    cost_value_loss           | 4.91e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000957    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.49         |
|    mean_cost_advantages      | 6.931787e-05 |
|    mean_reward_advantages    | -0.8486692   |
|    n_updates                 | 4450         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.24e-09    |
|    reward_explained_variance | -1.59        |
|    reward_value_loss         | 8.51         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.94         |
|    ep_rew_mean               | 5.9          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 447          |
|    time_elapsed              | 83639        |
|    total_timesteps           | 57216        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0228       |
|    cost_value_loss           | 8.75e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000967    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.98         |
|    mean_cost_advantages      | 5.050673e-05 |
|    mean_reward_advantages    | 0.58804846   |
|    n_updates                 | 4460         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.14e-09    |
|    reward_explained_variance | -0.878       |
|    reward_value_loss         | 13           |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.79          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 448           |
|    time_elapsed              | 83763         |
|    total_timesteps           | 57344         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00294      |
|    cost_value_loss           | 5.13e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000963     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.3           |
|    mean_cost_advantages      | -0.0010429601 |
|    mean_reward_advantages    | -0.9308913    |
|    n_updates                 | 4470          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.15e-09     |
|    reward_explained_variance | -1.24         |
|    reward_value_loss         | 12            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.55          |
|    ep_rew_mean               | 5.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 449           |
|    time_elapsed              | 83884         |
|    total_timesteps           | 57472         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.208        |
|    cost_value_loss           | 7.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000981     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.66          |
|    mean_cost_advantages      | 0.00091536925 |
|    mean_reward_advantages    | 1.239505      |
|    n_updates                 | 4480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.13e-10     |
|    reward_explained_variance | -1.36         |
|    reward_value_loss         | 12.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.96         |
|    ep_rew_mean               | 5.92         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 450          |
|    time_elapsed              | 84015        |
|    total_timesteps           | 57600        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0329       |
|    cost_value_loss           | 6.07e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00096     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.29         |
|    mean_cost_advantages      | 0.0005992944 |
|    mean_reward_advantages    | 0.07225333   |
|    n_updates                 | 4490         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.07e-09     |
|    reward_explained_variance | -1.23        |
|    reward_value_loss         | 8.35         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.95           |
|    ep_rew_mean               | 5.91           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 451            |
|    time_elapsed              | 84136          |
|    total_timesteps           | 57728          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0214        |
|    cost_value_loss           | 7.21e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00096       |
|    learning_rate             | 0.0005         |
|    loss                      | 7.73           |
|    mean_cost_advantages      | -0.00031207583 |
|    mean_reward_advantages    | 0.79164696     |
|    n_updates                 | 4500           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1e-08         |
|    reward_explained_variance | -3.2           |
|    reward_value_loss         | 15.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.79         |
|    ep_rew_mean               | 5.76         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 452          |
|    time_elapsed              | 84251        |
|    total_timesteps           | 57856        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0153       |
|    cost_value_loss           | 7.89e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000938    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.29         |
|    mean_cost_advantages      | 0.0008554786 |
|    mean_reward_advantages    | -1.2337217   |
|    n_updates                 | 4510         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.73e-09     |
|    reward_explained_variance | -0.746       |
|    reward_value_loss         | 7.95         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.25         |
|    ep_rew_mean               | 6.22         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 453          |
|    time_elapsed              | 84367        |
|    total_timesteps           | 57984        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0839      |
|    cost_value_loss           | 6.96e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000968    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.07         |
|    mean_cost_advantages      | 0.0011611172 |
|    mean_reward_advantages    | 0.84598196   |
|    n_updates                 | 4520         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.92e-09    |
|    reward_explained_variance | -0.411       |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.31           |
|    ep_rew_mean               | 6.29           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 454            |
|    time_elapsed              | 84481          |
|    total_timesteps           | 58112          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | -7.5289645e-05 |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.176         |
|    cost_value_loss           | 8.94e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000971      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.27           |
|    mean_cost_advantages      | -0.0025503952  |
|    mean_reward_advantages    | 1.444073       |
|    n_updates                 | 4530           |
|    nu                        | 1.05           |
|    nu_loss                   | 7.88e-05       |
|    policy_gradient_loss      | -1.76e-08      |
|    reward_explained_variance | -2.12          |
|    reward_value_loss         | 12.7           |
|    total_cost                | -0.009637075   |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.83         |
|    ep_rew_mean               | 5.81         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 455          |
|    time_elapsed              | 84593        |
|    total_timesteps           | 58240        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0114       |
|    cost_value_loss           | 3.67e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000958    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.36         |
|    mean_cost_advantages      | 0.0008955253 |
|    mean_reward_advantages    | -1.3236779   |
|    n_updates                 | 4540         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.07e-09     |
|    reward_explained_variance | 0.238        |
|    reward_value_loss         | 6.89         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.79          |
|    ep_rew_mean               | 5.77          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 456           |
|    time_elapsed              | 84702         |
|    total_timesteps           | 58368         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.076         |
|    cost_value_loss           | 3.01e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000963     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.38          |
|    mean_cost_advantages      | 7.4868905e-05 |
|    mean_reward_advantages    | -0.14705887   |
|    n_updates                 | 4550          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.04e-09      |
|    reward_explained_variance | -0.168        |
|    reward_value_loss         | 7.91          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.13           |
|    ep_rew_mean               | 6.1            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 457            |
|    time_elapsed              | 84814          |
|    total_timesteps           | 58496          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.015          |
|    cost_value_loss           | 5.17e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.00099       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.76           |
|    mean_cost_advantages      | -0.00021608995 |
|    mean_reward_advantages    | 0.24274716     |
|    n_updates                 | 4560           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.7e-09       |
|    reward_explained_variance | 0.0894         |
|    reward_value_loss         | 5.65           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.14         |
|    ep_rew_mean               | 6.11         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 458          |
|    time_elapsed              | 84930        |
|    total_timesteps           | 58624        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0737      |
|    cost_value_loss           | 9.82e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000962    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.4          |
|    mean_cost_advantages      | -0.000502854 |
|    mean_reward_advantages    | 2.6896873    |
|    n_updates                 | 4570         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.64e-09    |
|    reward_explained_variance | -0.55        |
|    reward_value_loss         | 15.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.05         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 459          |
|    time_elapsed              | 85047        |
|    total_timesteps           | 58752        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0841      |
|    cost_value_loss           | 7.28e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000953    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.69         |
|    mean_cost_advantages      | 0.0004614814 |
|    mean_reward_advantages    | -0.62762094  |
|    n_updates                 | 4580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.01e-09     |
|    reward_explained_variance | -1.12        |
|    reward_value_loss         | 11.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.52          |
|    ep_rew_mean               | 6.48          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 460           |
|    time_elapsed              | 85167         |
|    total_timesteps           | 58880         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0177       |
|    cost_value_loss           | 5.67e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000953     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.96          |
|    mean_cost_advantages      | -0.0008773337 |
|    mean_reward_advantages    | -1.1438723    |
|    n_updates                 | 4590          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.66e-09     |
|    reward_explained_variance | -0.167        |
|    reward_value_loss         | 5.01          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.66          |
|    ep_rew_mean               | 6.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 461           |
|    time_elapsed              | 85291         |
|    total_timesteps           | 59008         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0458        |
|    cost_value_loss           | 6.91e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000979     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.66          |
|    mean_cost_advantages      | 0.00089063204 |
|    mean_reward_advantages    | 0.27352846    |
|    n_updates                 | 4600          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.76e-10      |
|    reward_explained_variance | -0.583        |
|    reward_value_loss         | 9.2           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.73           |
|    ep_rew_mean               | 6.69           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 462            |
|    time_elapsed              | 85415          |
|    total_timesteps           | 59136          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0105        |
|    cost_value_loss           | 5.67e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000966      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.18           |
|    mean_cost_advantages      | -0.00031045818 |
|    mean_reward_advantages    | -0.034649506   |
|    n_updates                 | 4610           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.93e-11       |
|    reward_explained_variance | -0.251         |
|    reward_value_loss         | 5.14           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.55          |
|    ep_rew_mean               | 6.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 463           |
|    time_elapsed              | 85536         |
|    total_timesteps           | 59264         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.252        |
|    cost_value_loss           | 7.94e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000964     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.69          |
|    mean_cost_advantages      | -0.0006230762 |
|    mean_reward_advantages    | 0.7070472     |
|    n_updates                 | 4620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.86e-09     |
|    reward_explained_variance | -0.992        |
|    reward_value_loss         | 8.96          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.72          |
|    ep_rew_mean               | 6.67          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 464           |
|    time_elapsed              | 85654         |
|    total_timesteps           | 59392         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0268       |
|    cost_value_loss           | 8.7e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000973     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.51          |
|    mean_cost_advantages      | -0.0008087263 |
|    mean_reward_advantages    | 0.37342536    |
|    n_updates                 | 4630          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.67e-09      |
|    reward_explained_variance | -2.6          |
|    reward_value_loss         | 12.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.68          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 465           |
|    time_elapsed              | 85772         |
|    total_timesteps           | 59520         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0108       |
|    cost_value_loss           | 7.26e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000935     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.49          |
|    mean_cost_advantages      | 0.00092387485 |
|    mean_reward_advantages    | 0.048229218   |
|    n_updates                 | 4640          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.62e-09     |
|    reward_explained_variance | -1.17         |
|    reward_value_loss         | 6.51          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.85           |
|    ep_rew_mean               | 6.81           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 466            |
|    time_elapsed              | 85892          |
|    total_timesteps           | 59648          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.14          |
|    cost_value_loss           | 4.36e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000929      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.21           |
|    mean_cost_advantages      | -0.00048971816 |
|    mean_reward_advantages    | -0.31875414    |
|    n_updates                 | 4650           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.18e-09       |
|    reward_explained_variance | 0.523          |
|    reward_value_loss         | 4.42           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.85           |
|    ep_rew_mean               | 6.81           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 467            |
|    time_elapsed              | 86011          |
|    total_timesteps           | 59776          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0341         |
|    cost_value_loss           | 4.61e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000943      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.46           |
|    mean_cost_advantages      | -0.00074254005 |
|    mean_reward_advantages    | 1.4060307      |
|    n_updates                 | 4660           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.54e-09       |
|    reward_explained_variance | -1.06          |
|    reward_value_loss         | 7.73           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.57          |
|    ep_rew_mean               | 6.53          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 468           |
|    time_elapsed              | 86136         |
|    total_timesteps           | 59904         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0803       |
|    cost_value_loss           | 4.86e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00098      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.84          |
|    mean_cost_advantages      | -4.551161e-05 |
|    mean_reward_advantages    | 0.97802293    |
|    n_updates                 | 4670          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.41e-09     |
|    reward_explained_variance | -2.14         |
|    reward_value_loss         | 8.22          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.32           |
|    ep_rew_mean               | 6.29           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 469            |
|    time_elapsed              | 86251          |
|    total_timesteps           | 60032          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0253        |
|    cost_value_loss           | 5.02e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000949      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.18           |
|    mean_cost_advantages      | -0.00036540002 |
|    mean_reward_advantages    | -0.18771435    |
|    n_updates                 | 4680           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.6e-09        |
|    reward_explained_variance | -0.762         |
|    reward_value_loss         | 5.25           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.59          |
|    ep_rew_mean               | 6.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 470           |
|    time_elapsed              | 86362         |
|    total_timesteps           | 60160         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0117        |
|    cost_value_loss           | 3.93e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000953     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.61          |
|    mean_cost_advantages      | 0.00017824394 |
|    mean_reward_advantages    | 0.4450031     |
|    n_updates                 | 4690          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.87e-10     |
|    reward_explained_variance | -1.35         |
|    reward_value_loss         | 6.59          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.58          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 471           |
|    time_elapsed              | 86474         |
|    total_timesteps           | 60288         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00168       |
|    cost_value_loss           | 7e-05         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000973     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.51          |
|    mean_cost_advantages      | -0.0015783935 |
|    mean_reward_advantages    | 0.18076235    |
|    n_updates                 | 4700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.07e-09     |
|    reward_explained_variance | 0.0499        |
|    reward_value_loss         | 6.35          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.38           |
|    ep_rew_mean               | 6.34           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 472            |
|    time_elapsed              | 86586          |
|    total_timesteps           | 60416          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00798        |
|    cost_value_loss           | 6.86e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000949      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.14           |
|    mean_cost_advantages      | -3.9864943e-05 |
|    mean_reward_advantages    | 0.96806794     |
|    n_updates                 | 4710           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.6e-09       |
|    reward_explained_variance | -1.47          |
|    reward_value_loss         | 14.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.55         |
|    ep_rew_mean               | 6.52         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 473          |
|    time_elapsed              | 86696        |
|    total_timesteps           | 60544        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0252       |
|    cost_value_loss           | 5.11e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000932    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.04         |
|    mean_cost_advantages      | 0.0007505162 |
|    mean_reward_advantages    | -0.004029628 |
|    n_updates                 | 4720         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.83e-10    |
|    reward_explained_variance | -0.6         |
|    reward_value_loss         | 10.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.01          |
|    ep_rew_mean               | 6.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 474           |
|    time_elapsed              | 86807         |
|    total_timesteps           | 60672         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0626        |
|    cost_value_loss           | 7.8e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.00095      |
|    learning_rate             | 0.0005        |
|    loss                      | 7.11          |
|    mean_cost_advantages      | -0.0002235707 |
|    mean_reward_advantages    | 0.8578626     |
|    n_updates                 | 4730          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.38e-10      |
|    reward_explained_variance | -1.11         |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.27          |
|    ep_rew_mean               | 7.23          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 475           |
|    time_elapsed              | 86916         |
|    total_timesteps           | 60800         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00988      |
|    cost_value_loss           | 4.81e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000942     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.34          |
|    mean_cost_advantages      | -0.0012741529 |
|    mean_reward_advantages    | 0.62633824    |
|    n_updates                 | 4740          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.7e-09      |
|    reward_explained_variance | -0.999        |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.73          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 476           |
|    time_elapsed              | 87025         |
|    total_timesteps           | 60928         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00722      |
|    cost_value_loss           | 4.92e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000972     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.72          |
|    mean_cost_advantages      | 0.00010078811 |
|    mean_reward_advantages    | -0.29170287   |
|    n_updates                 | 4750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.04e-09      |
|    reward_explained_variance | -0.0218       |
|    reward_value_loss         | 5.24          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.45           |
|    ep_rew_mean               | 6.42           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 477            |
|    time_elapsed              | 87136          |
|    total_timesteps           | 61056          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00718       |
|    cost_value_loss           | 3.48e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000941      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.21           |
|    mean_cost_advantages      | -0.00048964925 |
|    mean_reward_advantages    | -0.016903043   |
|    n_updates                 | 4760           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.01e-09       |
|    reward_explained_variance | -0.42          |
|    reward_value_loss         | 6.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.14         |
|    ep_rew_mean               | 6.1          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 478          |
|    time_elapsed              | 87247        |
|    total_timesteps           | 61184        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0161      |
|    cost_value_loss           | 7.35e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000955    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.78         |
|    mean_cost_advantages      | 0.0016027042 |
|    mean_reward_advantages    | 0.26694572   |
|    n_updates                 | 4770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.27e-09     |
|    reward_explained_variance | -0.417       |
|    reward_value_loss         | 11.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 479           |
|    time_elapsed              | 87356         |
|    total_timesteps           | 61312         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00365       |
|    cost_value_loss           | 3.92e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000955     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.57          |
|    mean_cost_advantages      | -0.0005098529 |
|    mean_reward_advantages    | 0.07418291    |
|    n_updates                 | 4780          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.39e-09     |
|    reward_explained_variance | -0.349        |
|    reward_value_loss         | 8.99          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.69          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 480           |
|    time_elapsed              | 87467         |
|    total_timesteps           | 61440         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00325      |
|    cost_value_loss           | 4.41e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000956     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.79          |
|    mean_cost_advantages      | -0.0003798251 |
|    mean_reward_advantages    | 0.24170035    |
|    n_updates                 | 4790          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.05e-10      |
|    reward_explained_variance | -0.947        |
|    reward_value_loss         | 8.25          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.15        |
|    ep_rew_mean               | 5.12        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 481         |
|    time_elapsed              | 87578       |
|    total_timesteps           | 61568       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0703      |
|    cost_value_loss           | 5.08e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -0.000972   |
|    learning_rate             | 0.0005      |
|    loss                      | 5.17        |
|    mean_cost_advantages      | 0.00063469  |
|    mean_reward_advantages    | -0.47687992 |
|    n_updates                 | 4800        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -5.15e-09   |
|    reward_explained_variance | -2.52       |
|    reward_value_loss         | 8.16        |
|    total_cost                | 0.0         |
----------------------------------------------
---------------------------------------------
| rollout/                     |            |
|    ep_len_mean               | 6.02       |
|    ep_rew_mean               | 4.99       |
| time/                        |            |
|    fps                       | 0          |
|    iterations                | 482        |
|    time_elapsed              | 87689      |
|    total_timesteps           | 61696      |
| train/                       |            |
|    approx_kl                 | 0.0        |
|    average_cost              | 0.0        |
|    clip_fraction             | 0          |
|    clip_range                | 0.2        |
|    cost_explained_variance   | 0.0268     |
|    cost_value_loss           | 4.4e-05    |
|    early_stop_epoch          | 10         |
|    entropy_loss              | -0.000973  |
|    learning_rate             | 0.0005     |
|    loss                      | 2.24       |
|    mean_cost_advantages      | 0.00084605 |
|    mean_reward_advantages    | -1.2301173 |
|    n_updates                 | 4810       |
|    nu                        | 1.05       |
|    nu_loss                   | -0         |
|    policy_gradient_loss      | -1.76e-09  |
|    reward_explained_variance | -0.434     |
|    reward_value_loss         | 4.82       |
|    total_cost                | 0.0        |
---------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.09          |
|    ep_rew_mean               | 5.07          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 483           |
|    time_elapsed              | 87797         |
|    total_timesteps           | 61824         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0085       |
|    cost_value_loss           | 5.91e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000965     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.75          |
|    mean_cost_advantages      | 0.00056642777 |
|    mean_reward_advantages    | 0.26672003    |
|    n_updates                 | 4820          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.49e-09     |
|    reward_explained_variance | -1.04         |
|    reward_value_loss         | 5.97          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 5.78          |
|    ep_rew_mean               | 4.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 484           |
|    time_elapsed              | 87909         |
|    total_timesteps           | 61952         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0426        |
|    cost_value_loss           | 4e-05         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000931     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.23          |
|    mean_cost_advantages      | -0.0017541759 |
|    mean_reward_advantages    | 1.030081      |
|    n_updates                 | 4830          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.6e-09      |
|    reward_explained_variance | -1.42         |
|    reward_value_loss         | 7.95          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.26          |
|    ep_rew_mean               | 5.23          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 485           |
|    time_elapsed              | 88018         |
|    total_timesteps           | 62080         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0102       |
|    cost_value_loss           | 3.8e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000964     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.72          |
|    mean_cost_advantages      | -0.0006452157 |
|    mean_reward_advantages    | -0.33341023   |
|    n_updates                 | 4840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.51e-09     |
|    reward_explained_variance | -0.169        |
|    reward_value_loss         | 3.69          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.74          |
|    ep_rew_mean               | 5.71          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 486           |
|    time_elapsed              | 88131         |
|    total_timesteps           | 62208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0045        |
|    cost_value_loss           | 4.51e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000972     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.09          |
|    mean_cost_advantages      | -0.0009830897 |
|    mean_reward_advantages    | 0.7827847     |
|    n_updates                 | 4850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.71e-09      |
|    reward_explained_variance | -1.65         |
|    reward_value_loss         | 7.18          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.25          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 487           |
|    time_elapsed              | 88241         |
|    total_timesteps           | 62336         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.16         |
|    cost_value_loss           | 7.63e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000974     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.86          |
|    mean_cost_advantages      | -0.0005537804 |
|    mean_reward_advantages    | 1.621856      |
|    n_updates                 | 4860          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.63e-09      |
|    reward_explained_variance | -1.72         |
|    reward_value_loss         | 18            |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.89         |
|    ep_rew_mean               | 6.85         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 488          |
|    time_elapsed              | 88352        |
|    total_timesteps           | 62464        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.083       |
|    cost_value_loss           | 6.28e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000968    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.12         |
|    mean_cost_advantages      | 0.0007145123 |
|    mean_reward_advantages    | -1.0212951   |
|    n_updates                 | 4870         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.43e-09    |
|    reward_explained_variance | -1.25        |
|    reward_value_loss         | 9.4          |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.87           |
|    ep_rew_mean               | 6.83           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 489            |
|    time_elapsed              | 88464          |
|    total_timesteps           | 62592          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0391        |
|    cost_value_loss           | 5.95e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -0.000942      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.53           |
|    mean_cost_advantages      | -0.00041974132 |
|    mean_reward_advantages    | -0.2032308     |
|    n_updates                 | 4880           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.43e-09      |
|    reward_explained_variance | -1.11          |
|    reward_value_loss         | 9.93           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.73         |
|    ep_rew_mean               | 6.69         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 490          |
|    time_elapsed              | 88576        |
|    total_timesteps           | 62720        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00601     |
|    cost_value_loss           | 2.72e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.00095     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.93         |
|    mean_cost_advantages      | 0.0008213789 |
|    mean_reward_advantages    | -0.33965015  |
|    n_updates                 | 4890         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.38e-09     |
|    reward_explained_variance | -1.04        |
|    reward_value_loss         | 7.02         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.48         |
|    ep_rew_mean               | 6.45         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 491          |
|    time_elapsed              | 88687        |
|    total_timesteps           | 62848        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0474      |
|    cost_value_loss           | 3.49e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -0.000955    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.37         |
|    mean_cost_advantages      | 8.235256e-05 |
|    mean_reward_advantages    | 0.36773854   |
|    n_updates                 | 4900         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.6e-09     |
|    reward_explained_variance | -2.46        |
|    reward_value_loss         | 10.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 492           |
|    time_elapsed              | 88798         |
|    total_timesteps           | 62976         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0456        |
|    cost_value_loss           | 2.71e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000945     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.9           |
|    mean_cost_advantages      | 0.00016503266 |
|    mean_reward_advantages    | -0.59563005   |
|    n_updates                 | 4910          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.38e-09     |
|    reward_explained_variance | -1.11         |
|    reward_value_loss         | 6.05          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.8           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 493           |
|    time_elapsed              | 88910         |
|    total_timesteps           | 63104         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0271        |
|    cost_value_loss           | 3.02e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000963     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.89          |
|    mean_cost_advantages      | -0.0012192766 |
|    mean_reward_advantages    | 0.312812      |
|    n_updates                 | 4920          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.81e-09      |
|    reward_explained_variance | -6.22         |
|    reward_value_loss         | 11.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.92          |
|    ep_rew_mean               | 5.89          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 494           |
|    time_elapsed              | 89021         |
|    total_timesteps           | 63232         |
| train/                       |               |
|    approx_kl                 | 0.02754531    |
|    average_cost              | 0.0           |
|    clip_fraction             | 0.00703       |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0256       |
|    cost_value_loss           | 4.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -0.000283     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.12          |
|    mean_cost_advantages      | 0.00083862466 |
|    mean_reward_advantages    | 0.019157022   |
|    n_updates                 | 4930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -0.00142      |
|    reward_explained_variance | -0.368        |
|    reward_value_loss         | 4.86          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.43           |
|    ep_rew_mean               | 6.39           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 495            |
|    time_elapsed              | 89132          |
|    total_timesteps           | 63360          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0162        |
|    cost_value_loss           | 5.92e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.88           |
|    mean_cost_advantages      | -2.5588539e-05 |
|    mean_reward_advantages    | 0.72305477     |
|    n_updates                 | 4940           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.3e-08       |
|    reward_explained_variance | -0.414         |
|    reward_value_loss         | 7.72           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.55          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 496           |
|    time_elapsed              | 89243         |
|    total_timesteps           | 63488         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0165        |
|    cost_value_loss           | 3.19e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.06e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.09          |
|    mean_cost_advantages      | -0.0012990047 |
|    mean_reward_advantages    | 2.1470332     |
|    n_updates                 | 4950          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.34e-08     |
|    reward_explained_variance | -6.46         |
|    reward_value_loss         | 14.4          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.77        |
|    ep_rew_mean               | 5.73        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 497         |
|    time_elapsed              | 89355       |
|    total_timesteps           | 63616       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0289     |
|    cost_value_loss           | 6.35e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -5.01e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.3         |
|    mean_cost_advantages      | 0.000855619 |
|    mean_reward_advantages    | -1.4262888  |
|    n_updates                 | 4960        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -1.33e-09   |
|    reward_explained_variance | -1.85       |
|    reward_value_loss         | 10.7        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 498           |
|    time_elapsed              | 89467         |
|    total_timesteps           | 63744         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00833      |
|    cost_value_loss           | 5.67e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.79          |
|    mean_cost_advantages      | 0.00028428802 |
|    mean_reward_advantages    | -1.0114856    |
|    n_updates                 | 4970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.09e-09      |
|    reward_explained_variance | -0.15         |
|    reward_value_loss         | 6.36          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 499           |
|    time_elapsed              | 89578         |
|    total_timesteps           | 63872         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00693       |
|    cost_value_loss           | 5.32e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.1           |
|    mean_cost_advantages      | -0.0018680291 |
|    mean_reward_advantages    | 1.3771989     |
|    n_updates                 | 4980          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.41e-09      |
|    reward_explained_variance | -0.781        |
|    reward_value_loss         | 8.18          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.31           |
|    ep_rew_mean               | 5.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 500            |
|    time_elapsed              | 89690          |
|    total_timesteps           | 64000          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0197         |
|    cost_value_loss           | 4.11e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.95e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.02           |
|    mean_cost_advantages      | -0.00015550765 |
|    mean_reward_advantages    | -0.37106827    |
|    n_updates                 | 4990           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.86e-09       |
|    reward_explained_variance | -2.43          |
|    reward_value_loss         | 14.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.27           |
|    ep_rew_mean               | 5.24           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 501            |
|    time_elapsed              | 89803          |
|    total_timesteps           | 64128          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0188         |
|    cost_value_loss           | 2.19e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.9e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.87           |
|    mean_cost_advantages      | -1.8444902e-05 |
|    mean_reward_advantages    | -0.8938947     |
|    n_updates                 | 5000           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.56e-09      |
|    reward_explained_variance | -0.722         |
|    reward_value_loss         | 5.73           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.47         |
|    ep_rew_mean               | 5.45         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 502          |
|    time_elapsed              | 89916        |
|    total_timesteps           | 64256        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0136      |
|    cost_value_loss           | 5.14e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.88e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.18         |
|    mean_cost_advantages      | 0.0008329547 |
|    mean_reward_advantages    | -0.071894065 |
|    n_updates                 | 5010         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.38e-10     |
|    reward_explained_variance | -0.318       |
|    reward_value_loss         | 5.05         |
|    total_cost                | 0.0          |
-----------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.21        |
|    ep_rew_mean               | 5.19        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 503         |
|    time_elapsed              | 90030       |
|    total_timesteps           | 64384       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.00199     |
|    cost_value_loss           | 3.32e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -4.99e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 1.91        |
|    mean_cost_advantages      | 0.001231086 |
|    mean_reward_advantages    | -0.73314345 |
|    n_updates                 | 5020        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 1.75e-09    |
|    reward_explained_variance | 0.182       |
|    reward_value_loss         | 4.09        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.35          |
|    ep_rew_mean               | 5.33          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 504           |
|    time_elapsed              | 90142         |
|    total_timesteps           | 64512         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000438     |
|    cost_value_loss           | 3.68e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.75e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.33          |
|    mean_cost_advantages      | -0.0009944475 |
|    mean_reward_advantages    | 0.6958591     |
|    n_updates                 | 5030          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.78e-10      |
|    reward_explained_variance | -0.814        |
|    reward_value_loss         | 5.97          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.43          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 505           |
|    time_elapsed              | 90254         |
|    total_timesteps           | 64640         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0195       |
|    cost_value_loss           | 4.87e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.81e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.21          |
|    mean_cost_advantages      | 0.00033432024 |
|    mean_reward_advantages    | -0.04355541   |
|    n_updates                 | 5040          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.22e-09      |
|    reward_explained_variance | -0.99         |
|    reward_value_loss         | 6.91          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.4           |
|    ep_rew_mean               | 5.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 506           |
|    time_elapsed              | 90365         |
|    total_timesteps           | 64768         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0114        |
|    cost_value_loss           | 3.6e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.91e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.67          |
|    mean_cost_advantages      | -0.0008254947 |
|    mean_reward_advantages    | -0.16593203   |
|    n_updates                 | 5050          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.63e-10      |
|    reward_explained_variance | -1.46         |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.55        |
|    ep_rew_mean               | 5.52        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 507         |
|    time_elapsed              | 90478       |
|    total_timesteps           | 64896       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.0788     |
|    cost_value_loss           | 3.93e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -4.96e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 7.09        |
|    mean_cost_advantages      | 0.000568141 |
|    mean_reward_advantages    | 1.3872335   |
|    n_updates                 | 5060        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -4.73e-09   |
|    reward_explained_variance | -5.08       |
|    reward_value_loss         | 17          |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.93           |
|    ep_rew_mean               | 5.9            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 508            |
|    time_elapsed              | 90590          |
|    total_timesteps           | 65024          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0886        |
|    cost_value_loss           | 3.69e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.94e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.57           |
|    mean_cost_advantages      | -0.00014104901 |
|    mean_reward_advantages    | -1.2506338     |
|    n_updates                 | 5070           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.47e-09       |
|    reward_explained_variance | 0.0264         |
|    reward_value_loss         | 10.1           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.17         |
|    ep_rew_mean               | 6.14         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 509          |
|    time_elapsed              | 90703        |
|    total_timesteps           | 65152        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0129      |
|    cost_value_loss           | 4.36e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.05e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.22         |
|    mean_cost_advantages      | 0.0007054022 |
|    mean_reward_advantages    | 0.46500933   |
|    n_updates                 | 5080         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.07e-09    |
|    reward_explained_variance | -1.64        |
|    reward_value_loss         | 10.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 510           |
|    time_elapsed              | 90816         |
|    total_timesteps           | 65280         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0304        |
|    cost_value_loss           | 4.68e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.56          |
|    mean_cost_advantages      | -0.0013310021 |
|    mean_reward_advantages    | -0.51285446   |
|    n_updates                 | 5090          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.81e-09      |
|    reward_explained_variance | -1.04         |
|    reward_value_loss         | 7.29          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.69         |
|    ep_rew_mean               | 6.66         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 511          |
|    time_elapsed              | 90930        |
|    total_timesteps           | 65408        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00218     |
|    cost_value_loss           | 4.95e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.92e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.53         |
|    mean_cost_advantages      | 0.0007218675 |
|    mean_reward_advantages    | 0.6942383    |
|    n_updates                 | 5100         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.55e-09    |
|    reward_explained_variance | -0.171       |
|    reward_value_loss         | 6.26         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.7           |
|    ep_rew_mean               | 6.67          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 512           |
|    time_elapsed              | 91042         |
|    total_timesteps           | 65536         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0852        |
|    cost_value_loss           | 6.08e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.98e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.93          |
|    mean_cost_advantages      | -0.0013599878 |
|    mean_reward_advantages    | 1.2009219     |
|    n_updates                 | 5110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.32e-12      |
|    reward_explained_variance | -2.26         |
|    reward_value_loss         | 12.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.54           |
|    ep_rew_mean               | 6.51           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 513            |
|    time_elapsed              | 91157          |
|    total_timesteps           | 65664          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0559         |
|    cost_value_loss           | 4.79e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.8            |
|    mean_cost_advantages      | -0.00076829834 |
|    mean_reward_advantages    | -0.6602798     |
|    n_updates                 | 5120           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.25e-10      |
|    reward_explained_variance | -1.37          |
|    reward_value_loss         | 12             |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.48         |
|    ep_rew_mean               | 6.45         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 514          |
|    time_elapsed              | 91270        |
|    total_timesteps           | 65792        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00289      |
|    cost_value_loss           | 3.52e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.91         |
|    mean_cost_advantages      | 0.0012106892 |
|    mean_reward_advantages    | -0.81034565  |
|    n_updates                 | 5130         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.54e-10     |
|    reward_explained_variance | -0.307       |
|    reward_value_loss         | 7.38         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.49          |
|    ep_rew_mean               | 6.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 515           |
|    time_elapsed              | 91380         |
|    total_timesteps           | 65920         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0144       |
|    cost_value_loss           | 2.95e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.73          |
|    mean_cost_advantages      | 0.00036285497 |
|    mean_reward_advantages    | -0.107669026  |
|    n_updates                 | 5140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.02e-12      |
|    reward_explained_variance | 0.0477        |
|    reward_value_loss         | 6.53          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.33         |
|    ep_rew_mean               | 6.31         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 516          |
|    time_elapsed              | 91492        |
|    total_timesteps           | 66048        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0377       |
|    cost_value_loss           | 3.01e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.78e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.4          |
|    mean_cost_advantages      | 0.0008878524 |
|    mean_reward_advantages    | -0.36808342  |
|    n_updates                 | 5150         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.48e-09     |
|    reward_explained_variance | -0.511       |
|    reward_value_loss         | 8            |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.49          |
|    ep_rew_mean               | 6.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 517           |
|    time_elapsed              | 91604         |
|    total_timesteps           | 66176         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0104       |
|    cost_value_loss           | 5.13e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 10.2          |
|    mean_cost_advantages      | 0.00015746005 |
|    mean_reward_advantages    | 2.3729753     |
|    n_updates                 | 5160          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.15e-09     |
|    reward_explained_variance | -3.37         |
|    reward_value_loss         | 21.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.14          |
|    ep_rew_mean               | 6.12          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 518           |
|    time_elapsed              | 91719         |
|    total_timesteps           | 66304         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0094        |
|    cost_value_loss           | 3.84e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5e-05        |
|    learning_rate             | 0.0005        |
|    loss                      | 3.32          |
|    mean_cost_advantages      | 0.00032803573 |
|    mean_reward_advantages    | -1.493973     |
|    n_updates                 | 5170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.76e-09     |
|    reward_explained_variance | -1.19         |
|    reward_value_loss         | 8.95          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.44           |
|    ep_rew_mean               | 6.41           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 519            |
|    time_elapsed              | 91832          |
|    total_timesteps           | 66432          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0202         |
|    cost_value_loss           | 4.13e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.8e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.64           |
|    mean_cost_advantages      | -0.00084659347 |
|    mean_reward_advantages    | -0.54164875    |
|    n_updates                 | 5180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.45e-09       |
|    reward_explained_variance | -0.315         |
|    reward_value_loss         | 6.88           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.32          |
|    ep_rew_mean               | 6.29          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 520           |
|    time_elapsed              | 91944         |
|    total_timesteps           | 66560         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00166      |
|    cost_value_loss           | 2.73e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.25          |
|    mean_cost_advantages      | -0.0004209205 |
|    mean_reward_advantages    | 0.6116971     |
|    n_updates                 | 5190          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.6e-11       |
|    reward_explained_variance | -0.311        |
|    reward_value_loss         | 6.54          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.98         |
|    ep_rew_mean               | 5.95         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 521          |
|    time_elapsed              | 92057        |
|    total_timesteps           | 66688        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00467      |
|    cost_value_loss           | 3.83e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.95e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.24         |
|    mean_cost_advantages      | 6.631084e-05 |
|    mean_reward_advantages    | 0.25168952   |
|    n_updates                 | 5200         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.49e-09    |
|    reward_explained_variance | -0.041       |
|    reward_value_loss         | 4.32         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.66          |
|    ep_rew_mean               | 5.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 522           |
|    time_elapsed              | 92169         |
|    total_timesteps           | 66816         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00299      |
|    cost_value_loss           | 1.96e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.96e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.4           |
|    mean_cost_advantages      | -0.0002513182 |
|    mean_reward_advantages    | -0.01698643   |
|    n_updates                 | 5210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.76e-09      |
|    reward_explained_variance | -1.61         |
|    reward_value_loss         | 7.52          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.97           |
|    ep_rew_mean               | 5.95           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 523            |
|    time_elapsed              | 92281          |
|    total_timesteps           | 66944          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0153        |
|    cost_value_loss           | 4.55e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.58           |
|    mean_cost_advantages      | -3.6934594e-05 |
|    mean_reward_advantages    | 0.17126337     |
|    n_updates                 | 5220           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.54e-10       |
|    reward_explained_variance | -1.16          |
|    reward_value_loss         | 5.9            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.28          |
|    ep_rew_mean               | 6.26          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 524           |
|    time_elapsed              | 92393         |
|    total_timesteps           | 67072         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0227       |
|    cost_value_loss           | 3.17e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.11          |
|    mean_cost_advantages      | 0.00023603381 |
|    mean_reward_advantages    | 0.5591228     |
|    n_updates                 | 5230          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.26e-12     |
|    reward_explained_variance | -0.766        |
|    reward_value_loss         | 6.03          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.1           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 525           |
|    time_elapsed              | 92504         |
|    total_timesteps           | 67200         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00508       |
|    cost_value_loss           | 4.62e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.8e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.54          |
|    mean_cost_advantages      | -0.0003904354 |
|    mean_reward_advantages    | 0.2017805     |
|    n_updates                 | 5240          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.12e-09     |
|    reward_explained_variance | -0.17         |
|    reward_value_loss         | 5.58          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.11          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 526           |
|    time_elapsed              | 92616         |
|    total_timesteps           | 67328         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0094       |
|    cost_value_loss           | 4.54e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.88e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.66          |
|    mean_cost_advantages      | 8.2593964e-05 |
|    mean_reward_advantages    | 0.7933539     |
|    n_updates                 | 5250          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.8e-10      |
|    reward_explained_variance | -2.06         |
|    reward_value_loss         | 14.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.5           |
|    ep_rew_mean               | 6.47          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 527           |
|    time_elapsed              | 92728         |
|    total_timesteps           | 67456         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.175        |
|    cost_value_loss           | 4.74e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.05e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.48          |
|    mean_cost_advantages      | -0.0006923438 |
|    mean_reward_advantages    | 0.035301328   |
|    n_updates                 | 5260          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.62e-10     |
|    reward_explained_variance | -2.47         |
|    reward_value_loss         | 16.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.51          |
|    ep_rew_mean               | 6.48          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 528           |
|    time_elapsed              | 92838         |
|    total_timesteps           | 67584         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0279        |
|    cost_value_loss           | 3.84e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.91e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.13          |
|    mean_cost_advantages      | -0.0006224936 |
|    mean_reward_advantages    | 0.14325273    |
|    n_updates                 | 5270          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.05e-11     |
|    reward_explained_variance | -0.825        |
|    reward_value_loss         | 8.56          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.51           |
|    ep_rew_mean               | 6.48           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 529            |
|    time_elapsed              | 92956          |
|    total_timesteps           | 67712          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0187        |
|    cost_value_loss           | 3.6e-05        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.85e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.97           |
|    mean_cost_advantages      | -0.00088044675 |
|    mean_reward_advantages    | 0.53409284     |
|    n_updates                 | 5280           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.39e-09       |
|    reward_explained_variance | -0.95          |
|    reward_value_loss         | 7.1            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.51          |
|    ep_rew_mean               | 6.47          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 530           |
|    time_elapsed              | 93072         |
|    total_timesteps           | 67840         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.008         |
|    cost_value_loss           | 3.76e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.65          |
|    mean_cost_advantages      | 0.00040252978 |
|    mean_reward_advantages    | 0.7717785     |
|    n_updates                 | 5290          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.4e-10      |
|    reward_explained_variance | -1            |
|    reward_value_loss         | 8.16          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.08         |
|    ep_rew_mean               | 6.04         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 531          |
|    time_elapsed              | 93194        |
|    total_timesteps           | 67968        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0946      |
|    cost_value_loss           | 3.23e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.01e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.77         |
|    mean_cost_advantages      | 0.0007913413 |
|    mean_reward_advantages    | 0.3988366    |
|    n_updates                 | 5300         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.41e-09     |
|    reward_explained_variance | -2.15        |
|    reward_value_loss         | 11.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 532           |
|    time_elapsed              | 93314         |
|    total_timesteps           | 68096         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.211        |
|    cost_value_loss           | 5.17e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.84e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.81          |
|    mean_cost_advantages      | -0.0018565344 |
|    mean_reward_advantages    | -0.17237681   |
|    n_updates                 | 5310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.95e-10      |
|    reward_explained_variance | -0.868        |
|    reward_value_loss         | 8.29          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.32         |
|    ep_rew_mean               | 6.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 533          |
|    time_elapsed              | 93435        |
|    total_timesteps           | 68224        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0142       |
|    cost_value_loss           | 3.07e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.06e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.06         |
|    mean_cost_advantages      | 0.0008869037 |
|    mean_reward_advantages    | 0.7535549    |
|    n_updates                 | 5320         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.73e-09    |
|    reward_explained_variance | -0.819       |
|    reward_value_loss         | 9.07         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.22           |
|    ep_rew_mean               | 6.18           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 534            |
|    time_elapsed              | 93554          |
|    total_timesteps           | 68352          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.05           |
|    cost_value_loss           | 4.1e-05        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.94e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.89           |
|    mean_cost_advantages      | -0.00078666327 |
|    mean_reward_advantages    | 1.3659866      |
|    n_updates                 | 5330           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.67e-09      |
|    reward_explained_variance | -1.17          |
|    reward_value_loss         | 11.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.6            |
|    ep_rew_mean               | 5.57           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 535            |
|    time_elapsed              | 93672          |
|    total_timesteps           | 68480          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0245         |
|    cost_value_loss           | 2.72e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.85e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.12           |
|    mean_cost_advantages      | -0.00040995365 |
|    mean_reward_advantages    | 0.35471717     |
|    n_updates                 | 5340           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.53e-09      |
|    reward_explained_variance | -1.28          |
|    reward_value_loss         | 10.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.89          |
|    ep_rew_mean               | 5.86          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 536           |
|    time_elapsed              | 93796         |
|    total_timesteps           | 68608         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0592       |
|    cost_value_loss           | 3.75e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.71          |
|    mean_cost_advantages      | -0.0004237534 |
|    mean_reward_advantages    | -0.5055007    |
|    n_updates                 | 5350          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.03e-09     |
|    reward_explained_variance | -0.375        |
|    reward_value_loss         | 5.34          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.14          |
|    ep_rew_mean               | 6.11          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 537           |
|    time_elapsed              | 93914         |
|    total_timesteps           | 68736         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000796     |
|    cost_value_loss           | 3.67e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.04e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.6           |
|    mean_cost_advantages      | -0.0002570544 |
|    mean_reward_advantages    | 0.3582079     |
|    n_updates                 | 5360          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.39e-09      |
|    reward_explained_variance | -1.67         |
|    reward_value_loss         | 8.29          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.9            |
|    ep_rew_mean               | 5.87           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 538            |
|    time_elapsed              | 94031          |
|    total_timesteps           | 68864          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0171        |
|    cost_value_loss           | 3.71e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.89e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.25           |
|    mean_cost_advantages      | -0.00043108553 |
|    mean_reward_advantages    | 1.2622759      |
|    n_updates                 | 5370           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.51e-09       |
|    reward_explained_variance | -1.42          |
|    reward_value_loss         | 10.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.97          |
|    ep_rew_mean               | 5.94          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 539           |
|    time_elapsed              | 94148         |
|    total_timesteps           | 68992         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0697       |
|    cost_value_loss           | 2.49e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.39          |
|    mean_cost_advantages      | 0.00089030777 |
|    mean_reward_advantages    | -0.5071709    |
|    n_updates                 | 5380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.6e-09      |
|    reward_explained_variance | -0.254        |
|    reward_value_loss         | 5.78          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.33           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 540            |
|    time_elapsed              | 94266          |
|    total_timesteps           | 69120          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.295         |
|    cost_value_loss           | 4.38e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.96e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.75           |
|    mean_cost_advantages      | -0.00015804928 |
|    mean_reward_advantages    | 1.3064325      |
|    n_updates                 | 5390           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.29e-10      |
|    reward_explained_variance | -2.2           |
|    reward_value_loss         | 16.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 541           |
|    time_elapsed              | 94382         |
|    total_timesteps           | 69248         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0441        |
|    cost_value_loss           | 3.34e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.78e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.69          |
|    mean_cost_advantages      | 6.2516134e-05 |
|    mean_reward_advantages    | -0.44052076   |
|    n_updates                 | 5400          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.48e-09      |
|    reward_explained_variance | -1.82         |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.14         |
|    ep_rew_mean               | 6.12         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 542          |
|    time_elapsed              | 94498        |
|    total_timesteps           | 69376        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0105       |
|    cost_value_loss           | 2.51e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.83e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.75         |
|    mean_cost_advantages      | 0.0003640477 |
|    mean_reward_advantages    | -0.44571856  |
|    n_updates                 | 5410         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.7e-10     |
|    reward_explained_variance | -0.364       |
|    reward_value_loss         | 7.22         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.05         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 543          |
|    time_elapsed              | 94611        |
|    total_timesteps           | 69504        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.22        |
|    cost_value_loss           | 4.3e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.81e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 8.31         |
|    mean_cost_advantages      | 0.0011732734 |
|    mean_reward_advantages    | 1.6441052    |
|    n_updates                 | 5420         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 9.15e-10     |
|    reward_explained_variance | -2.23        |
|    reward_value_loss         | 20.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.33           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 544            |
|    time_elapsed              | 94725          |
|    total_timesteps           | 69632          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00186       |
|    cost_value_loss           | 2.67e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.04e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.71           |
|    mean_cost_advantages      | -0.00033884958 |
|    mean_reward_advantages    | -2.3472838     |
|    n_updates                 | 5430           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.09e-09      |
|    reward_explained_variance | -0.511         |
|    reward_value_loss         | 7.26           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.9           |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 545           |
|    time_elapsed              | 94837         |
|    total_timesteps           | 69760         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00672      |
|    cost_value_loss           | 3.34e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.84e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.31          |
|    mean_cost_advantages      | -0.0007866355 |
|    mean_reward_advantages    | 1.9896954     |
|    n_updates                 | 5440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.69e-10      |
|    reward_explained_variance | -3.11         |
|    reward_value_loss         | 12.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.27           |
|    ep_rew_mean               | 6.24           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 546            |
|    time_elapsed              | 94950          |
|    total_timesteps           | 69888          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.326         |
|    cost_value_loss           | 3.66e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.97e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.04           |
|    mean_cost_advantages      | -0.00062003016 |
|    mean_reward_advantages    | 0.04708184     |
|    n_updates                 | 5450           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.89e-10       |
|    reward_explained_variance | -0.115         |
|    reward_value_loss         | 5.62           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.42          |
|    ep_rew_mean               | 6.39          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 547           |
|    time_elapsed              | 95061         |
|    total_timesteps           | 70016         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00982      |
|    cost_value_loss           | 4.61e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.94e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.61          |
|    mean_cost_advantages      | 1.3710291e-05 |
|    mean_reward_advantages    | 1.3712721     |
|    n_updates                 | 5460          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.13e-09     |
|    reward_explained_variance | -1.14         |
|    reward_value_loss         | 9.6           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8             |
|    ep_rew_mean               | 6.95          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 548           |
|    time_elapsed              | 95174         |
|    total_timesteps           | 70144         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00678      |
|    cost_value_loss           | 4.08e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.96e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 10.6          |
|    mean_cost_advantages      | 0.00021387408 |
|    mean_reward_advantages    | 1.5929929     |
|    n_updates                 | 5470          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.14e-09      |
|    reward_explained_variance | -5.15         |
|    reward_value_loss         | 25.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.13          |
|    ep_rew_mean               | 7.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 549           |
|    time_elapsed              | 95285         |
|    total_timesteps           | 70272         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0111        |
|    cost_value_loss           | 4.21e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5e-05        |
|    learning_rate             | 0.0005        |
|    loss                      | 6.56          |
|    mean_cost_advantages      | -0.0010731376 |
|    mean_reward_advantages    | -1.0439179    |
|    n_updates                 | 5480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.64e-10     |
|    reward_explained_variance | -3.65         |
|    reward_value_loss         | 13.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.14          |
|    ep_rew_mean               | 7.1           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 550           |
|    time_elapsed              | 95395         |
|    total_timesteps           | 70400         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0803        |
|    cost_value_loss           | 2.34e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.94          |
|    mean_cost_advantages      | 0.00080284814 |
|    mean_reward_advantages    | -1.9917974    |
|    n_updates                 | 5490          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.7e-11      |
|    reward_explained_variance | -0.465        |
|    reward_value_loss         | 6.81          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.8           |
|    ep_rew_mean               | 6.77          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 551           |
|    time_elapsed              | 95508         |
|    total_timesteps           | 70528         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0124       |
|    cost_value_loss           | 3.12e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.02e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.78          |
|    mean_cost_advantages      | 0.00026482408 |
|    mean_reward_advantages    | 0.13976151    |
|    n_updates                 | 5500          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.84e-10     |
|    reward_explained_variance | -0.227        |
|    reward_value_loss         | 6.49          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.58           |
|    ep_rew_mean               | 6.55           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 552            |
|    time_elapsed              | 95620          |
|    total_timesteps           | 70656          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00987        |
|    cost_value_loss           | 2.05e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.79e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.71           |
|    mean_cost_advantages      | -0.00046458788 |
|    mean_reward_advantages    | -0.4473633     |
|    n_updates                 | 5510           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.86e-10      |
|    reward_explained_variance | -0.822         |
|    reward_value_loss         | 7.74           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.62           |
|    ep_rew_mean               | 6.58           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 553            |
|    time_elapsed              | 95731          |
|    total_timesteps           | 70784          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0269        |
|    cost_value_loss           | 3.04e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.93e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.87           |
|    mean_cost_advantages      | -0.00041987724 |
|    mean_reward_advantages    | 1.1434475      |
|    n_updates                 | 5520           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.2e-10        |
|    reward_explained_variance | -0.527         |
|    reward_value_loss         | 8.74           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.31          |
|    ep_rew_mean               | 6.28          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 554           |
|    time_elapsed              | 95843         |
|    total_timesteps           | 70912         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0169        |
|    cost_value_loss           | 2.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.01e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.13          |
|    mean_cost_advantages      | 0.00070544065 |
|    mean_reward_advantages    | 0.77266777    |
|    n_updates                 | 5530          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.24e-09     |
|    reward_explained_variance | -1.33         |
|    reward_value_loss         | 8.28          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.36         |
|    ep_rew_mean               | 6.33         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 555          |
|    time_elapsed              | 95954        |
|    total_timesteps           | 71040        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00362      |
|    cost_value_loss           | 2.61e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.89e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.45         |
|    mean_cost_advantages      | 0.0010913127 |
|    mean_reward_advantages    | -0.016642824 |
|    n_updates                 | 5540         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.98e-10    |
|    reward_explained_variance | -0.767       |
|    reward_value_loss         | 8.82         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.58          |
|    ep_rew_mean               | 6.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 556           |
|    time_elapsed              | 96069         |
|    total_timesteps           | 71168         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00344       |
|    cost_value_loss           | 3.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.02e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.45          |
|    mean_cost_advantages      | -0.0011007569 |
|    mean_reward_advantages    | 1.3397309     |
|    n_updates                 | 5550          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.79e-09     |
|    reward_explained_variance | -3.73         |
|    reward_value_loss         | 19.5          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.8          |
|    ep_rew_mean               | 6.76         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 557          |
|    time_elapsed              | 96186        |
|    total_timesteps           | 71296        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0543       |
|    cost_value_loss           | 3.35e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.89e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 8.04         |
|    mean_cost_advantages      | 2.429908e-06 |
|    mean_reward_advantages    | 0.38117403   |
|    n_updates                 | 5560         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.6e-10     |
|    reward_explained_variance | -2.68        |
|    reward_value_loss         | 18.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.31         |
|    ep_rew_mean               | 6.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 558          |
|    time_elapsed              | 96298        |
|    total_timesteps           | 71424        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00982      |
|    cost_value_loss           | 2.12e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.91e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.18         |
|    mean_cost_advantages      | 0.0009261532 |
|    mean_reward_advantages    | -1.7467599   |
|    n_updates                 | 5570         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.09e-09     |
|    reward_explained_variance | -0.709       |
|    reward_value_loss         | 8.67         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.56         |
|    ep_rew_mean               | 6.53         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 559          |
|    time_elapsed              | 96425        |
|    total_timesteps           | 71552        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0157      |
|    cost_value_loss           | 2.17e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.91e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.87         |
|    mean_cost_advantages      | 0.0003670142 |
|    mean_reward_advantages    | -0.42747584  |
|    n_updates                 | 5580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.15e-09    |
|    reward_explained_variance | -0.00133     |
|    reward_value_loss         | 5.74         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.67           |
|    ep_rew_mean               | 6.63           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 560            |
|    time_elapsed              | 96560          |
|    total_timesteps           | 71680          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0295         |
|    cost_value_loss           | 3.85e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.9e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 6.04           |
|    mean_cost_advantages      | -0.00018898533 |
|    mean_reward_advantages    | 1.1008811      |
|    n_updates                 | 5590           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.19e-09      |
|    reward_explained_variance | -3.08          |
|    reward_value_loss         | 13.1           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 561           |
|    time_elapsed              | 96683         |
|    total_timesteps           | 71808         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00486      |
|    cost_value_loss           | 3.21e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.96e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.85          |
|    mean_cost_advantages      | -0.0004036233 |
|    mean_reward_advantages    | 0.8601966     |
|    n_updates                 | 5600          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.37e-09      |
|    reward_explained_variance | -1.64         |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.32          |
|    ep_rew_mean               | 6.29          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 562           |
|    time_elapsed              | 96807         |
|    total_timesteps           | 71936         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0145        |
|    cost_value_loss           | 3.37e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.62          |
|    mean_cost_advantages      | -3.324676e-06 |
|    mean_reward_advantages    | 0.010994732   |
|    n_updates                 | 5610          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.27e-09      |
|    reward_explained_variance | -3.15         |
|    reward_value_loss         | 12.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.37         |
|    ep_rew_mean               | 6.33         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 563          |
|    time_elapsed              | 96925        |
|    total_timesteps           | 72064        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0322      |
|    cost_value_loss           | 2.38e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5e-05       |
|    learning_rate             | 0.0005       |
|    loss                      | 5.22         |
|    mean_cost_advantages      | 0.0013959758 |
|    mean_reward_advantages    | -0.18420683  |
|    n_updates                 | 5620         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.45e-09    |
|    reward_explained_variance | -3.4         |
|    reward_value_loss         | 11           |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.87          |
|    ep_rew_mean               | 5.83          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 564           |
|    time_elapsed              | 97045         |
|    total_timesteps           | 72192         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00363      |
|    cost_value_loss           | 2e-05         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.87e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.5           |
|    mean_cost_advantages      | 0.00022297178 |
|    mean_reward_advantages    | 0.29926717    |
|    n_updates                 | 5630          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.73e-09     |
|    reward_explained_variance | -0.715        |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.46         |
|    ep_rew_mean               | 5.43         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 565          |
|    time_elapsed              | 97167        |
|    total_timesteps           | 72320        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0386       |
|    cost_value_loss           | 3.03e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.76e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.42         |
|    mean_cost_advantages      | 0.0002563456 |
|    mean_reward_advantages    | -0.48470992  |
|    n_updates                 | 5640         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.4e-10     |
|    reward_explained_variance | -2.34        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.69          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 566           |
|    time_elapsed              | 97286         |
|    total_timesteps           | 72448         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0336        |
|    cost_value_loss           | 2.46e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.98e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.85          |
|    mean_cost_advantages      | -0.0013032934 |
|    mean_reward_advantages    | 0.36309248    |
|    n_updates                 | 5650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.08e-09     |
|    reward_explained_variance | -3.71         |
|    reward_value_loss         | 9.85          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.87           |
|    ep_rew_mean               | 5.85           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 567            |
|    time_elapsed              | 97398          |
|    total_timesteps           | 72576          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0163        |
|    cost_value_loss           | 2.26e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.8e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 3.08           |
|    mean_cost_advantages      | -0.00027662935 |
|    mean_reward_advantages    | 1.5566754      |
|    n_updates                 | 5660           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.3e-09       |
|    reward_explained_variance | -1.7           |
|    reward_value_loss         | 7.64           |
|    total_cost                | 0.0            |
-------------------------------------------------
---------------------------------------------
| rollout/                     |            |
|    ep_len_mean               | 7.12       |
|    ep_rew_mean               | 6.09       |
| time/                        |            |
|    fps                       | 0          |
|    iterations                | 568        |
|    time_elapsed              | 97511      |
|    total_timesteps           | 72704      |
| train/                       |            |
|    approx_kl                 | 0.0        |
|    average_cost              | 0.0        |
|    clip_fraction             | 0          |
|    clip_range                | 0.2        |
|    cost_explained_variance   | -0.0774    |
|    cost_value_loss           | 3.6e-05    |
|    early_stop_epoch          | 10         |
|    entropy_loss              | -4.82e-05  |
|    learning_rate             | 0.0005     |
|    loss                      | 4.11       |
|    mean_cost_advantages      | 0.00047717 |
|    mean_reward_advantages    | 1.4859102  |
|    n_updates                 | 5670       |
|    nu                        | 1.05       |
|    nu_loss                   | -0         |
|    policy_gradient_loss      | 1.8e-09    |
|    reward_explained_variance | -1.55      |
|    reward_value_loss         | 11         |
|    total_cost                | 0.0        |
---------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.33         |
|    ep_rew_mean               | 6.3          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 569          |
|    time_elapsed              | 97630        |
|    total_timesteps           | 72832        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0134      |
|    cost_value_loss           | 2.16e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.95e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.55         |
|    mean_cost_advantages      | 0.0006641284 |
|    mean_reward_advantages    | -0.020477623 |
|    n_updates                 | 5680         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.26e-10    |
|    reward_explained_variance | -6.21        |
|    reward_value_loss         | 14.9         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.59          |
|    ep_rew_mean               | 6.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 570           |
|    time_elapsed              | 97748         |
|    total_timesteps           | 72960         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0152        |
|    cost_value_loss           | 2.49e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.8e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.89          |
|    mean_cost_advantages      | 0.00034740698 |
|    mean_reward_advantages    | 1.1562773     |
|    n_updates                 | 5690          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.32e-09     |
|    reward_explained_variance | -1.62         |
|    reward_value_loss         | 12.7          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.31          |
|    ep_rew_mean               | 6.29          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 571           |
|    time_elapsed              | 97862         |
|    total_timesteps           | 73088         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0768       |
|    cost_value_loss           | 2.06e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.74e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.16          |
|    mean_cost_advantages      | -0.0013671662 |
|    mean_reward_advantages    | -0.9485202    |
|    n_updates                 | 5700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.46e-09      |
|    reward_explained_variance | -1.23         |
|    reward_value_loss         | 8.26          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.52         |
|    ep_rew_mean               | 6.49         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 572          |
|    time_elapsed              | 97976        |
|    total_timesteps           | 73216        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00922     |
|    cost_value_loss           | 2.25e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.83e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.39         |
|    mean_cost_advantages      | 0.0014701774 |
|    mean_reward_advantages    | -0.85033226  |
|    n_updates                 | 5710         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.39e-09     |
|    reward_explained_variance | -0.452       |
|    reward_value_loss         | 4.63         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.42          |
|    ep_rew_mean               | 5.4           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 573           |
|    time_elapsed              | 98098         |
|    total_timesteps           | 73344         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0265        |
|    cost_value_loss           | 2.71e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.07e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.86          |
|    mean_cost_advantages      | -0.0011180992 |
|    mean_reward_advantages    | 1.7902048     |
|    n_updates                 | 5720          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.59e-11      |
|    reward_explained_variance | -1.52         |
|    reward_value_loss         | 13.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.06         |
|    ep_rew_mean               | 5.04         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 574          |
|    time_elapsed              | 98218        |
|    total_timesteps           | 73472        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0108       |
|    cost_value_loss           | 1.91e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.84e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.67         |
|    mean_cost_advantages      | 0.0006556156 |
|    mean_reward_advantages    | -1.9407454   |
|    n_updates                 | 5730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.82e-10     |
|    reward_explained_variance | -0.0722      |
|    reward_value_loss         | 5.02         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.05          |
|    ep_rew_mean               | 5.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 575           |
|    time_elapsed              | 98333         |
|    total_timesteps           | 73600         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0291        |
|    cost_value_loss           | 2.25e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.36          |
|    mean_cost_advantages      | -9.251604e-05 |
|    mean_reward_advantages    | 0.1552011     |
|    n_updates                 | 5740          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.56e-10      |
|    reward_explained_variance | -0.294        |
|    reward_value_loss         | 4.87          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.04          |
|    ep_rew_mean               | 5.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 576           |
|    time_elapsed              | 98452         |
|    total_timesteps           | 73728         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0182        |
|    cost_value_loss           | 2.39e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.82e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.57          |
|    mean_cost_advantages      | -0.0005446747 |
|    mean_reward_advantages    | 0.37914398    |
|    n_updates                 | 5750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.16e-10      |
|    reward_explained_variance | -1.12         |
|    reward_value_loss         | 6.55          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.04         |
|    ep_rew_mean               | 5.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 577          |
|    time_elapsed              | 98574        |
|    total_timesteps           | 73856        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000654    |
|    cost_value_loss           | 1.9e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.87e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.06         |
|    mean_cost_advantages      | 0.0005074053 |
|    mean_reward_advantages    | 0.36036703   |
|    n_updates                 | 5760         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.2e-10      |
|    reward_explained_variance | -2.02        |
|    reward_value_loss         | 9.44         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.56         |
|    ep_rew_mean               | 5.54         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 578          |
|    time_elapsed              | 98688        |
|    total_timesteps           | 73984        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0155      |
|    cost_value_loss           | 2.51e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.87e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.25         |
|    mean_cost_advantages      | 0.0006583044 |
|    mean_reward_advantages    | 0.16613936   |
|    n_updates                 | 5770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.18e-10    |
|    reward_explained_variance | -0.383       |
|    reward_value_loss         | 6.89         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.61           |
|    ep_rew_mean               | 5.57           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 579            |
|    time_elapsed              | 98803          |
|    total_timesteps           | 74112          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0444        |
|    cost_value_loss           | 1.99e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.97e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.65           |
|    mean_cost_advantages      | -0.00046157284 |
|    mean_reward_advantages    | -0.33520484    |
|    n_updates                 | 5780           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.45e-10       |
|    reward_explained_variance | -1.98          |
|    reward_value_loss         | 11.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.13           |
|    ep_rew_mean               | 6.09           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 580            |
|    time_elapsed              | 98927          |
|    total_timesteps           | 74240          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0176         |
|    cost_value_loss           | 2.25e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.99e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.65           |
|    mean_cost_advantages      | -0.00012598999 |
|    mean_reward_advantages    | -0.6015389     |
|    n_updates                 | 5790           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.11e-10       |
|    reward_explained_variance | -0.699         |
|    reward_value_loss         | 7.56           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.35        |
|    ep_rew_mean               | 6.3         |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 581         |
|    time_elapsed              | 99059       |
|    total_timesteps           | 74368       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.00548    |
|    cost_value_loss           | 3.07e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -5.02e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 5.47        |
|    mean_cost_advantages      | -0.00040069 |
|    mean_reward_advantages    | 1.7448977   |
|    n_updates                 | 5800        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -5.44e-10   |
|    reward_explained_variance | -1.12       |
|    reward_value_loss         | 11.3        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.66           |
|    ep_rew_mean               | 6.61           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 582            |
|    time_elapsed              | 99180          |
|    total_timesteps           | 74496          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.192         |
|    cost_value_loss           | 1.67e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.01           |
|    mean_cost_advantages      | -0.00021757382 |
|    mean_reward_advantages    | -0.718715      |
|    n_updates                 | 5810           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.23e-09       |
|    reward_explained_variance | -3.11          |
|    reward_value_loss         | 10.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.85         |
|    ep_rew_mean               | 6.8          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 583          |
|    time_elapsed              | 99299        |
|    total_timesteps           | 74624        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.069        |
|    cost_value_loss           | 2.11e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.89         |
|    mean_cost_advantages      | -0.000621345 |
|    mean_reward_advantages    | 0.34694105   |
|    n_updates                 | 5820         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.36e-09    |
|    reward_explained_variance | -2.19        |
|    reward_value_loss         | 15.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.88           |
|    ep_rew_mean               | 6.83           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 584            |
|    time_elapsed              | 99419          |
|    total_timesteps           | 74752          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.000556       |
|    cost_value_loss           | 2.33e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.32           |
|    mean_cost_advantages      | -7.5167525e-05 |
|    mean_reward_advantages    | 0.56507057     |
|    n_updates                 | 5830           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.95e-09      |
|    reward_explained_variance | -2.73          |
|    reward_value_loss         | 14.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.91          |
|    ep_rew_mean               | 6.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 585           |
|    time_elapsed              | 99538         |
|    total_timesteps           | 74880         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0208       |
|    cost_value_loss           | 1.53e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.4           |
|    mean_cost_advantages      | 0.00026872937 |
|    mean_reward_advantages    | -0.889737     |
|    n_updates                 | 5840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.44e-09      |
|    reward_explained_variance | -0.683        |
|    reward_value_loss         | 8.14          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.22          |
|    ep_rew_mean               | 6.19          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 586           |
|    time_elapsed              | 99661         |
|    total_timesteps           | 75008         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00251       |
|    cost_value_loss           | 2.08e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.27          |
|    mean_cost_advantages      | -0.0006004161 |
|    mean_reward_advantages    | -0.43007118   |
|    n_updates                 | 5850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.18e-09     |
|    reward_explained_variance | -2.07         |
|    reward_value_loss         | 10            |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.55           |
|    ep_rew_mean               | 6.52           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 587            |
|    time_elapsed              | 99778          |
|    total_timesteps           | 75136          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00976       |
|    cost_value_loss           | 1.56e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.75e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.63           |
|    mean_cost_advantages      | -0.00015382477 |
|    mean_reward_advantages    | -1.3260372     |
|    n_updates                 | 5860           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.11e-09      |
|    reward_explained_variance | 0.267          |
|    reward_value_loss         | 7.6            |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.27           |
|    ep_rew_mean               | 6.24           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 588            |
|    time_elapsed              | 99898          |
|    total_timesteps           | 75264          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | -6.2519895e-05 |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.159         |
|    cost_value_loss           | 2.84e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.01e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 10.2           |
|    mean_cost_advantages      | -0.0011566433  |
|    mean_reward_advantages    | 2.9009662      |
|    n_updates                 | 5870           |
|    nu                        | 1.05           |
|    nu_loss                   | 6.54e-05       |
|    policy_gradient_loss      | -2.68e-09      |
|    reward_explained_variance | -5.11          |
|    reward_value_loss         | 27             |
|    total_cost                | -0.008002547   |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.58         |
|    ep_rew_mean               | 5.56         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 589          |
|    time_elapsed              | 100020       |
|    total_timesteps           | 75392        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00423      |
|    cost_value_loss           | 2.4e-05      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.96e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.32         |
|    mean_cost_advantages      | 0.0006116815 |
|    mean_reward_advantages    | -2.0981321   |
|    n_updates                 | 5880         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.23e-09    |
|    reward_explained_variance | -0.594       |
|    reward_value_loss         | 13.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.78          |
|    ep_rew_mean               | 5.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 590           |
|    time_elapsed              | 100144        |
|    total_timesteps           | 75520         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0278        |
|    cost_value_loss           | 1.76e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.82e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.21          |
|    mean_cost_advantages      | 0.00014161275 |
|    mean_reward_advantages    | 0.31045377    |
|    n_updates                 | 5890          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.13e-10     |
|    reward_explained_variance | -1.93         |
|    reward_value_loss         | 10.7          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.9           |
|    ep_rew_mean               | 5.88          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 591           |
|    time_elapsed              | 100265        |
|    total_timesteps           | 75648         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00425       |
|    cost_value_loss           | 2.13e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.94e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.57          |
|    mean_cost_advantages      | -0.0008648606 |
|    mean_reward_advantages    | -1.1263334    |
|    n_updates                 | 5900          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.48e-09      |
|    reward_explained_variance | -0.108        |
|    reward_value_loss         | 6.5           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.23           |
|    ep_rew_mean               | 6.2            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 592            |
|    time_elapsed              | 100384         |
|    total_timesteps           | 75776          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00456        |
|    cost_value_loss           | 1.51e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.57           |
|    mean_cost_advantages      | 1.45337835e-05 |
|    mean_reward_advantages    | -0.25384778    |
|    n_updates                 | 5910           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.33e-10      |
|    reward_explained_variance | -0.457         |
|    reward_value_loss         | 5.7            |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.54           |
|    ep_rew_mean               | 5.52           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 593            |
|    time_elapsed              | 100507         |
|    total_timesteps           | 75904          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0284         |
|    cost_value_loss           | 2.91e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.88e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.59           |
|    mean_cost_advantages      | -0.00014846916 |
|    mean_reward_advantages    | 0.8622731      |
|    n_updates                 | 5920           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.64e-10      |
|    reward_explained_variance | -0.575         |
|    reward_value_loss         | 7.2            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 594           |
|    time_elapsed              | 100629        |
|    total_timesteps           | 76032         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0504        |
|    cost_value_loss           | 2.01e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.15          |
|    mean_cost_advantages      | 0.00014039042 |
|    mean_reward_advantages    | 0.13795313    |
|    n_updates                 | 5930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.01e-10      |
|    reward_explained_variance | -1.96         |
|    reward_value_loss         | 11.4          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.59           |
|    ep_rew_mean               | 6.56           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 595            |
|    time_elapsed              | 100751         |
|    total_timesteps           | 76160          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.024          |
|    cost_value_loss           | 1.45e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.98e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.78           |
|    mean_cost_advantages      | -0.00026557257 |
|    mean_reward_advantages    | 0.575526       |
|    n_updates                 | 5940           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.8e-10       |
|    reward_explained_variance | -1.17          |
|    reward_value_loss         | 9.44           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.57           |
|    ep_rew_mean               | 6.54           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 596            |
|    time_elapsed              | 100869         |
|    total_timesteps           | 76288          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.305         |
|    cost_value_loss           | 2.49e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.77e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.56           |
|    mean_cost_advantages      | -0.00042973878 |
|    mean_reward_advantages    | 0.10871799     |
|    n_updates                 | 5950           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.36e-09       |
|    reward_explained_variance | -1.08          |
|    reward_value_loss         | 7.12           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.51         |
|    ep_rew_mean               | 6.48         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 597          |
|    time_elapsed              | 100980       |
|    total_timesteps           | 76416        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0813       |
|    cost_value_loss           | 1.64e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.73e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.04         |
|    mean_cost_advantages      | 0.0006706107 |
|    mean_reward_advantages    | 0.043491215  |
|    n_updates                 | 5960         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.81e-09    |
|    reward_explained_variance | -0.345       |
|    reward_value_loss         | 5.24         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 598           |
|    time_elapsed              | 101094        |
|    total_timesteps           | 76544         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0065        |
|    cost_value_loss           | 1.87e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.9e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.18          |
|    mean_cost_advantages      | 0.00018114924 |
|    mean_reward_advantages    | 0.011010602   |
|    n_updates                 | 5970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.77e-09     |
|    reward_explained_variance | 0.0441        |
|    reward_value_loss         | 5.71          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.14          |
|    ep_rew_mean               | 6.12          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 599           |
|    time_elapsed              | 101206        |
|    total_timesteps           | 76672         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0172       |
|    cost_value_loss           | 2.97e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.9e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 7.14          |
|    mean_cost_advantages      | -7.526259e-05 |
|    mean_reward_advantages    | 0.40813044    |
|    n_updates                 | 5980          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.49e-09      |
|    reward_explained_variance | -1.14         |
|    reward_value_loss         | 15.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.36          |
|    ep_rew_mean               | 6.34          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 600           |
|    time_elapsed              | 101316        |
|    total_timesteps           | 76800         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.182        |
|    cost_value_loss           | 2.74e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.77e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.26          |
|    mean_cost_advantages      | 1.6763632e-05 |
|    mean_reward_advantages    | 0.3879195     |
|    n_updates                 | 5990          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.56e-10      |
|    reward_explained_variance | -1.03         |
|    reward_value_loss         | 9.84          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.79          |
|    ep_rew_mean               | 5.77          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 601           |
|    time_elapsed              | 101426        |
|    total_timesteps           | 76928         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0537        |
|    cost_value_loss           | 1.66e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.2           |
|    mean_cost_advantages      | -0.0015584455 |
|    mean_reward_advantages    | 1.1097121     |
|    n_updates                 | 6000          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.06e-09      |
|    reward_explained_variance | -2.06         |
|    reward_value_loss         | 14.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.88          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 602           |
|    time_elapsed              | 101536        |
|    total_timesteps           | 77056         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00554      |
|    cost_value_loss           | 2.24e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.88e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.96          |
|    mean_cost_advantages      | 0.00044812195 |
|    mean_reward_advantages    | -0.52603906   |
|    n_updates                 | 6010          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.1e-10      |
|    reward_explained_variance | -0.97         |
|    reward_value_loss         | 11.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.59         |
|    ep_rew_mean               | 5.57         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 603          |
|    time_elapsed              | 101643       |
|    total_timesteps           | 77184        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0478       |
|    cost_value_loss           | 2.01e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.15         |
|    mean_cost_advantages      | 0.0004184464 |
|    mean_reward_advantages    | 0.0002603531 |
|    n_updates                 | 6020         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.73e-09     |
|    reward_explained_variance | -0.322       |
|    reward_value_loss         | 7.19         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.92           |
|    ep_rew_mean               | 5.9            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 604            |
|    time_elapsed              | 101752         |
|    total_timesteps           | 77312          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0158         |
|    cost_value_loss           | 1.99e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.93e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.94           |
|    mean_cost_advantages      | -0.00069701904 |
|    mean_reward_advantages    | -0.38925496    |
|    n_updates                 | 6030           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.89e-11       |
|    reward_explained_variance | 0.156          |
|    reward_value_loss         | 6.1            |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.02         |
|    ep_rew_mean               | 6            |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 605          |
|    time_elapsed              | 101861       |
|    total_timesteps           | 77440        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0292      |
|    cost_value_loss           | 2.08e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.81e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.44         |
|    mean_cost_advantages      | 0.0003071864 |
|    mean_reward_advantages    | 1.0215933    |
|    n_updates                 | 6040         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.48e-09    |
|    reward_explained_variance | 0.0698       |
|    reward_value_loss         | 7.33         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 606           |
|    time_elapsed              | 101970        |
|    total_timesteps           | 77568         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.029         |
|    cost_value_loss           | 1.73e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.85e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.86          |
|    mean_cost_advantages      | 0.00010959383 |
|    mean_reward_advantages    | -0.24284767   |
|    n_updates                 | 6050          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.29e-11      |
|    reward_explained_variance | -0.486        |
|    reward_value_loss         | 8.86          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.27          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 607           |
|    time_elapsed              | 102080        |
|    total_timesteps           | 77696         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0345        |
|    cost_value_loss           | 1.92e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.94e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.25          |
|    mean_cost_advantages      | -0.0004955542 |
|    mean_reward_advantages    | -0.039595515  |
|    n_updates                 | 6060          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.53e-11      |
|    reward_explained_variance | -0.718        |
|    reward_value_loss         | 6.31          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.49         |
|    ep_rew_mean               | 6.46         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 608          |
|    time_elapsed              | 102187       |
|    total_timesteps           | 77824        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.162       |
|    cost_value_loss           | 2.06e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.87e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.16         |
|    mean_cost_advantages      | 0.0003266901 |
|    mean_reward_advantages    | 0.41584525   |
|    n_updates                 | 6070         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.29e-09     |
|    reward_explained_variance | -0.37        |
|    reward_value_loss         | 10.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.7            |
|    ep_rew_mean               | 6.67           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 609            |
|    time_elapsed              | 102295         |
|    total_timesteps           | 77952          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0392         |
|    cost_value_loss           | 1.91e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.78e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.26           |
|    mean_cost_advantages      | -0.00072479824 |
|    mean_reward_advantages    | -0.7077117     |
|    n_updates                 | 6080           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.62e-09      |
|    reward_explained_variance | -0.588         |
|    reward_value_loss         | 7.32           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.74          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 610           |
|    time_elapsed              | 102410        |
|    total_timesteps           | 78080         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00599       |
|    cost_value_loss           | 1.31e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.91e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.39          |
|    mean_cost_advantages      | 0.00033667544 |
|    mean_reward_advantages    | 0.10003598    |
|    n_updates                 | 6090          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.59e-10     |
|    reward_explained_variance | -0.801        |
|    reward_value_loss         | 6.5           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.27          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 611           |
|    time_elapsed              | 102533        |
|    total_timesteps           | 78208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00174       |
|    cost_value_loss           | 1.97e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.84          |
|    mean_cost_advantages      | 0.00044011776 |
|    mean_reward_advantages    | 1.291297      |
|    n_updates                 | 6100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.52e-10     |
|    reward_explained_variance | -1.47         |
|    reward_value_loss         | 13.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.82           |
|    ep_rew_mean               | 6.78           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 612            |
|    time_elapsed              | 102648         |
|    total_timesteps           | 78336          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0141        |
|    cost_value_loss           | 1.91e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.8e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.58           |
|    mean_cost_advantages      | -0.00058287266 |
|    mean_reward_advantages    | -1.5277765     |
|    n_updates                 | 6110           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.8e-09       |
|    reward_explained_variance | -0.249         |
|    reward_value_loss         | 9.13           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.35          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 613           |
|    time_elapsed              | 102763        |
|    total_timesteps           | 78464         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00571      |
|    cost_value_loss           | 1.87e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.09          |
|    mean_cost_advantages      | 0.00046812452 |
|    mean_reward_advantages    | 0.7606317     |
|    n_updates                 | 6120          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.14e-09      |
|    reward_explained_variance | -1.55         |
|    reward_value_loss         | 16.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.26          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 614           |
|    time_elapsed              | 102877        |
|    total_timesteps           | 78592         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0369       |
|    cost_value_loss           | 1.12e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.96e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.02          |
|    mean_cost_advantages      | 0.00037594454 |
|    mean_reward_advantages    | -0.5909569    |
|    n_updates                 | 6130          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.64e-09     |
|    reward_explained_variance | -0.468        |
|    reward_value_loss         | 7.84          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.89          |
|    ep_rew_mean               | 5.86          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 615           |
|    time_elapsed              | 102989        |
|    total_timesteps           | 78720         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0183       |
|    cost_value_loss           | 1.63e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.95e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.29          |
|    mean_cost_advantages      | 0.00022883153 |
|    mean_reward_advantages    | -0.0715973    |
|    n_updates                 | 6140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.91e-10      |
|    reward_explained_variance | -0.396        |
|    reward_value_loss         | 6.39          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.14          |
|    ep_rew_mean               | 6.11          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 616           |
|    time_elapsed              | 103102        |
|    total_timesteps           | 78848         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0212       |
|    cost_value_loss           | 1.56e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.77e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.41          |
|    mean_cost_advantages      | -0.0009435008 |
|    mean_reward_advantages    | 1.1235826     |
|    n_updates                 | 6150          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.55e-11      |
|    reward_explained_variance | -1.11         |
|    reward_value_loss         | 7.05          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.39         |
|    ep_rew_mean               | 6.35         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 617          |
|    time_elapsed              | 103212       |
|    total_timesteps           | 78976        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000208    |
|    cost_value_loss           | 1.36e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.9e-05     |
|    learning_rate             | 0.0005       |
|    loss                      | 3.4          |
|    mean_cost_advantages      | 0.0007714699 |
|    mean_reward_advantages    | 0.39756763   |
|    n_updates                 | 6160         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.05e-09     |
|    reward_explained_variance | -0.95        |
|    reward_value_loss         | 7.6          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 618           |
|    time_elapsed              | 103325        |
|    total_timesteps           | 79104         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0185        |
|    cost_value_loss           | 1.81e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.47          |
|    mean_cost_advantages      | -6.976364e-05 |
|    mean_reward_advantages    | 0.8873613     |
|    n_updates                 | 6170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.28e-09     |
|    reward_explained_variance | -1.7          |
|    reward_value_loss         | 10.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.13          |
|    ep_rew_mean               | 6.1           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 619           |
|    time_elapsed              | 103446        |
|    total_timesteps           | 79232         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.02          |
|    cost_value_loss           | 1.1e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.78          |
|    mean_cost_advantages      | 0.00024071013 |
|    mean_reward_advantages    | 0.14958167    |
|    n_updates                 | 6180          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.42e-10      |
|    reward_explained_variance | -0.289        |
|    reward_value_loss         | 9.77          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 620           |
|    time_elapsed              | 103578        |
|    total_timesteps           | 79360         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0169        |
|    cost_value_loss           | 1.46e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.74e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.12          |
|    mean_cost_advantages      | 0.00035639468 |
|    mean_reward_advantages    | -0.8137208    |
|    n_updates                 | 6190          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.53e-09      |
|    reward_explained_variance | 0.00829       |
|    reward_value_loss         | 5.29          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.33           |
|    ep_rew_mean               | 6.3            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 621            |
|    time_elapsed              | 103693         |
|    total_timesteps           | 79488          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00768       |
|    cost_value_loss           | 1.88e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.89           |
|    mean_cost_advantages      | -0.00081985036 |
|    mean_reward_advantages    | 1.4465731      |
|    n_updates                 | 6200           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.65e-09      |
|    reward_explained_variance | -2.34          |
|    reward_value_loss         | 14.9           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.08          |
|    ep_rew_mean               | 6.05          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 622           |
|    time_elapsed              | 103807        |
|    total_timesteps           | 79616         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.12          |
|    cost_value_loss           | 1.47e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.89e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.25          |
|    mean_cost_advantages      | -0.0014865352 |
|    mean_reward_advantages    | 1.1165998     |
|    n_updates                 | 6210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.07e-10      |
|    reward_explained_variance | -2.64         |
|    reward_value_loss         | 18.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.75          |
|    ep_rew_mean               | 5.73          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 623           |
|    time_elapsed              | 103924        |
|    total_timesteps           | 79744         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00573      |
|    cost_value_loss           | 1.38e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.25          |
|    mean_cost_advantages      | 0.00044333714 |
|    mean_reward_advantages    | -1.6714919    |
|    n_updates                 | 6220          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.09e-09      |
|    reward_explained_variance | -0.0523       |
|    reward_value_loss         | 5.53          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.01          |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 624           |
|    time_elapsed              | 104038        |
|    total_timesteps           | 79872         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.131        |
|    cost_value_loss           | 1.45e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.94e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.49          |
|    mean_cost_advantages      | 0.00040357892 |
|    mean_reward_advantages    | -0.3075149    |
|    n_updates                 | 6230          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.14e-10      |
|    reward_explained_variance | 0.211         |
|    reward_value_loss         | 4.02          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.27          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 625           |
|    time_elapsed              | 104154        |
|    total_timesteps           | 80000         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0456       |
|    cost_value_loss           | 1.29e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.87e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.34          |
|    mean_cost_advantages      | 7.2222916e-05 |
|    mean_reward_advantages    | 0.434921      |
|    n_updates                 | 6240          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.92e-09      |
|    reward_explained_variance | -0.449        |
|    reward_value_loss         | 7.49          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.3           |
|    ep_rew_mean               | 6.28          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 626           |
|    time_elapsed              | 104266        |
|    total_timesteps           | 80128         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.054         |
|    cost_value_loss           | 1.62e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.75e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.95          |
|    mean_cost_advantages      | -7.507727e-05 |
|    mean_reward_advantages    | 1.3464566     |
|    n_updates                 | 6250          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.8e-09       |
|    reward_explained_variance | -0.887        |
|    reward_value_loss         | 12.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.47           |
|    ep_rew_mean               | 6.45           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 627            |
|    time_elapsed              | 104384         |
|    total_timesteps           | 80256          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0192         |
|    cost_value_loss           | 1.16e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.72e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.65           |
|    mean_cost_advantages      | -0.00017373153 |
|    mean_reward_advantages    | 0.19568634     |
|    n_updates                 | 6260           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.69e-10      |
|    reward_explained_variance | -0.548         |
|    reward_value_loss         | 10.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.61          |
|    ep_rew_mean               | 6.59          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 628           |
|    time_elapsed              | 104510        |
|    total_timesteps           | 80384         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0121        |
|    cost_value_loss           | 1.15e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.82e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.62          |
|    mean_cost_advantages      | -0.0007292397 |
|    mean_reward_advantages    | 0.9294262     |
|    n_updates                 | 6270          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.96e-09     |
|    reward_explained_variance | -1.07         |
|    reward_value_loss         | 10.8          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.01          |
|    ep_rew_mean               | 6.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 629           |
|    time_elapsed              | 104635        |
|    total_timesteps           | 80512         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0557       |
|    cost_value_loss           | 1.04e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.98e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.21          |
|    mean_cost_advantages      | 0.00019426271 |
|    mean_reward_advantages    | -1.0720732    |
|    n_updates                 | 6280          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.27e-09     |
|    reward_explained_variance | 0.0571        |
|    reward_value_loss         | 7.81          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.59          |
|    ep_rew_mean               | 6.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 630           |
|    time_elapsed              | 104753        |
|    total_timesteps           | 80640         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0651       |
|    cost_value_loss           | 1.61e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.88e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.52          |
|    mean_cost_advantages      | 0.00032513993 |
|    mean_reward_advantages    | -0.6463358    |
|    n_updates                 | 6290          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.84e-09     |
|    reward_explained_variance | -0.607        |
|    reward_value_loss         | 8.8           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 631           |
|    time_elapsed              | 104877        |
|    total_timesteps           | 80768         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0282       |
|    cost_value_loss           | 1.25e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.93          |
|    mean_cost_advantages      | 4.3892214e-06 |
|    mean_reward_advantages    | -0.7451729    |
|    n_updates                 | 6300          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.01e-09     |
|    reward_explained_variance | 0.322         |
|    reward_value_loss         | 6.35          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.3           |
|    ep_rew_mean               | 5.26          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 632           |
|    time_elapsed              | 105002        |
|    total_timesteps           | 80896         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0275       |
|    cost_value_loss           | 1.26e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.39          |
|    mean_cost_advantages      | 0.00012880284 |
|    mean_reward_advantages    | -0.5368055    |
|    n_updates                 | 6310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.51e-09     |
|    reward_explained_variance | 0.164         |
|    reward_value_loss         | 3.6           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.45          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 633           |
|    time_elapsed              | 105122        |
|    total_timesteps           | 81024         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00925       |
|    cost_value_loss           | 1.47e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.9e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.25          |
|    mean_cost_advantages      | 0.00043986226 |
|    mean_reward_advantages    | 0.15763319    |
|    n_updates                 | 6320          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.83e-09     |
|    reward_explained_variance | -0.194        |
|    reward_value_loss         | 2.98          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.61          |
|    ep_rew_mean               | 5.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 634           |
|    time_elapsed              | 105245        |
|    total_timesteps           | 81152         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0234        |
|    cost_value_loss           | 1.53e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.89e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 8.52          |
|    mean_cost_advantages      | -0.0008446933 |
|    mean_reward_advantages    | 3.0061593     |
|    n_updates                 | 6330          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.02e-09      |
|    reward_explained_variance | -4.62         |
|    reward_value_loss         | 22.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 635           |
|    time_elapsed              | 105373        |
|    total_timesteps           | 81280         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0554        |
|    cost_value_loss           | 1.99e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.84e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.15          |
|    mean_cost_advantages      | 0.00036080508 |
|    mean_reward_advantages    | -0.9617709    |
|    n_updates                 | 6340          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.05e-10     |
|    reward_explained_variance | -1.17         |
|    reward_value_loss         | 13.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.07         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 636          |
|    time_elapsed              | 105497       |
|    total_timesteps           | 81408        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00564     |
|    cost_value_loss           | 9.7e-06      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.03e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.96         |
|    mean_cost_advantages      | 0.0009174008 |
|    mean_reward_advantages    | 0.19384852   |
|    n_updates                 | 6350         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.32e-09     |
|    reward_explained_variance | -1.02        |
|    reward_value_loss         | 10.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.32         |
|    ep_rew_mean               | 6.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 637          |
|    time_elapsed              | 105626       |
|    total_timesteps           | 81536        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0424       |
|    cost_value_loss           | 1.73e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.83e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.98         |
|    mean_cost_advantages      | 0.0002959579 |
|    mean_reward_advantages    | -0.012971133 |
|    n_updates                 | 6360         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.07e-09     |
|    reward_explained_variance | -0.471       |
|    reward_value_loss         | 6.72         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.79         |
|    ep_rew_mean               | 6.75         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 638          |
|    time_elapsed              | 105749       |
|    total_timesteps           | 81664        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0086      |
|    cost_value_loss           | 1.09e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.61         |
|    mean_cost_advantages      | 3.350849e-05 |
|    mean_reward_advantages    | 0.38022873   |
|    n_updates                 | 6370         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1e-09        |
|    reward_explained_variance | -0.052       |
|    reward_value_loss         | 5.39         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.61          |
|    ep_rew_mean               | 6.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 639           |
|    time_elapsed              | 105874        |
|    total_timesteps           | 81792         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0166       |
|    cost_value_loss           | 9.48e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.43          |
|    mean_cost_advantages      | -0.0016360224 |
|    mean_reward_advantages    | 0.67988527    |
|    n_updates                 | 6380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.86e-10     |
|    reward_explained_variance | -1.25         |
|    reward_value_loss         | 10.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.67          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 640           |
|    time_elapsed              | 105997        |
|    total_timesteps           | 81920         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0222        |
|    cost_value_loss           | 1.46e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.98e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.22          |
|    mean_cost_advantages      | 0.00055717974 |
|    mean_reward_advantages    | -0.58389497   |
|    n_updates                 | 6390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.49e-09     |
|    reward_explained_variance | -1.01         |
|    reward_value_loss         | 9.1           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.78         |
|    ep_rew_mean               | 6.75         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 641          |
|    time_elapsed              | 106121       |
|    total_timesteps           | 82048        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00677      |
|    cost_value_loss           | 1.01e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.86e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.17         |
|    mean_cost_advantages      | 0.0003563211 |
|    mean_reward_advantages    | 0.4605648    |
|    n_updates                 | 6400         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3e-09        |
|    reward_explained_variance | -0.862       |
|    reward_value_loss         | 10.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.16          |
|    ep_rew_mean               | 7.13          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 642           |
|    time_elapsed              | 106247        |
|    total_timesteps           | 82176         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.014         |
|    cost_value_loss           | 1.28e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.89e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.7           |
|    mean_cost_advantages      | 0.00035902142 |
|    mean_reward_advantages    | 0.94451153    |
|    n_updates                 | 6410          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.59e-10      |
|    reward_explained_variance | -1.07         |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.77          |
|    ep_rew_mean               | 7.72          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 643           |
|    time_elapsed              | 106368        |
|    total_timesteps           | 82304         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0258       |
|    cost_value_loss           | 7.24e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5e-05        |
|    learning_rate             | 0.0005        |
|    loss                      | 3.66          |
|    mean_cost_advantages      | 7.6467244e-05 |
|    mean_reward_advantages    | 0.10533115    |
|    n_updates                 | 6420          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.7e-09      |
|    reward_explained_variance | -0.384        |
|    reward_value_loss         | 8.1           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.51           |
|    ep_rew_mean               | 7.47           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 644            |
|    time_elapsed              | 106495         |
|    total_timesteps           | 82432          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.102          |
|    cost_value_loss           | 1.65e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.93e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.62           |
|    mean_cost_advantages      | -0.00089275395 |
|    mean_reward_advantages    | 1.3426981      |
|    n_updates                 | 6430           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.85e-09       |
|    reward_explained_variance | -2.17          |
|    reward_value_loss         | 13.8           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.76         |
|    ep_rew_mean               | 6.72         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 645          |
|    time_elapsed              | 106620       |
|    total_timesteps           | 82560        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0192       |
|    cost_value_loss           | 1e-05        |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.67         |
|    mean_cost_advantages      | 0.0003446851 |
|    mean_reward_advantages    | 0.06699258   |
|    n_updates                 | 6440         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.06e-09     |
|    reward_explained_variance | -1.18        |
|    reward_value_loss         | 12.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.32         |
|    ep_rew_mean               | 6.27         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 646          |
|    time_elapsed              | 106745       |
|    total_timesteps           | 82688        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0328      |
|    cost_value_loss           | 1.06e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.88e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.6          |
|    mean_cost_advantages      | 0.0003669649 |
|    mean_reward_advantages    | -0.5194466   |
|    n_updates                 | 6450         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5e-11        |
|    reward_explained_variance | -0.596       |
|    reward_value_loss         | 5.77         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.1          |
|    ep_rew_mean               | 6.06         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 647          |
|    time_elapsed              | 106871       |
|    total_timesteps           | 82816        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.046       |
|    cost_value_loss           | 1.12e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5e-05       |
|    learning_rate             | 0.0005       |
|    loss                      | 1.46         |
|    mean_cost_advantages      | 0.0008406719 |
|    mean_reward_advantages    | -0.7754632   |
|    n_updates                 | 6460         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.78e-10     |
|    reward_explained_variance | 0.37         |
|    reward_value_loss         | 3.94         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.81          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 648           |
|    time_elapsed              | 106994        |
|    total_timesteps           | 82944         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0599       |
|    cost_value_loss           | 9.97e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.68          |
|    mean_cost_advantages      | -0.0004096587 |
|    mean_reward_advantages    | 1.290695      |
|    n_updates                 | 6470          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.63e-10      |
|    reward_explained_variance | -0.965        |
|    reward_value_loss         | 9.91          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.76          |
|    ep_rew_mean               | 5.72          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 649           |
|    time_elapsed              | 107115        |
|    total_timesteps           | 83072         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0144       |
|    cost_value_loss           | 1.3e-05       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.94e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.93          |
|    mean_cost_advantages      | 2.0429507e-05 |
|    mean_reward_advantages    | -0.29336596   |
|    n_updates                 | 6480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.39e-09      |
|    reward_explained_variance | -0.454        |
|    reward_value_loss         | 9.27          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.42        |
|    ep_rew_mean               | 6.38        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 650         |
|    time_elapsed              | 107238      |
|    total_timesteps           | 83200       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0311      |
|    cost_value_loss           | 1.33e-05    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -4.88e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 2.32        |
|    mean_cost_advantages      | 6.97379e-05 |
|    mean_reward_advantages    | -1.0024595  |
|    n_updates                 | 6490        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -6.44e-09   |
|    reward_explained_variance | 0.259       |
|    reward_value_loss         | 4.98        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.65          |
|    ep_rew_mean               | 6.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 651           |
|    time_elapsed              | 107361        |
|    total_timesteps           | 83328         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.034         |
|    cost_value_loss           | 1.27e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.14          |
|    mean_cost_advantages      | -0.0006739175 |
|    mean_reward_advantages    | 1.9611945     |
|    n_updates                 | 6500          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.22e-09      |
|    reward_explained_variance | -1.43         |
|    reward_value_loss         | 13.6          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.87         |
|    ep_rew_mean               | 6.82         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 652          |
|    time_elapsed              | 107486       |
|    total_timesteps           | 83456        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0453      |
|    cost_value_loss           | 1.09e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.03e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.07         |
|    mean_cost_advantages      | 0.0005375414 |
|    mean_reward_advantages    | -0.7705902   |
|    n_updates                 | 6510         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.75e-10     |
|    reward_explained_variance | -0.0369      |
|    reward_value_loss         | 7.07         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.05          |
|    ep_rew_mean               | 7             |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 653           |
|    time_elapsed              | 146502        |
|    total_timesteps           | 83584         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0573        |
|    cost_value_loss           | 1.14e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.91e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.8           |
|    mean_cost_advantages      | -0.0009778359 |
|    mean_reward_advantages    | 0.9242338     |
|    n_updates                 | 6520          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.59e-09      |
|    reward_explained_variance | -1.4          |
|    reward_value_loss         | 14.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.13          |
|    ep_rew_mean               | 7.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 654           |
|    time_elapsed              | 146603        |
|    total_timesteps           | 83712         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0308        |
|    cost_value_loss           | 1.02e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.8e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.75          |
|    mean_cost_advantages      | 0.00038460206 |
|    mean_reward_advantages    | 0.45020962    |
|    n_updates                 | 6530          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.73e-09      |
|    reward_explained_variance | -0.769        |
|    reward_value_loss         | 10.8          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.15           |
|    ep_rew_mean               | 7.1            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 655            |
|    time_elapsed              | 146702         |
|    total_timesteps           | 83840          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0452        |
|    cost_value_loss           | 9.45e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.78e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.53           |
|    mean_cost_advantages      | -0.00011220412 |
|    mean_reward_advantages    | 0.4347067      |
|    n_updates                 | 6540           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.22e-10      |
|    reward_explained_variance | 0.294          |
|    reward_value_loss         | 5.87           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.45           |
|    ep_rew_mean               | 6.41           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 656            |
|    time_elapsed              | 146801         |
|    total_timesteps           | 83968          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00177        |
|    cost_value_loss           | 1.09e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.98e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.69           |
|    mean_cost_advantages      | -0.00017096699 |
|    mean_reward_advantages    | -0.6105999     |
|    n_updates                 | 6550           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.29e-10      |
|    reward_explained_variance | -0.212         |
|    reward_value_loss         | 8.2            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 657           |
|    time_elapsed              | 146907        |
|    total_timesteps           | 84096         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0122        |
|    cost_value_loss           | 1.05e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.75e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.01          |
|    mean_cost_advantages      | 0.00012240378 |
|    mean_reward_advantages    | -0.44952825   |
|    n_updates                 | 6560          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.65e-09     |
|    reward_explained_variance | 0.205         |
|    reward_value_loss         | 5.1           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.48          |
|    ep_rew_mean               | 6.45          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 658           |
|    time_elapsed              | 147013        |
|    total_timesteps           | 84224         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0348        |
|    cost_value_loss           | 1.13e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.9e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 4             |
|    mean_cost_advantages      | -0.0008166571 |
|    mean_reward_advantages    | 0.68514496    |
|    n_updates                 | 6570          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.95e-10      |
|    reward_explained_variance | -1.09         |
|    reward_value_loss         | 8.76          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 659           |
|    time_elapsed              | 147124        |
|    total_timesteps           | 84352         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0522        |
|    cost_value_loss           | 1.15e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.48          |
|    mean_cost_advantages      | -0.0005341515 |
|    mean_reward_advantages    | 1.8522108     |
|    n_updates                 | 6580          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.42e-09     |
|    reward_explained_variance | -0.986        |
|    reward_value_loss         | 13.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 660           |
|    time_elapsed              | 147236        |
|    total_timesteps           | 84480         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00592      |
|    cost_value_loss           | 9.6e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.92e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.28          |
|    mean_cost_advantages      | -0.0005293312 |
|    mean_reward_advantages    | -1.2770904    |
|    n_updates                 | 6590          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.28e-09      |
|    reward_explained_variance | 0.506         |
|    reward_value_loss         | 4.21          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.94           |
|    ep_rew_mean               | 5.92           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 661            |
|    time_elapsed              | 147346         |
|    total_timesteps           | 84608          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0319         |
|    cost_value_loss           | 9.51e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.85e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.69           |
|    mean_cost_advantages      | -0.00046882307 |
|    mean_reward_advantages    | 0.2837574      |
|    n_updates                 | 6600           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.59e-09      |
|    reward_explained_variance | 0.152          |
|    reward_value_loss         | 6.2            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.36          |
|    ep_rew_mean               | 6.34          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 662           |
|    time_elapsed              | 147459        |
|    total_timesteps           | 84736         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0058       |
|    cost_value_loss           | 1.18e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.71e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.73          |
|    mean_cost_advantages      | -0.0003210039 |
|    mean_reward_advantages    | -0.28482872   |
|    n_updates                 | 6610          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.31e-10      |
|    reward_explained_variance | -0.0973       |
|    reward_value_loss         | 8.78          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.6           |
|    ep_rew_mean               | 6.58          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 663           |
|    time_elapsed              | 147571        |
|    total_timesteps           | 84864         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0831        |
|    cost_value_loss           | 8.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.79e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.73          |
|    mean_cost_advantages      | -0.0009256024 |
|    mean_reward_advantages    | 0.5497732     |
|    n_updates                 | 6620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.61e-09      |
|    reward_explained_variance | -0.558        |
|    reward_value_loss         | 11            |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.67         |
|    ep_rew_mean               | 6.64         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 664          |
|    time_elapsed              | 147684       |
|    total_timesteps           | 84992        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0953       |
|    cost_value_loss           | 1.36e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.76e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.74         |
|    mean_cost_advantages      | 0.0002626523 |
|    mean_reward_advantages    | 0.48562366   |
|    n_updates                 | 6630         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.32e-09    |
|    reward_explained_variance | -0.982       |
|    reward_value_loss         | 16.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.76         |
|    ep_rew_mean               | 6.73         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 665          |
|    time_elapsed              | 147797       |
|    total_timesteps           | 85120        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0365       |
|    cost_value_loss           | 8.87e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.97e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.48         |
|    mean_cost_advantages      | 0.0005781463 |
|    mean_reward_advantages    | -0.48276192  |
|    n_updates                 | 6640         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.23e-10    |
|    reward_explained_variance | 0.436        |
|    reward_value_loss         | 4.96         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.49          |
|    ep_rew_mean               | 6.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 666           |
|    time_elapsed              | 147910        |
|    total_timesteps           | 85248         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0433        |
|    cost_value_loss           | 1.06e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.93e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.92          |
|    mean_cost_advantages      | -0.0008801272 |
|    mean_reward_advantages    | 0.70439047    |
|    n_updates                 | 6650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.66e-09      |
|    reward_explained_variance | -0.615        |
|    reward_value_loss         | 8.91          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.93          |
|    ep_rew_mean               | 6.9           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 667           |
|    time_elapsed              | 148022        |
|    total_timesteps           | 85376         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0622        |
|    cost_value_loss           | 8.66e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.78e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.81          |
|    mean_cost_advantages      | 0.00038303732 |
|    mean_reward_advantages    | 0.06007999    |
|    n_updates                 | 6660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.74e-11      |
|    reward_explained_variance | 0.0733        |
|    reward_value_loss         | 4.76          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.64          |
|    ep_rew_mean               | 6.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 668           |
|    time_elapsed              | 148134        |
|    total_timesteps           | 85504         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.042        |
|    cost_value_loss           | 1.22e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.78e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.65          |
|    mean_cost_advantages      | -0.0011358415 |
|    mean_reward_advantages    | 1.25693       |
|    n_updates                 | 6670          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.72e-09     |
|    reward_explained_variance | -1.23         |
|    reward_value_loss         | 11.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.62         |
|    ep_rew_mean               | 6.59         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 669          |
|    time_elapsed              | 148247       |
|    total_timesteps           | 85632        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0285      |
|    cost_value_loss           | 9.13e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.91e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.19         |
|    mean_cost_advantages      | 0.0009241998 |
|    mean_reward_advantages    | -0.19199525  |
|    n_updates                 | 6680         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.67e-10    |
|    reward_explained_variance | 0.453        |
|    reward_value_loss         | 3.06         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7             |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 670           |
|    time_elapsed              | 148360        |
|    total_timesteps           | 85760         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0155        |
|    cost_value_loss           | 1.14e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.82e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.06          |
|    mean_cost_advantages      | 0.00050501147 |
|    mean_reward_advantages    | 1.3629918     |
|    n_updates                 | 6690          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.82e-10      |
|    reward_explained_variance | -0.353        |
|    reward_value_loss         | 7.48          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.43           |
|    ep_rew_mean               | 6.4            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 671            |
|    time_elapsed              | 148484         |
|    total_timesteps           | 85888          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0216         |
|    cost_value_loss           | 1.07e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.74e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.24           |
|    mean_cost_advantages      | -1.2346907e-05 |
|    mean_reward_advantages    | 0.27051598     |
|    n_updates                 | 6700           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.29e-09      |
|    reward_explained_variance | -0.235         |
|    reward_value_loss         | 7.06           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.86         |
|    ep_rew_mean               | 6.83         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 672          |
|    time_elapsed              | 148613       |
|    total_timesteps           | 86016        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0512       |
|    cost_value_loss           | 7.24e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.98e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.45         |
|    mean_cost_advantages      | 0.0004676699 |
|    mean_reward_advantages    | 0.13007858   |
|    n_updates                 | 6710         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.75e-10    |
|    reward_explained_variance | -0.0849      |
|    reward_value_loss         | 4.88         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.29           |
|    ep_rew_mean               | 6.26           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 673            |
|    time_elapsed              | 148726         |
|    total_timesteps           | 86144          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00968        |
|    cost_value_loss           | 9.05e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.82e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.51           |
|    mean_cost_advantages      | -0.00013739837 |
|    mean_reward_advantages    | 1.4877079      |
|    n_updates                 | 6720           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.13e-09      |
|    reward_explained_variance | -1.69          |
|    reward_value_loss         | 11.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.85         |
|    ep_rew_mean               | 6.81         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 674          |
|    time_elapsed              | 148834       |
|    total_timesteps           | 86272        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.14        |
|    cost_value_loss           | 1.12e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.77e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.35         |
|    mean_cost_advantages      | 0.0008971366 |
|    mean_reward_advantages    | -0.07126038  |
|    n_updates                 | 6730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.39e-10    |
|    reward_explained_variance | -0.825       |
|    reward_value_loss         | 9.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.07           |
|    ep_rew_mean               | 7.03           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 675            |
|    time_elapsed              | 148940         |
|    total_timesteps           | 86400          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0279         |
|    cost_value_loss           | 1.12e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.84e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.12           |
|    mean_cost_advantages      | -0.00015471145 |
|    mean_reward_advantages    | 1.2850504      |
|    n_updates                 | 6740           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.12e-10      |
|    reward_explained_variance | -0.777         |
|    reward_value_loss         | 12.1           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.26         |
|    ep_rew_mean               | 7.22         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 676          |
|    time_elapsed              | 149045       |
|    total_timesteps           | 86528        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0182       |
|    cost_value_loss           | 8.05e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.85e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.57         |
|    mean_cost_advantages      | 0.0006710684 |
|    mean_reward_advantages    | -0.46277285  |
|    n_updates                 | 6750         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2e-10       |
|    reward_explained_variance | -0.422       |
|    reward_value_loss         | 6.25         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.2          |
|    ep_rew_mean               | 7.16         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 677          |
|    time_elapsed              | 149152       |
|    total_timesteps           | 86656        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0207       |
|    cost_value_loss           | 7.16e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.97e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.01         |
|    mean_cost_advantages      | 0.0001341564 |
|    mean_reward_advantages    | 1.3428688    |
|    n_updates                 | 6760         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.05e-08    |
|    reward_explained_variance | -0.629       |
|    reward_value_loss         | 10.4         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.68           |
|    ep_rew_mean               | 6.65           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 678            |
|    time_elapsed              | 149263         |
|    total_timesteps           | 86784          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00551        |
|    cost_value_loss           | 8.57e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.82e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.85           |
|    mean_cost_advantages      | -0.00037701998 |
|    mean_reward_advantages    | -1.0178423     |
|    n_updates                 | 6770           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.85e-09       |
|    reward_explained_variance | -0.323         |
|    reward_value_loss         | 6.91           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.75          |
|    ep_rew_mean               | 6.71          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 679           |
|    time_elapsed              | 149339        |
|    total_timesteps           | 86912         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0181        |
|    cost_value_loss           | 9.39e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.83e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.8           |
|    mean_cost_advantages      | 0.00048798593 |
|    mean_reward_advantages    | 0.54620403    |
|    n_updates                 | 6780          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.73e-10      |
|    reward_explained_variance | -0.196        |
|    reward_value_loss         | 5.77          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.96          |
|    ep_rew_mean               | 5.93          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 680           |
|    time_elapsed              | 149415        |
|    total_timesteps           | 87040         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0262        |
|    cost_value_loss           | 1.07e-05      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.85e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.43          |
|    mean_cost_advantages      | -0.0005005599 |
|    mean_reward_advantages    | 0.49843246    |
|    n_updates                 | 6790          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.87e-10      |
|    reward_explained_variance | -0.801        |
|    reward_value_loss         | 9.07          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.02           |
|    ep_rew_mean               | 5.99           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 681            |
|    time_elapsed              | 149490         |
|    total_timesteps           | 87168          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00911        |
|    cost_value_loss           | 7.12e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.88e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.73           |
|    mean_cost_advantages      | -0.00017218901 |
|    mean_reward_advantages    | 0.19717482     |
|    n_updates                 | 6800           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.87e-09       |
|    reward_explained_variance | 0.0118         |
|    reward_value_loss         | 7.37           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.76         |
|    ep_rew_mean               | 5.73         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 682          |
|    time_elapsed              | 149566       |
|    total_timesteps           | 87296        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0877      |
|    cost_value_loss           | 8.68e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.9e-05     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.22         |
|    mean_cost_advantages      | 0.0007987203 |
|    mean_reward_advantages    | 0.24177298   |
|    n_updates                 | 6810         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.26e-10     |
|    reward_explained_variance | -0.116       |
|    reward_value_loss         | 5.37         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.43           |
|    ep_rew_mean               | 5.4            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 683            |
|    time_elapsed              | 149641         |
|    total_timesteps           | 87424          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.061         |
|    cost_value_loss           | 8.37e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.99e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.74           |
|    mean_cost_advantages      | -0.00051388785 |
|    mean_reward_advantages    | -0.101146966   |
|    n_updates                 | 6820           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.23e-09       |
|    reward_explained_variance | -0.516         |
|    reward_value_loss         | 5.46           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.59           |
|    ep_rew_mean               | 5.56           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 684            |
|    time_elapsed              | 149718         |
|    total_timesteps           | 87552          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0222         |
|    cost_value_loss           | 1.03e-05       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.88e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.57           |
|    mean_cost_advantages      | -0.00048367615 |
|    mean_reward_advantages    | -0.22807959    |
|    n_updates                 | 6830           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.19e-10       |
|    reward_explained_variance | -1.18          |
|    reward_value_loss         | 8.1            |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.71         |
|    ep_rew_mean               | 5.68         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 685          |
|    time_elapsed              | 149794       |
|    total_timesteps           | 87680        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00315     |
|    cost_value_loss           | 8.25e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.87e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.4          |
|    mean_cost_advantages      | 9.591793e-06 |
|    mean_reward_advantages    | 0.75374746   |
|    n_updates                 | 6840         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.6e-10     |
|    reward_explained_variance | -0.484       |
|    reward_value_loss         | 7.95         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.77          |
|    ep_rew_mean               | 5.74          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 686           |
|    time_elapsed              | 149871        |
|    total_timesteps           | 87808         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00829      |
|    cost_value_loss           | 9.37e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.5           |
|    mean_cost_advantages      | 0.00040384382 |
|    mean_reward_advantages    | -0.17173186   |
|    n_updates                 | 6850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.3e-09       |
|    reward_explained_variance | 0.0692        |
|    reward_value_loss         | 4.22          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 687           |
|    time_elapsed              | 149945        |
|    total_timesteps           | 87936         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0233       |
|    cost_value_loss           | 8.74e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.9e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.87          |
|    mean_cost_advantages      | -0.0002705026 |
|    mean_reward_advantages    | -0.1793226    |
|    n_updates                 | 6860          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.22e-10     |
|    reward_explained_variance | 0.119         |
|    reward_value_loss         | 5.2           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.32           |
|    ep_rew_mean               | 6.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 688            |
|    time_elapsed              | 150020         |
|    total_timesteps           | 88064          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0105         |
|    cost_value_loss           | 8.51e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.92e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.11           |
|    mean_cost_advantages      | -0.00036720204 |
|    mean_reward_advantages    | 0.9167611      |
|    n_updates                 | 6870           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.45e-10      |
|    reward_explained_variance | -0.71          |
|    reward_value_loss         | 8.92           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.59          |
|    ep_rew_mean               | 6.55          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 689           |
|    time_elapsed              | 150099        |
|    total_timesteps           | 88192         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0211       |
|    cost_value_loss           | 9.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.95e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.13          |
|    mean_cost_advantages      | 0.00053131336 |
|    mean_reward_advantages    | -0.16357312   |
|    n_updates                 | 6880          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.64e-09     |
|    reward_explained_variance | -0.896        |
|    reward_value_loss         | 8.57          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.67           |
|    ep_rew_mean               | 6.63           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 690            |
|    time_elapsed              | 150174         |
|    total_timesteps           | 88320          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0341         |
|    cost_value_loss           | 7.9e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.83e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.06           |
|    mean_cost_advantages      | -0.00032342263 |
|    mean_reward_advantages    | -0.5824157     |
|    n_updates                 | 6890           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.93e-10       |
|    reward_explained_variance | -0.608         |
|    reward_value_loss         | 8.13           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.8         |
|    ep_rew_mean               | 6.76        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 691         |
|    time_elapsed              | 150246      |
|    total_timesteps           | 88448       |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.00515     |
|    cost_value_loss           | 8.81e-06    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -4.83e-05   |
|    learning_rate             | 0.0005      |
|    loss                      | 5.68        |
|    mean_cost_advantages      | 0.000623704 |
|    mean_reward_advantages    | 0.70643896  |
|    n_updates                 | 6900        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 1.37e-09    |
|    reward_explained_variance | -1.48       |
|    reward_value_loss         | 14.1        |
|    total_cost                | 0.0         |
----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.21         |
|    ep_rew_mean               | 7.16         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 692          |
|    time_elapsed              | 150334       |
|    total_timesteps           | 88576        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0315      |
|    cost_value_loss           | 8.16e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.05e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.18         |
|    mean_cost_advantages      | 0.0008840587 |
|    mean_reward_advantages    | 0.23694159   |
|    n_updates                 | 6910         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.07e-09    |
|    reward_explained_variance | -0.397       |
|    reward_value_loss         | 7.27         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.69          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 693           |
|    time_elapsed              | 150455        |
|    total_timesteps           | 88704         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0297       |
|    cost_value_loss           | 7.81e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.02e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.86          |
|    mean_cost_advantages      | -0.0007350788 |
|    mean_reward_advantages    | 1.0614086     |
|    n_updates                 | 6920          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.19e-09     |
|    reward_explained_variance | -0.742        |
|    reward_value_loss         | 9.97          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.76           |
|    ep_rew_mean               | 6.71           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 694            |
|    time_elapsed              | 150570         |
|    total_timesteps           | 88832          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0112         |
|    cost_value_loss           | 7.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.85e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.36           |
|    mean_cost_advantages      | -0.00029593677 |
|    mean_reward_advantages    | -0.6908927     |
|    n_updates                 | 6930           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.41e-11      |
|    reward_explained_variance | 0.29           |
|    reward_value_loss         | 4.47           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.51           |
|    ep_rew_mean               | 6.47           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 695            |
|    time_elapsed              | 150688         |
|    total_timesteps           | 88960          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.019         |
|    cost_value_loss           | 7.18e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.9e-05       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.05           |
|    mean_cost_advantages      | -0.00026078816 |
|    mean_reward_advantages    | 0.8748697      |
|    n_updates                 | 6940           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.21e-09      |
|    reward_explained_variance | -0.193         |
|    reward_value_loss         | 5.53           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.23         |
|    ep_rew_mean               | 6.19         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 696          |
|    time_elapsed              | 150796       |
|    total_timesteps           | 89088        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00553      |
|    cost_value_loss           | 7.22e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.93e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.32         |
|    mean_cost_advantages      | 0.0003988092 |
|    mean_reward_advantages    | 0.052843776  |
|    n_updates                 | 6950         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.41e-09     |
|    reward_explained_variance | -0.33        |
|    reward_value_loss         | 5.02         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.02          |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 697           |
|    time_elapsed              | 150902        |
|    total_timesteps           | 89216         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0254        |
|    cost_value_loss           | 7.25e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.89e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.75          |
|    mean_cost_advantages      | 0.00028736348 |
|    mean_reward_advantages    | 1.1305542     |
|    n_updates                 | 6960          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.45e-10     |
|    reward_explained_variance | -0.835        |
|    reward_value_loss         | 8.77          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.79          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 698           |
|    time_elapsed              | 151009        |
|    total_timesteps           | 89344         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00544       |
|    cost_value_loss           | 8.09e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.99e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.52          |
|    mean_cost_advantages      | 0.00014771783 |
|    mean_reward_advantages    | -0.42596668   |
|    n_updates                 | 6970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.83e-11     |
|    reward_explained_variance | -0.238        |
|    reward_value_loss         | 7.25          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 699           |
|    time_elapsed              | 151112        |
|    total_timesteps           | 89472         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0842       |
|    cost_value_loss           | 7.07e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.84e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.5           |
|    mean_cost_advantages      | 5.4258155e-05 |
|    mean_reward_advantages    | -0.79283834   |
|    n_updates                 | 6980          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.14e-09      |
|    reward_explained_variance | -0.376        |
|    reward_value_loss         | 5.77          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.91          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 700           |
|    time_elapsed              | 151216        |
|    total_timesteps           | 89600         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00784       |
|    cost_value_loss           | 6.87e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.8e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.45          |
|    mean_cost_advantages      | -0.0006847602 |
|    mean_reward_advantages    | 0.5383775     |
|    n_updates                 | 6990          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.96e-09      |
|    reward_explained_variance | -0.698        |
|    reward_value_loss         | 6.44          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.96           |
|    ep_rew_mean               | 5.92           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 701            |
|    time_elapsed              | 151321         |
|    total_timesteps           | 89728          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0428         |
|    cost_value_loss           | 7.55e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.87e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.91           |
|    mean_cost_advantages      | -0.00022283729 |
|    mean_reward_advantages    | 0.0053740367   |
|    n_updates                 | 7000           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.94e-10      |
|    reward_explained_variance | -0.517         |
|    reward_value_loss         | 5.2            |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.58         |
|    ep_rew_mean               | 5.55         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 702          |
|    time_elapsed              | 151424       |
|    total_timesteps           | 89856        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00149     |
|    cost_value_loss           | 6.13e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.78e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.8          |
|    mean_cost_advantages      | 0.0004928686 |
|    mean_reward_advantages    | 0.3664481    |
|    n_updates                 | 7010         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.06e-10    |
|    reward_explained_variance | -1.42        |
|    reward_value_loss         | 10.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.95         |
|    ep_rew_mean               | 5.92         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 703          |
|    time_elapsed              | 151529       |
|    total_timesteps           | 89984        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0439      |
|    cost_value_loss           | 1.07e-05     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.96e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.99         |
|    mean_cost_advantages      | 0.0008919018 |
|    mean_reward_advantages    | -2.026031    |
|    n_updates                 | 7020         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.65e-09    |
|    reward_explained_variance | 0.0715       |
|    reward_value_loss         | 9.35         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.29           |
|    ep_rew_mean               | 6.26           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 704            |
|    time_elapsed              | 151632         |
|    total_timesteps           | 90112          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0415        |
|    cost_value_loss           | 5.76e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.83e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.78           |
|    mean_cost_advantages      | -0.00027914337 |
|    mean_reward_advantages    | 1.6226468      |
|    n_updates                 | 7030           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.04e-09      |
|    reward_explained_variance | -0.37          |
|    reward_value_loss         | 10             |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.91         |
|    ep_rew_mean               | 5.89         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 705          |
|    time_elapsed              | 151737       |
|    total_timesteps           | 90240        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0241       |
|    cost_value_loss           | 5.76e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.9e-05     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.45         |
|    mean_cost_advantages      | 0.0001231402 |
|    mean_reward_advantages    | 0.67399347   |
|    n_updates                 | 7040         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.66e-09    |
|    reward_explained_variance | -0.0899      |
|    reward_value_loss         | 8.01         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.38         |
|    ep_rew_mean               | 5.36         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 706          |
|    time_elapsed              | 151842       |
|    total_timesteps           | 90368        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000292    |
|    cost_value_loss           | 6.67e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.79e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.36         |
|    mean_cost_advantages      | 4.014295e-05 |
|    mean_reward_advantages    | -1.5961013   |
|    n_updates                 | 7050         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -7.72e-10    |
|    reward_explained_variance | 0.161        |
|    reward_value_loss         | 5.33         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.01         |
|    ep_rew_mean               | 5.98         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 707          |
|    time_elapsed              | 151944       |
|    total_timesteps           | 90496        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.267       |
|    cost_value_loss           | 9.84e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -4.89e-05    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.31         |
|    mean_cost_advantages      | 0.0003342852 |
|    mean_reward_advantages    | 1.1823857    |
|    n_updates                 | 7060         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 9.51e-11     |
|    reward_explained_variance | -0.683       |
|    reward_value_loss         | 7.99         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.94          |
|    ep_rew_mean               | 5.91          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 708           |
|    time_elapsed              | 152049        |
|    total_timesteps           | 90624         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0631       |
|    cost_value_loss           | 6.54e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.98e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.17          |
|    mean_cost_advantages      | -0.0007568653 |
|    mean_reward_advantages    | 0.08960604    |
|    n_updates                 | 7070          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.88e-09      |
|    reward_explained_variance | -0.0314       |
|    reward_value_loss         | 5.42          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.77          |
|    ep_rew_mean               | 5.74          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 709           |
|    time_elapsed              | 152153        |
|    total_timesteps           | 90752         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0156       |
|    cost_value_loss           | 6.24e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.91e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.98          |
|    mean_cost_advantages      | 0.00077435316 |
|    mean_reward_advantages    | -0.51769596   |
|    n_updates                 | 7080          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.32e-10     |
|    reward_explained_variance | 0.0134        |
|    reward_value_loss         | 4.11          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 710           |
|    time_elapsed              | 152256        |
|    total_timesteps           | 90880         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00294      |
|    cost_value_loss           | 7.58e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.84e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.45          |
|    mean_cost_advantages      | 0.00068713306 |
|    mean_reward_advantages    | 0.21865389    |
|    n_updates                 | 7090          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.29e-10      |
|    reward_explained_variance | 0.396         |
|    reward_value_loss         | 3.01          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.97           |
|    ep_rew_mean               | 5.94           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 711            |
|    time_elapsed              | 152361         |
|    total_timesteps           | 91008          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.049          |
|    cost_value_loss           | 7.71e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.86e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.33           |
|    mean_cost_advantages      | -0.00037601177 |
|    mean_reward_advantages    | 2.1466033      |
|    n_updates                 | 7100           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.37e-09       |
|    reward_explained_variance | -0.987         |
|    reward_value_loss         | 12.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.03          |
|    ep_rew_mean               | 6             |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 712           |
|    time_elapsed              | 152463        |
|    total_timesteps           | 91136         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0467        |
|    cost_value_loss           | 8.09e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.7e-05      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.56          |
|    mean_cost_advantages      | 0.00044034485 |
|    mean_reward_advantages    | -2.6636713    |
|    n_updates                 | 7110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.95e-10      |
|    reward_explained_variance | 0.289         |
|    reward_value_loss         | 5.57          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.94           |
|    ep_rew_mean               | 5.91           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 713            |
|    time_elapsed              | 152567         |
|    total_timesteps           | 91264          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0017         |
|    cost_value_loss           | 7.11e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.88e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.62           |
|    mean_cost_advantages      | -0.00022232837 |
|    mean_reward_advantages    | -0.12298922    |
|    n_updates                 | 7120           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.32e-09      |
|    reward_explained_variance | 0.408          |
|    reward_value_loss         | 3.63           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.53          |
|    ep_rew_mean               | 5.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 714           |
|    time_elapsed              | 152669        |
|    total_timesteps           | 91392         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0192       |
|    cost_value_loss           | 6.82e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.86e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.36          |
|    mean_cost_advantages      | -0.0006952173 |
|    mean_reward_advantages    | 0.7690057     |
|    n_updates                 | 7130          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.43e-09      |
|    reward_explained_variance | -1.14         |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.89          |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 715           |
|    time_elapsed              | 152774        |
|    total_timesteps           | 91520         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0682       |
|    cost_value_loss           | 6.88e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -4.74e-05     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.55          |
|    mean_cost_advantages      | -0.0005682411 |
|    mean_reward_advantages    | -1.0663433    |
|    n_updates                 | 7140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.14e-10      |
|    reward_explained_variance | 0.403         |
|    reward_value_loss         | 3.72          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.9            |
|    ep_rew_mean               | 5.88           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 716            |
|    time_elapsed              | 152879         |
|    total_timesteps           | 91648          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0321        |
|    cost_value_loss           | 6.79e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -4.82e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.99           |
|    mean_cost_advantages      | -0.00017034257 |
|    mean_reward_advantages    | 1.7511399      |
|    n_updates                 | 7150           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.79e-09       |
|    reward_explained_variance | -0.346         |
|    reward_value_loss         | 9.98           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.28           |
|    ep_rew_mean               | 6.25           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 717            |
|    time_elapsed              | 152982         |
|    total_timesteps           | 91776          |
| train/                       |                |
|    approx_kl                 | 0.010875637    |
|    average_cost              | 0.0            |
|    clip_fraction             | 0.00703        |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0108        |
|    cost_value_loss           | 5.74e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -1.99e-05      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.82           |
|    mean_cost_advantages      | -0.00013993365 |
|    mean_reward_advantages    | 0.04976044     |
|    n_updates                 | 7160           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -0.00055       |
|    reward_explained_variance | -0.655         |
|    reward_value_loss         | 9.44           |
|    total_cost                | 0.0            |
-------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 7.42            |
|    ep_rew_mean               | 6.39            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 718             |
|    time_elapsed              | 153089          |
|    total_timesteps           | 91904           |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | 0.00181         |
|    cost_value_loss           | 8.36e-06        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -6.7e-06        |
|    learning_rate             | 0.0005          |
|    loss                      | 1.81            |
|    mean_cost_advantages      | -0.000111792484 |
|    mean_reward_advantages    | -0.6595533      |
|    n_updates                 | 7170            |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | -1.34e-08       |
|    reward_explained_variance | 0.00835         |
|    reward_value_loss         | 4.63            |
|    total_cost                | 0.0             |
--------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.06           |
|    ep_rew_mean               | 6.03           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 719            |
|    time_elapsed              | 153194         |
|    total_timesteps           | 92032          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0216        |
|    cost_value_loss           | 6.69e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.07e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.03           |
|    mean_cost_advantages      | -0.00031730035 |
|    mean_reward_advantages    | 0.08688859     |
|    n_updates                 | 7180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.45e-09       |
|    reward_explained_variance | 0.126          |
|    reward_value_loss         | 6.29           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.71          |
|    ep_rew_mean               | 6.67          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 720           |
|    time_elapsed              | 153299        |
|    total_timesteps           | 92160         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0796       |
|    cost_value_loss           | 8.15e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.57          |
|    mean_cost_advantages      | 3.9487448e-05 |
|    mean_reward_advantages    | 0.2723252     |
|    n_updates                 | 7190          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.12e-09     |
|    reward_explained_variance | -0.555        |
|    reward_value_loss         | 12.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 721           |
|    time_elapsed              | 153403        |
|    total_timesteps           | 92288         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0317       |
|    cost_value_loss           | 6.04e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.72e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.72          |
|    mean_cost_advantages      | 0.00039709147 |
|    mean_reward_advantages    | -0.62425745   |
|    n_updates                 | 7200          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.91e-09     |
|    reward_explained_variance | 0.0317        |
|    reward_value_loss         | 5.15          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.68          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 722           |
|    time_elapsed              | 153507        |
|    total_timesteps           | 92416         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0335        |
|    cost_value_loss           | 6.51e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.98          |
|    mean_cost_advantages      | 3.4691322e-05 |
|    mean_reward_advantages    | 0.2927794     |
|    n_updates                 | 7210          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.82e-10      |
|    reward_explained_variance | -0.912        |
|    reward_value_loss         | 7.88          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.63          |
|    ep_rew_mean               | 6.59          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 723           |
|    time_elapsed              | 153612        |
|    total_timesteps           | 92544         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0025       |
|    cost_value_loss           | 4.11e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.11e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.5           |
|    mean_cost_advantages      | -0.0003072261 |
|    mean_reward_advantages    | 2.2185407     |
|    n_updates                 | 7220          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.28e-09     |
|    reward_explained_variance | -1.39         |
|    reward_value_loss         | 16.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.52           |
|    ep_rew_mean               | 6.48           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 724            |
|    time_elapsed              | 153717         |
|    total_timesteps           | 92672          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0955        |
|    cost_value_loss           | 6.3e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.86e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.33           |
|    mean_cost_advantages      | -0.00021440409 |
|    mean_reward_advantages    | -0.8463812     |
|    n_updates                 | 7230           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.37e-10      |
|    reward_explained_variance | -0.209         |
|    reward_value_loss         | 6.64           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.56           |
|    ep_rew_mean               | 6.52           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 725            |
|    time_elapsed              | 153821         |
|    total_timesteps           | 92800          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00666       |
|    cost_value_loss           | 6.32e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.22           |
|    mean_cost_advantages      | -0.00046670216 |
|    mean_reward_advantages    | -0.29415432    |
|    n_updates                 | 7240           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.77e-10      |
|    reward_explained_variance | -0.186         |
|    reward_value_loss         | 5              |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.65          |
|    ep_rew_mean               | 6.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 726           |
|    time_elapsed              | 153925        |
|    total_timesteps           | 92928         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0301       |
|    cost_value_loss           | 7.26e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.75          |
|    mean_cost_advantages      | 0.00038114694 |
|    mean_reward_advantages    | 0.59273744    |
|    n_updates                 | 7250          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.12e-09     |
|    reward_explained_variance | -0.632        |
|    reward_value_loss         | 7.07          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.79           |
|    ep_rew_mean               | 6.74           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 727            |
|    time_elapsed              | 154030         |
|    total_timesteps           | 93056          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0609        |
|    cost_value_loss           | 6.78e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.97           |
|    mean_cost_advantages      | -0.00066985225 |
|    mean_reward_advantages    | 1.2877984      |
|    n_updates                 | 7260           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.75e-09       |
|    reward_explained_variance | -1.55          |
|    reward_value_loss         | 13.7           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.14           |
|    ep_rew_mean               | 6.11           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 728            |
|    time_elapsed              | 154136         |
|    total_timesteps           | 93184          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00919       |
|    cost_value_loss           | 6.9e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.97           |
|    mean_cost_advantages      | -1.0954667e-05 |
|    mean_reward_advantages    | 0.9712406      |
|    n_updates                 | 7270           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.46e-09       |
|    reward_explained_variance | -0.784         |
|    reward_value_loss         | 9.82           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.74         |
|    ep_rew_mean               | 5.71         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 729          |
|    time_elapsed              | 154237       |
|    total_timesteps           | 93312        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0319       |
|    cost_value_loss           | 7.04e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.76e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.51         |
|    mean_cost_advantages      | 0.0003211333 |
|    mean_reward_advantages    | -0.32022727  |
|    n_updates                 | 7280         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.87e-09     |
|    reward_explained_variance | -1.3         |
|    reward_value_loss         | 13           |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.68         |
|    ep_rew_mean               | 5.65         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 730          |
|    time_elapsed              | 154342       |
|    total_timesteps           | 93440        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0481       |
|    cost_value_loss           | 7.36e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.85e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.46         |
|    mean_cost_advantages      | 0.0002670982 |
|    mean_reward_advantages    | -0.4982511   |
|    n_updates                 | 7290         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.69e-10     |
|    reward_explained_variance | 0.22         |
|    reward_value_loss         | 6.11         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.81          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 731           |
|    time_elapsed              | 154444        |
|    total_timesteps           | 93568         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0119       |
|    cost_value_loss           | 5.51e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.26          |
|    mean_cost_advantages      | 0.00022846147 |
|    mean_reward_advantages    | 0.11398481    |
|    n_updates                 | 7300          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.52e-10     |
|    reward_explained_variance | -0.15         |
|    reward_value_loss         | 4.73          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.25           |
|    ep_rew_mean               | 5.23           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 732            |
|    time_elapsed              | 154548         |
|    total_timesteps           | 93696          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00445       |
|    cost_value_loss           | 6.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.61           |
|    mean_cost_advantages      | -0.00030192526 |
|    mean_reward_advantages    | 0.022434114    |
|    n_updates                 | 7310           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.23e-09       |
|    reward_explained_variance | -0.0661        |
|    reward_value_loss         | 4.13           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.39          |
|    ep_rew_mean               | 5.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 733           |
|    time_elapsed              | 154654        |
|    total_timesteps           | 93824         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0167        |
|    cost_value_loss           | 6.15e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.82          |
|    mean_cost_advantages      | 0.00047857544 |
|    mean_reward_advantages    | -0.25400817   |
|    n_updates                 | 7320          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.41e-09     |
|    reward_explained_variance | -0.225        |
|    reward_value_loss         | 4.84          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.54          |
|    ep_rew_mean               | 5.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 734           |
|    time_elapsed              | 154757        |
|    total_timesteps           | 93952         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0518        |
|    cost_value_loss           | 7.4e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.17          |
|    mean_cost_advantages      | -0.0006326797 |
|    mean_reward_advantages    | 0.25824344    |
|    n_updates                 | 7330          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.1e-09       |
|    reward_explained_variance | -0.69         |
|    reward_value_loss         | 6.81          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.76           |
|    ep_rew_mean               | 5.73           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 735            |
|    time_elapsed              | 154863         |
|    total_timesteps           | 94080          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0184         |
|    cost_value_loss           | 6.3e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.75           |
|    mean_cost_advantages      | -0.00039954082 |
|    mean_reward_advantages    | 1.2627751      |
|    n_updates                 | 7340           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.09e-11       |
|    reward_explained_variance | -1.28          |
|    reward_value_loss         | 10.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.26          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 736           |
|    time_elapsed              | 154967        |
|    total_timesteps           | 94208         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00435       |
|    cost_value_loss           | 6.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.3           |
|    mean_cost_advantages      | 7.1280956e-05 |
|    mean_reward_advantages    | -0.11858696   |
|    n_updates                 | 7350          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.31e-10      |
|    reward_explained_variance | -0.244        |
|    reward_value_loss         | 6.45          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.19          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 737           |
|    time_elapsed              | 155071        |
|    total_timesteps           | 94336         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00454       |
|    cost_value_loss           | 4.79e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.09e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.44          |
|    mean_cost_advantages      | 0.00017444415 |
|    mean_reward_advantages    | 1.6661361     |
|    n_updates                 | 7360          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.87e-09     |
|    reward_explained_variance | -0.917        |
|    reward_value_loss         | 7.65          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.86          |
|    ep_rew_mean               | 5.82          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 738           |
|    time_elapsed              | 155176        |
|    total_timesteps           | 94464         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.028         |
|    cost_value_loss           | 6.35e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.57          |
|    mean_cost_advantages      | 0.00074620586 |
|    mean_reward_advantages    | -0.7868587    |
|    n_updates                 | 7370          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.39e-09      |
|    reward_explained_variance | -0.12         |
|    reward_value_loss         | 4.46          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.96           |
|    ep_rew_mean               | 5.92           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 739            |
|    time_elapsed              | 155278         |
|    total_timesteps           | 94592          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00281        |
|    cost_value_loss           | 6.88e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.75e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.5            |
|    mean_cost_advantages      | 0.000105118386 |
|    mean_reward_advantages    | 0.19578154     |
|    n_updates                 | 7380           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.75e-11       |
|    reward_explained_variance | -1.43          |
|    reward_value_loss         | 8.3            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.82          |
|    ep_rew_mean               | 5.79          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 740           |
|    time_elapsed              | 155383        |
|    total_timesteps           | 94720         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.064         |
|    cost_value_loss           | 6.08e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.88          |
|    mean_cost_advantages      | 8.2353334e-05 |
|    mean_reward_advantages    | 0.15183897    |
|    n_updates                 | 7390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.76e-10      |
|    reward_explained_variance | -0.207        |
|    reward_value_loss         | 6.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.59          |
|    ep_rew_mean               | 5.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 741           |
|    time_elapsed              | 155485        |
|    total_timesteps           | 94848         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0668        |
|    cost_value_loss           | 5.34e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.67e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.84          |
|    mean_cost_advantages      | 0.00016376056 |
|    mean_reward_advantages    | 1.4414691     |
|    n_updates                 | 7400          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.05e-11      |
|    reward_explained_variance | -1.65         |
|    reward_value_loss         | 12.1          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.72           |
|    ep_rew_mean               | 5.7            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 742            |
|    time_elapsed              | 155590         |
|    total_timesteps           | 94976          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0301        |
|    cost_value_loss           | 6.72e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.85e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.06           |
|    mean_cost_advantages      | 0.000102332204 |
|    mean_reward_advantages    | 0.24555579     |
|    n_updates                 | 7410           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.44e-09      |
|    reward_explained_variance | 0.149          |
|    reward_value_loss         | 7.23           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.15          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 743           |
|    time_elapsed              | 155694        |
|    total_timesteps           | 95104         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00602      |
|    cost_value_loss           | 5.6e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.75e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.12          |
|    mean_cost_advantages      | 0.00030425523 |
|    mean_reward_advantages    | 0.40010527    |
|    n_updates                 | 7420          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.74e-10     |
|    reward_explained_variance | -0.307        |
|    reward_value_loss         | 9.91          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7             |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 744           |
|    time_elapsed              | 155798        |
|    total_timesteps           | 95232         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0469        |
|    cost_value_loss           | 6.05e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.09          |
|    mean_cost_advantages      | -8.229159e-05 |
|    mean_reward_advantages    | 0.653245      |
|    n_updates                 | 7430          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.3e-09       |
|    reward_explained_variance | -0.699        |
|    reward_value_loss         | 8.58          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 745           |
|    time_elapsed              | 155907        |
|    total_timesteps           | 95360         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0166        |
|    cost_value_loss           | 5.22e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.98          |
|    mean_cost_advantages      | 0.00022956017 |
|    mean_reward_advantages    | -0.08554489   |
|    n_updates                 | 7440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.57e-10      |
|    reward_explained_variance | 0.0203        |
|    reward_value_loss         | 5.71          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.71          |
|    ep_rew_mean               | 5.69          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 746           |
|    time_elapsed              | 156030        |
|    total_timesteps           | 95488         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0225        |
|    cost_value_loss           | 5.27e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.71          |
|    mean_cost_advantages      | 0.00012624415 |
|    mean_reward_advantages    | -0.17954886   |
|    n_updates                 | 7450          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.63e-10     |
|    reward_explained_variance | 0.117         |
|    reward_value_loss         | 4.18          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.38          |
|    ep_rew_mean               | 5.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 747           |
|    time_elapsed              | 156141        |
|    total_timesteps           | 95616         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0169       |
|    cost_value_loss           | 6.74e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.6           |
|    mean_cost_advantages      | 0.00026189373 |
|    mean_reward_advantages    | -0.04575704   |
|    n_updates                 | 7460          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.25e-10     |
|    reward_explained_variance | -0.263        |
|    reward_value_loss         | 5.61          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.35           |
|    ep_rew_mean               | 5.33           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 748            |
|    time_elapsed              | 156245         |
|    total_timesteps           | 95744          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.067         |
|    cost_value_loss           | 8.14e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.44           |
|    mean_cost_advantages      | -4.9001035e-05 |
|    mean_reward_advantages    | 1.6657547      |
|    n_updates                 | 7470           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.24e-10      |
|    reward_explained_variance | -1.73          |
|    reward_value_loss         | 12.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.78           |
|    ep_rew_mean               | 5.76           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 749            |
|    time_elapsed              | 156350         |
|    total_timesteps           | 95872          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0266        |
|    cost_value_loss           | 5.67e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.78           |
|    mean_cost_advantages      | -4.2663778e-05 |
|    mean_reward_advantages    | 0.5341282      |
|    n_updates                 | 7480           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.77e-10      |
|    reward_explained_variance | -0.673         |
|    reward_value_loss         | 12.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.54           |
|    ep_rew_mean               | 5.52           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 750            |
|    time_elapsed              | 156453         |
|    total_timesteps           | 96000          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0155         |
|    cost_value_loss           | 3.97e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.92e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.81           |
|    mean_cost_advantages      | -0.00016061284 |
|    mean_reward_advantages    | -0.063961826   |
|    n_updates                 | 7490           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.91e-10      |
|    reward_explained_variance | 0.244          |
|    reward_value_loss         | 4.97           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.7          |
|    ep_rew_mean               | 5.67         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 751          |
|    time_elapsed              | 156559       |
|    total_timesteps           | 96128        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00131     |
|    cost_value_loss           | 4.74e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.88e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.92         |
|    mean_cost_advantages      | 0.0004609721 |
|    mean_reward_advantages    | -0.3668568   |
|    n_updates                 | 7500         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.4e-10      |
|    reward_explained_variance | 0.259        |
|    reward_value_loss         | 4.77         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.21         |
|    ep_rew_mean               | 6.18         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 752          |
|    time_elapsed              | 156660       |
|    total_timesteps           | 96256        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0119       |
|    cost_value_loss           | 5.5e-06      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.85e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.93         |
|    mean_cost_advantages      | 0.0004149874 |
|    mean_reward_advantages    | 0.65089756   |
|    n_updates                 | 7510         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.62e-09     |
|    reward_explained_variance | -0.679       |
|    reward_value_loss         | 8.83         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.2            |
|    ep_rew_mean               | 6.18           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 753            |
|    time_elapsed              | 156770         |
|    total_timesteps           | 96384          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0128        |
|    cost_value_loss           | 5.38e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.88           |
|    mean_cost_advantages      | -0.00065311184 |
|    mean_reward_advantages    | 0.950472       |
|    n_updates                 | 7520           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.52e-10      |
|    reward_explained_variance | -0.784         |
|    reward_value_loss         | 8.62           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.33           |
|    ep_rew_mean               | 6.3            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 754            |
|    time_elapsed              | 156876         |
|    total_timesteps           | 96512          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.026          |
|    cost_value_loss           | 5.27e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.51           |
|    mean_cost_advantages      | -0.00019830224 |
|    mean_reward_advantages    | 1.0887218      |
|    n_updates                 | 7530           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.09e-09      |
|    reward_explained_variance | -0.985         |
|    reward_value_loss         | 13.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.51         |
|    ep_rew_mean               | 6.48         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 755          |
|    time_elapsed              | 156978       |
|    total_timesteps           | 96640        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0259      |
|    cost_value_loss           | 5.13e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.98e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.22         |
|    mean_cost_advantages      | 8.172523e-05 |
|    mean_reward_advantages    | -1.0028838   |
|    n_updates                 | 7540         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -9.61e-10    |
|    reward_explained_variance | 0.406        |
|    reward_value_loss         | 5.77         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.79         |
|    ep_rew_mean               | 6.75         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 756          |
|    time_elapsed              | 157083       |
|    total_timesteps           | 96768        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0197      |
|    cost_value_loss           | 7.24e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.94e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.99         |
|    mean_cost_advantages      | -0.000579747 |
|    mean_reward_advantages    | 1.791746     |
|    n_updates                 | 7550         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.68e-09    |
|    reward_explained_variance | 0.00733      |
|    reward_value_loss         | 9.26         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.35         |
|    ep_rew_mean               | 7.3          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 757          |
|    time_elapsed              | 157187       |
|    total_timesteps           | 96896        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00031     |
|    cost_value_loss           | 5.81e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.99e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.16         |
|    mean_cost_advantages      | 0.0013225988 |
|    mean_reward_advantages    | -1.1308429   |
|    n_updates                 | 7560         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.39e-10     |
|    reward_explained_variance | -0.316       |
|    reward_value_loss         | 8.83         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.33           |
|    ep_rew_mean               | 7.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 758            |
|    time_elapsed              | 157295         |
|    total_timesteps           | 97024          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0119         |
|    cost_value_loss           | 4.68e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.85e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 8.84           |
|    mean_cost_advantages      | -0.00020383752 |
|    mean_reward_advantages    | 3.2309399      |
|    n_updates                 | 7570           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.26e-09       |
|    reward_explained_variance | -2.21          |
|    reward_value_loss         | 23.7           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.54         |
|    ep_rew_mean               | 6.5          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 759          |
|    time_elapsed              | 157399       |
|    total_timesteps           | 97152        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00889     |
|    cost_value_loss           | 6.4e-06      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -6.02e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.37         |
|    mean_cost_advantages      | 0.0006589684 |
|    mean_reward_advantages    | -2.7592597   |
|    n_updates                 | 7580         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.01e-09    |
|    reward_explained_variance | 0.0291       |
|    reward_value_loss         | 10.3         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.4          |
|    ep_rew_mean               | 6.36         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 760          |
|    time_elapsed              | 157506       |
|    total_timesteps           | 97280        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0169      |
|    cost_value_loss           | 5.77e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.82e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.99         |
|    mean_cost_advantages      | -0.000453084 |
|    mean_reward_advantages    | -0.19967917  |
|    n_updates                 | 7590         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.2e-09      |
|    reward_explained_variance | 0.246        |
|    reward_value_loss         | 4.82         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.1           |
|    ep_rew_mean               | 6.07          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 761           |
|    time_elapsed              | 157624        |
|    total_timesteps           | 97408         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00723       |
|    cost_value_loss           | 5.24e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 5.07          |
|    mean_cost_advantages      | 0.00021487544 |
|    mean_reward_advantages    | 1.1045678     |
|    n_updates                 | 7600          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.09e-09     |
|    reward_explained_variance | -0.91         |
|    reward_value_loss         | 9.4           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.11         |
|    ep_rew_mean               | 6.07         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 762          |
|    time_elapsed              | 157742       |
|    total_timesteps           | 97536        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.012       |
|    cost_value_loss           | 5.32e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.73e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.77         |
|    mean_cost_advantages      | 0.0001612789 |
|    mean_reward_advantages    | 0.91434854   |
|    n_updates                 | 7610         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.12e-09    |
|    reward_explained_variance | -1.53        |
|    reward_value_loss         | 14.2         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.59          |
|    ep_rew_mean               | 5.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 763           |
|    time_elapsed              | 157856        |
|    total_timesteps           | 97664         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0036       |
|    cost_value_loss           | 5.57e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.94          |
|    mean_cost_advantages      | 0.00038140852 |
|    mean_reward_advantages    | -1.1688929    |
|    n_updates                 | 7620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.57e-09     |
|    reward_explained_variance | 0.131         |
|    reward_value_loss         | 6             |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.81           |
|    ep_rew_mean               | 5.77           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 764            |
|    time_elapsed              | 157975         |
|    total_timesteps           | 97792          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.011         |
|    cost_value_loss           | 5.88e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.9e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.98           |
|    mean_cost_advantages      | -1.2060518e-05 |
|    mean_reward_advantages    | -0.8399414     |
|    n_updates                 | 7630           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.87e-09       |
|    reward_explained_variance | 0.0363         |
|    reward_value_loss         | 4.78           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.88          |
|    ep_rew_mean               | 5.84          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 765           |
|    time_elapsed              | 158094        |
|    total_timesteps           | 97920         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0119       |
|    cost_value_loss           | 5.39e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.45          |
|    mean_cost_advantages      | -9.043804e-05 |
|    mean_reward_advantages    | 0.1957756     |
|    n_updates                 | 7640          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.74e-10      |
|    reward_explained_variance | -0.532        |
|    reward_value_loss         | 8.3           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.34          |
|    ep_rew_mean               | 6.3           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 766           |
|    time_elapsed              | 158217        |
|    total_timesteps           | 98048         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.179        |
|    cost_value_loss           | 7.03e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.8           |
|    mean_cost_advantages      | -0.0011533282 |
|    mean_reward_advantages    | 1.2313089     |
|    n_updates                 | 7650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.7e-09      |
|    reward_explained_variance | -1.63         |
|    reward_value_loss         | 11.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.48          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 767           |
|    time_elapsed              | 158343        |
|    total_timesteps           | 98176         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.029        |
|    cost_value_loss           | 4.62e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.61          |
|    mean_cost_advantages      | -0.0011839081 |
|    mean_reward_advantages    | 1.0521593     |
|    n_updates                 | 7660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.78e-09      |
|    reward_explained_variance | -1.51         |
|    reward_value_loss         | 13.6          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 768           |
|    time_elapsed              | 158467        |
|    total_timesteps           | 98304         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0059        |
|    cost_value_loss           | 5.22e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.35          |
|    mean_cost_advantages      | 6.4375316e-05 |
|    mean_reward_advantages    | -0.73911756   |
|    n_updates                 | 7670          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.09e-10     |
|    reward_explained_variance | -0.0663       |
|    reward_value_loss         | 8.39          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.74           |
|    ep_rew_mean               | 6.71           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 769            |
|    time_elapsed              | 158586         |
|    total_timesteps           | 98432          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00442       |
|    cost_value_loss           | 6.18e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.91           |
|    mean_cost_advantages      | -4.6961613e-05 |
|    mean_reward_advantages    | -0.4443707     |
|    n_updates                 | 7680           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.73e-09       |
|    reward_explained_variance | 0.0469         |
|    reward_value_loss         | 6.44           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.7          |
|    ep_rew_mean               | 6.67         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 770          |
|    time_elapsed              | 158711       |
|    total_timesteps           | 98560        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0197       |
|    cost_value_loss           | 4.77e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.75e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 4.24         |
|    mean_cost_advantages      | 0.0004055563 |
|    mean_reward_advantages    | -0.31803763  |
|    n_updates                 | 7690         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.67e-10    |
|    reward_explained_variance | -0.44        |
|    reward_value_loss         | 9.62         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.64          |
|    ep_rew_mean               | 6.6           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 771           |
|    time_elapsed              | 158837        |
|    total_timesteps           | 98688         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0194       |
|    cost_value_loss           | 5.38e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.98e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.16          |
|    mean_cost_advantages      | -0.0002758817 |
|    mean_reward_advantages    | -0.124204     |
|    n_updates                 | 7700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.67e-10      |
|    reward_explained_variance | -0.352        |
|    reward_value_loss         | 6.9           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.81           |
|    ep_rew_mean               | 5.78           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 772            |
|    time_elapsed              | 158957         |
|    total_timesteps           | 98816          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0186        |
|    cost_value_loss           | 5.47e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.94e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.03           |
|    mean_cost_advantages      | -0.00013831316 |
|    mean_reward_advantages    | 0.36069316     |
|    n_updates                 | 7710           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.53e-10       |
|    reward_explained_variance | 0.19           |
|    reward_value_loss         | 4.5            |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.69         |
|    ep_rew_mean               | 5.66         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 773          |
|    time_elapsed              | 159080       |
|    total_timesteps           | 98944        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00154      |
|    cost_value_loss           | 4.22e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.75e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.66         |
|    mean_cost_advantages      | 0.0005069677 |
|    mean_reward_advantages    | -0.056860983 |
|    n_updates                 | 7720         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.5e-10      |
|    reward_explained_variance | 0.105        |
|    reward_value_loss         | 3.58         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.99         |
|    ep_rew_mean               | 5.96         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 774          |
|    time_elapsed              | 159202       |
|    total_timesteps           | 99072        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0186      |
|    cost_value_loss           | 5.55e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.82e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.76         |
|    mean_cost_advantages      | 0.0006713777 |
|    mean_reward_advantages    | -0.023770973 |
|    n_updates                 | 7730         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1e-09        |
|    reward_explained_variance | -0.477       |
|    reward_value_loss         | 5.84         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.3            |
|    ep_rew_mean               | 6.27           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 775            |
|    time_elapsed              | 159326         |
|    total_timesteps           | 99200          |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0398         |
|    cost_value_loss           | 5.17e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.66e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.96           |
|    mean_cost_advantages      | -0.00029300342 |
|    mean_reward_advantages    | 1.0184598      |
|    n_updates                 | 7740           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.24e-10      |
|    reward_explained_variance | -0.691         |
|    reward_value_loss         | 10.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.23          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 776           |
|    time_elapsed              | 159451        |
|    total_timesteps           | 99328         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0267        |
|    cost_value_loss           | 4.95e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.89          |
|    mean_cost_advantages      | -0.0007688337 |
|    mean_reward_advantages    | 1.7466038     |
|    n_updates                 | 7750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.92e-09      |
|    reward_explained_variance | -0.393        |
|    reward_value_loss         | 9.15          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.02          |
|    ep_rew_mean               | 6             |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 777           |
|    time_elapsed              | 159567        |
|    total_timesteps           | 99456         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0467        |
|    cost_value_loss           | 4.54e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.72          |
|    mean_cost_advantages      | -0.0001875796 |
|    mean_reward_advantages    | -0.4038325    |
|    n_updates                 | 7760          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.05e-09      |
|    reward_explained_variance | -0.767        |
|    reward_value_loss         | 14.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.85         |
|    ep_rew_mean               | 5.83         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 778          |
|    time_elapsed              | 159679       |
|    total_timesteps           | 99584        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0141       |
|    cost_value_loss           | 4.37e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.81e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.78         |
|    mean_cost_advantages      | 0.0003150902 |
|    mean_reward_advantages    | -1.5714467   |
|    n_updates                 | 7770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -5.28e-09    |
|    reward_explained_variance | 0.329        |
|    reward_value_loss         | 6.6          |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.32         |
|    ep_rew_mean               | 5.29         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 779          |
|    time_elapsed              | 159789       |
|    total_timesteps           | 99712        |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0202      |
|    cost_value_loss           | 6.85e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.82e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.26         |
|    mean_cost_advantages      | 9.972895e-05 |
|    mean_reward_advantages    | -0.29008526  |
|    n_updates                 | 7780         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.36e-09    |
|    reward_explained_variance | 0.0677       |
|    reward_value_loss         | 5.54         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 5.87          |
|    ep_rew_mean               | 4.85          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 780           |
|    time_elapsed              | 159901        |
|    total_timesteps           | 99840         |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0177       |
|    cost_value_loss           | 5.07e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.28          |
|    mean_cost_advantages      | -0.0003065068 |
|    mean_reward_advantages    | 0.25498778    |
|    n_updates                 | 7790          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.52e-10     |
|    reward_explained_variance | -0.388        |
|    reward_value_loss         | 6.14          |
|    total_cost                | 0.0           |
------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 5.7             |
|    ep_rew_mean               | 4.68            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 781             |
|    time_elapsed              | 160014          |
|    total_timesteps           | 99968           |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | 0.0025          |
|    cost_value_loss           | 5.37e-06        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -5.87e-06       |
|    learning_rate             | 0.0005          |
|    loss                      | 1.88            |
|    mean_cost_advantages      | -0.000107644955 |
|    mean_reward_advantages    | 0.45870507      |
|    n_updates                 | 7800            |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | 6.81e-10        |
|    reward_explained_variance | -0.0851         |
|    reward_value_loss         | 3.61            |
|    total_cost                | 0.0             |
--------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.03          |
|    ep_rew_mean               | 5.01          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 782           |
|    time_elapsed              | 160138        |
|    total_timesteps           | 100096        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0211        |
|    cost_value_loss           | 6.08e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.83          |
|    mean_cost_advantages      | -0.0004821318 |
|    mean_reward_advantages    | 1.0790774     |
|    n_updates                 | 7810          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.19e-10     |
|    reward_explained_variance | -0.388        |
|    reward_value_loss         | 6.28          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.51           |
|    ep_rew_mean               | 5.49           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 783            |
|    time_elapsed              | 160260         |
|    total_timesteps           | 100224         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0327         |
|    cost_value_loss           | 4.69e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.7e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.88           |
|    mean_cost_advantages      | -0.00040022258 |
|    mean_reward_advantages    | 0.717023       |
|    n_updates                 | 7820           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.23e-09      |
|    reward_explained_variance | -1.34          |
|    reward_value_loss         | 8.92           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.33          |
|    ep_rew_mean               | 5.31          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 784           |
|    time_elapsed              | 160377        |
|    total_timesteps           | 100352        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00621      |
|    cost_value_loss           | 4.18e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.78e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.73          |
|    mean_cost_advantages      | 0.00033006107 |
|    mean_reward_advantages    | 0.54118764    |
|    n_updates                 | 7830          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.67e-10     |
|    reward_explained_variance | 0.252         |
|    reward_value_loss         | 4.24          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.76          |
|    ep_rew_mean               | 5.74          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 785           |
|    time_elapsed              | 160492        |
|    total_timesteps           | 100480        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0608        |
|    cost_value_loss           | 4.91e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 4.98          |
|    mean_cost_advantages      | 0.00032946054 |
|    mean_reward_advantages    | 0.20276038    |
|    n_updates                 | 7840          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.55e-09     |
|    reward_explained_variance | -0.918        |
|    reward_value_loss         | 9.32          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.22          |
|    ep_rew_mean               | 6.19          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 786           |
|    time_elapsed              | 160608        |
|    total_timesteps           | 100608        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0819       |
|    cost_value_loss           | 5.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.44          |
|    mean_cost_advantages      | -0.0008655381 |
|    mean_reward_advantages    | -0.24031597   |
|    n_updates                 | 7850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.49e-10     |
|    reward_explained_variance | -0.496        |
|    reward_value_loss         | 8.4           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.29           |
|    ep_rew_mean               | 6.26           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 787            |
|    time_elapsed              | 160729         |
|    total_timesteps           | 100736         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00229        |
|    cost_value_loss           | 4.02e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.87           |
|    mean_cost_advantages      | -0.00031692395 |
|    mean_reward_advantages    | 0.84821147     |
|    n_updates                 | 7860           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.18e-10       |
|    reward_explained_variance | -0.348         |
|    reward_value_loss         | 6.67           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.88         |
|    ep_rew_mean               | 5.85         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 788          |
|    time_elapsed              | 160845       |
|    total_timesteps           | 100864       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0202       |
|    cost_value_loss           | 5.38e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.85e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.23         |
|    mean_cost_advantages      | 9.788702e-05 |
|    mean_reward_advantages    | 0.34534982   |
|    n_updates                 | 7870         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.86e-09    |
|    reward_explained_variance | -0.265       |
|    reward_value_loss         | 6.4          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7             |
|    ep_rew_mean               | 5.97          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 789           |
|    time_elapsed              | 160962        |
|    total_timesteps           | 100992        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00568      |
|    cost_value_loss           | 5.71e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.26          |
|    mean_cost_advantages      | 0.00039224568 |
|    mean_reward_advantages    | -1.1871508    |
|    n_updates                 | 7880          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.43e-10      |
|    reward_explained_variance | 0.349         |
|    reward_value_loss         | 5.76          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.14          |
|    ep_rew_mean               | 6.11          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 790           |
|    time_elapsed              | 161077        |
|    total_timesteps           | 101120        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.148        |
|    cost_value_loss           | 4.75e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.44          |
|    mean_cost_advantages      | 0.00029573278 |
|    mean_reward_advantages    | -0.068304144  |
|    n_updates                 | 7890          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.08e-10     |
|    reward_explained_variance | 0.395         |
|    reward_value_loss         | 3.68          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.61           |
|    ep_rew_mean               | 5.58           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 791            |
|    time_elapsed              | 161194         |
|    total_timesteps           | 101248         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0178        |
|    cost_value_loss           | 4.95e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.95e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.42           |
|    mean_cost_advantages      | -0.00030559802 |
|    mean_reward_advantages    | 0.29687813     |
|    n_updates                 | 7900           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.82e-09       |
|    reward_explained_variance | -0.0726        |
|    reward_value_loss         | 6.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.38         |
|    ep_rew_mean               | 5.36         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 792          |
|    time_elapsed              | 161305       |
|    total_timesteps           | 101376       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0126      |
|    cost_value_loss           | 6.44e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.89e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.53         |
|    mean_cost_advantages      | 0.0011056644 |
|    mean_reward_advantages    | -0.072924435 |
|    n_updates                 | 7910         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.48e-10     |
|    reward_explained_variance | -0.0404      |
|    reward_value_loss         | 4.23         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.88           |
|    ep_rew_mean               | 5.85           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 793            |
|    time_elapsed              | 161425         |
|    total_timesteps           | 101504         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.126          |
|    cost_value_loss           | 5.38e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.98           |
|    mean_cost_advantages      | -0.00013822668 |
|    mean_reward_advantages    | 0.6198353      |
|    n_updates                 | 7920           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1e-09         |
|    reward_explained_variance | -0.306         |
|    reward_value_loss         | 6.39           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.41          |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 794           |
|    time_elapsed              | 161533        |
|    total_timesteps           | 101632        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0405       |
|    cost_value_loss           | 3.89e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.93          |
|    mean_cost_advantages      | -0.0002103727 |
|    mean_reward_advantages    | 2.7642279     |
|    n_updates                 | 7930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.89e-10      |
|    reward_explained_variance | -2.07         |
|    reward_value_loss         | 17.4          |
|    total_cost                | 0.0           |
------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.35        |
|    ep_rew_mean               | 6.32        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 795         |
|    time_elapsed              | 161641      |
|    total_timesteps           | 101760      |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0279      |
|    cost_value_loss           | 4.51e-06    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -6.06e-06   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.39        |
|    mean_cost_advantages      | 0.000323097 |
|    mean_reward_advantages    | -1.4393028  |
|    n_updates                 | 7940        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 3.86e-10    |
|    reward_explained_variance | -0.353      |
|    reward_value_loss         | 9.83        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.08           |
|    ep_rew_mean               | 6.05           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 796            |
|    time_elapsed              | 161747         |
|    total_timesteps           | 101888         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0224        |
|    cost_value_loss           | 5.49e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.87e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.83           |
|    mean_cost_advantages      | -0.00055304513 |
|    mean_reward_advantages    | -1.1616435     |
|    n_updates                 | 7950           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.29e-09       |
|    reward_explained_variance | 0.289          |
|    reward_value_loss         | 5.71           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.29           |
|    ep_rew_mean               | 6.26           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 797            |
|    time_elapsed              | 161855         |
|    total_timesteps           | 102016         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0256         |
|    cost_value_loss           | 4.33e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.1            |
|    mean_cost_advantages      | -0.00041847394 |
|    mean_reward_advantages    | 0.4083348      |
|    n_updates                 | 7960           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.34e-10      |
|    reward_explained_variance | 0.104          |
|    reward_value_loss         | 6.74           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.21           |
|    ep_rew_mean               | 6.18           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 798            |
|    time_elapsed              | 161964         |
|    total_timesteps           | 102144         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0107         |
|    cost_value_loss           | 4.62e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.52           |
|    mean_cost_advantages      | -0.00024084644 |
|    mean_reward_advantages    | 0.3660665      |
|    n_updates                 | 7970           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.69e-10       |
|    reward_explained_variance | -0.332         |
|    reward_value_loss         | 7.75           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.13           |
|    ep_rew_mean               | 6.1            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 799            |
|    time_elapsed              | 162068         |
|    total_timesteps           | 102272         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00357       |
|    cost_value_loss           | 5.04e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.64e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.53           |
|    mean_cost_advantages      | -3.5548066e-05 |
|    mean_reward_advantages    | -1.519641      |
|    n_updates                 | 7980           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.32e-10      |
|    reward_explained_variance | 0.436          |
|    reward_value_loss         | 7.02           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.76           |
|    ep_rew_mean               | 5.74           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 800            |
|    time_elapsed              | 162177         |
|    total_timesteps           | 102400         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00407       |
|    cost_value_loss           | 4.77e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.75e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.82           |
|    mean_cost_advantages      | -0.00012252925 |
|    mean_reward_advantages    | 0.9609968      |
|    n_updates                 | 7990           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.7e-10        |
|    reward_explained_variance | -0.273         |
|    reward_value_loss         | 6.68           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.05           |
|    ep_rew_mean               | 6.03           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 801            |
|    time_elapsed              | 162285         |
|    total_timesteps           | 102528         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00409        |
|    cost_value_loss           | 4.14e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.88e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.55           |
|    mean_cost_advantages      | -0.00024754967 |
|    mean_reward_advantages    | 0.6552519      |
|    n_updates                 | 8000           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.48e-09       |
|    reward_explained_variance | -1.06          |
|    reward_value_loss         | 10.1           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.71         |
|    ep_rew_mean               | 5.69         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 802          |
|    time_elapsed              | 162389       |
|    total_timesteps           | 102656       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0262       |
|    cost_value_loss           | 5.01e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.72e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.99         |
|    mean_cost_advantages      | 0.0004210475 |
|    mean_reward_advantages    | -1.0853107   |
|    n_updates                 | 8010         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.86e-09     |
|    reward_explained_variance | 0.175        |
|    reward_value_loss         | 4.58         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.99           |
|    ep_rew_mean               | 5.96           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 803            |
|    time_elapsed              | 162497         |
|    total_timesteps           | 102784         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0723        |
|    cost_value_loss           | 5.58e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.64           |
|    mean_cost_advantages      | -0.00028963274 |
|    mean_reward_advantages    | -0.06983955    |
|    n_updates                 | 8020           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.66e-09       |
|    reward_explained_variance | 0.344          |
|    reward_value_loss         | 4.08           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.23           |
|    ep_rew_mean               | 6.19           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 804            |
|    time_elapsed              | 162603         |
|    total_timesteps           | 102912         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0178         |
|    cost_value_loss           | 5.34e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.73           |
|    mean_cost_advantages      | -0.00031106436 |
|    mean_reward_advantages    | 0.85337627     |
|    n_updates                 | 8030           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.01e-09      |
|    reward_explained_variance | 0.00557        |
|    reward_value_loss         | 6.02           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.61          |
|    ep_rew_mean               | 5.58          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 805           |
|    time_elapsed              | 162709        |
|    total_timesteps           | 103040        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0532        |
|    cost_value_loss           | 3.41e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 6.24          |
|    mean_cost_advantages      | 0.00041590474 |
|    mean_reward_advantages    | 1.1262045     |
|    n_updates                 | 8040          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.77e-09      |
|    reward_explained_variance | -1.48         |
|    reward_value_loss         | 13.7          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 806           |
|    time_elapsed              | 162817        |
|    total_timesteps           | 103168        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0124        |
|    cost_value_loss           | 4.48e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.94          |
|    mean_cost_advantages      | 0.00016640668 |
|    mean_reward_advantages    | -1.0626512    |
|    n_updates                 | 8050          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.19e-10      |
|    reward_explained_variance | 0.271         |
|    reward_value_loss         | 5.43          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.39           |
|    ep_rew_mean               | 6.35           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 807            |
|    time_elapsed              | 162928         |
|    total_timesteps           | 103296         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0301         |
|    cost_value_loss           | 2.95e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.23           |
|    mean_cost_advantages      | -0.00035982567 |
|    mean_reward_advantages    | 0.91855454     |
|    n_updates                 | 8060           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.65e-09       |
|    reward_explained_variance | -1.27          |
|    reward_value_loss         | 10.7           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.58           |
|    ep_rew_mean               | 6.54           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 808            |
|    time_elapsed              | 163041         |
|    total_timesteps           | 103424         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.045          |
|    cost_value_loss           | 3.01e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.9e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 3.36           |
|    mean_cost_advantages      | -0.00034953989 |
|    mean_reward_advantages    | -0.09192114    |
|    n_updates                 | 8070           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.98e-11       |
|    reward_explained_variance | -1.21          |
|    reward_value_loss         | 9.08           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.11          |
|    ep_rew_mean               | 6.06          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 809           |
|    time_elapsed              | 163150        |
|    total_timesteps           | 103552        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0492        |
|    cost_value_loss           | 4.85e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.74e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.83          |
|    mean_cost_advantages      | 0.00021782264 |
|    mean_reward_advantages    | -0.5201188    |
|    n_updates                 | 8080          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.92e-10      |
|    reward_explained_variance | 0.0244        |
|    reward_value_loss         | 7.03          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.39           |
|    ep_rew_mean               | 6.35           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 810            |
|    time_elapsed              | 163268         |
|    total_timesteps           | 103680         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.146         |
|    cost_value_loss           | 5.07e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.1e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 2.37           |
|    mean_cost_advantages      | -0.00029602647 |
|    mean_reward_advantages    | 1.3697987      |
|    n_updates                 | 8090           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.19e-10      |
|    reward_explained_variance | -0.198         |
|    reward_value_loss         | 8.9            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.75          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 811           |
|    time_elapsed              | 163398        |
|    total_timesteps           | 103808        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0465        |
|    cost_value_loss           | 5.92e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.67e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.4           |
|    mean_cost_advantages      | -0.0004660653 |
|    mean_reward_advantages    | 1.89844       |
|    n_updates                 | 8100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.12e-10      |
|    reward_explained_variance | -1.77         |
|    reward_value_loss         | 12.9          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.63          |
|    ep_rew_mean               | 6.59          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 812           |
|    time_elapsed              | 163525        |
|    total_timesteps           | 103936        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0706       |
|    cost_value_loss           | 4.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.95e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.12          |
|    mean_cost_advantages      | 0.00036737707 |
|    mean_reward_advantages    | -0.28279272   |
|    n_updates                 | 8110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.8e-10       |
|    reward_explained_variance | -0.0156       |
|    reward_value_loss         | 6.22          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.38          |
|    ep_rew_mean               | 6.35          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 813           |
|    time_elapsed              | 163647        |
|    total_timesteps           | 104064        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0256       |
|    cost_value_loss           | 3.87e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.69e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.94          |
|    mean_cost_advantages      | 0.00020820714 |
|    mean_reward_advantages    | 0.41679633    |
|    n_updates                 | 8120          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.25e-10     |
|    reward_explained_variance | -0.188        |
|    reward_value_loss         | 8.67          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.31           |
|    ep_rew_mean               | 6.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 814            |
|    time_elapsed              | 163764         |
|    total_timesteps           | 104192         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0321        |
|    cost_value_loss           | 4.59e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.75e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.25           |
|    mean_cost_advantages      | -0.00018903535 |
|    mean_reward_advantages    | -1.1604395     |
|    n_updates                 | 8130           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.15e-09      |
|    reward_explained_variance | 0.486          |
|    reward_value_loss         | 5.36           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.22          |
|    ep_rew_mean               | 6.2           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 815           |
|    time_elapsed              | 163874        |
|    total_timesteps           | 104320        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0346        |
|    cost_value_loss           | 5.18e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.06e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.29          |
|    mean_cost_advantages      | 0.00019704975 |
|    mean_reward_advantages    | 0.16588865    |
|    n_updates                 | 8140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.26e-10      |
|    reward_explained_variance | 0.473         |
|    reward_value_loss         | 3.88          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.05          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 816           |
|    time_elapsed              | 163978        |
|    total_timesteps           | 104448        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0227       |
|    cost_value_loss           | 4.98e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.65          |
|    mean_cost_advantages      | -0.0004082685 |
|    mean_reward_advantages    | -0.098525584  |
|    n_updates                 | 8150          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.91e-10      |
|    reward_explained_variance | 0.104         |
|    reward_value_loss         | 4.77          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.84           |
|    ep_rew_mean               | 5.81           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 817            |
|    time_elapsed              | 164085         |
|    total_timesteps           | 104576         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00656       |
|    cost_value_loss           | 3.37e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.56           |
|    mean_cost_advantages      | -0.00035831862 |
|    mean_reward_advantages    | 0.49979264     |
|    n_updates                 | 8160           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.39e-10       |
|    reward_explained_variance | -0.346         |
|    reward_value_loss         | 10.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.71          |
|    ep_rew_mean               | 5.67          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 818           |
|    time_elapsed              | 164190        |
|    total_timesteps           | 104704        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0629        |
|    cost_value_loss           | 3.95e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.82          |
|    mean_cost_advantages      | 0.00019549097 |
|    mean_reward_advantages    | -0.6799829    |
|    n_updates                 | 8170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.27e-09      |
|    reward_explained_variance | -0.22         |
|    reward_value_loss         | 5.96          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.11           |
|    ep_rew_mean               | 6.07           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 819            |
|    time_elapsed              | 164296         |
|    total_timesteps           | 104832         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0225         |
|    cost_value_loss           | 4.52e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.22           |
|    mean_cost_advantages      | -0.00047210767 |
|    mean_reward_advantages    | 1.2621019      |
|    n_updates                 | 8180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.46e-09       |
|    reward_explained_variance | -2.43          |
|    reward_value_loss         | 17.4           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.38          |
|    ep_rew_mean               | 6.34          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 820           |
|    time_elapsed              | 164401        |
|    total_timesteps           | 104960        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0236        |
|    cost_value_loss           | 3.44e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.56          |
|    mean_cost_advantages      | 0.00015682964 |
|    mean_reward_advantages    | 0.17295466    |
|    n_updates                 | 8190          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.69e-11      |
|    reward_explained_variance | -0.704        |
|    reward_value_loss         | 9.84          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.45           |
|    ep_rew_mean               | 6.42           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 821            |
|    time_elapsed              | 164506         |
|    total_timesteps           | 105088         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0598         |
|    cost_value_loss           | 3.28e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.09           |
|    mean_cost_advantages      | -0.00019852769 |
|    mean_reward_advantages    | -0.3305118     |
|    n_updates                 | 8200           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.38e-11      |
|    reward_explained_variance | 0.01           |
|    reward_value_loss         | 9.3            |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.41           |
|    ep_rew_mean               | 6.39           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 822            |
|    time_elapsed              | 164612         |
|    total_timesteps           | 105216         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00462        |
|    cost_value_loss           | 4.11e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.78e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.9            |
|    mean_cost_advantages      | -9.8749835e-05 |
|    mean_reward_advantages    | -0.5248771     |
|    n_updates                 | 8210           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.14e-09       |
|    reward_explained_variance | -0.0662        |
|    reward_value_loss         | 7.06           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.31         |
|    ep_rew_mean               | 6.29         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 823          |
|    time_elapsed              | 164716       |
|    total_timesteps           | 105344       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00314     |
|    cost_value_loss           | 3.6e-06      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.76e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.41         |
|    mean_cost_advantages      | 0.0004078374 |
|    mean_reward_advantages    | -0.18454729  |
|    n_updates                 | 8220         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.1e-09     |
|    reward_explained_variance | -0.104       |
|    reward_value_loss         | 5.37         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.51          |
|    ep_rew_mean               | 6.49          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 824           |
|    time_elapsed              | 164822        |
|    total_timesteps           | 105472        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0311        |
|    cost_value_loss           | 4e-06         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.67e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.31          |
|    mean_cost_advantages      | 4.0497813e-05 |
|    mean_reward_advantages    | 0.5889114     |
|    n_updates                 | 8230          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.96e-09     |
|    reward_explained_variance | -0.85         |
|    reward_value_loss         | 7.66          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.55          |
|    ep_rew_mean               | 6.53          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 825           |
|    time_elapsed              | 164925        |
|    total_timesteps           | 105600        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0184        |
|    cost_value_loss           | 4.29e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.58          |
|    mean_cost_advantages      | 0.00022955917 |
|    mean_reward_advantages    | -0.034757793  |
|    n_updates                 | 8240          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.66e-09      |
|    reward_explained_variance | -0.215        |
|    reward_value_loss         | 7.73          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.6            |
|    ep_rew_mean               | 6.58           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 826            |
|    time_elapsed              | 165032         |
|    total_timesteps           | 105728         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0161         |
|    cost_value_loss           | 3.85e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.88e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.18           |
|    mean_cost_advantages      | -0.00050568907 |
|    mean_reward_advantages    | 0.2414327      |
|    n_updates                 | 8250           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.2e-09        |
|    reward_explained_variance | 0.119          |
|    reward_value_loss         | 5.05           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.24          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 827           |
|    time_elapsed              | 165137        |
|    total_timesteps           | 105856        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0139       |
|    cost_value_loss           | 4.02e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.09          |
|    mean_cost_advantages      | 0.00013361213 |
|    mean_reward_advantages    | -0.12963922   |
|    n_updates                 | 8260          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.28e-09      |
|    reward_explained_variance | -0.287        |
|    reward_value_loss         | 6.82          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.66         |
|    ep_rew_mean               | 6.64         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 828          |
|    time_elapsed              | 165241       |
|    total_timesteps           | 105984       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00709     |
|    cost_value_loss           | 3.91e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.92e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.62         |
|    mean_cost_advantages      | 0.0007161747 |
|    mean_reward_advantages    | -1.2383292   |
|    n_updates                 | 8270         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 9.13e-10     |
|    reward_explained_variance | 0.535        |
|    reward_value_loss         | 3.62         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 829           |
|    time_elapsed              | 165351        |
|    total_timesteps           | 106112        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0267       |
|    cost_value_loss           | 3.7e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.64e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.51          |
|    mean_cost_advantages      | -0.0006416481 |
|    mean_reward_advantages    | 1.2226555     |
|    n_updates                 | 8280          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.17e-09     |
|    reward_explained_variance | 0.053         |
|    reward_value_loss         | 6.89          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.81         |
|    ep_rew_mean               | 6.78         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 830          |
|    time_elapsed              | 165454       |
|    total_timesteps           | 106240       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0406      |
|    cost_value_loss           | 3.58e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.83e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.37         |
|    mean_cost_advantages      | -0.000636294 |
|    mean_reward_advantages    | -0.31107518  |
|    n_updates                 | 8290         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.71e-10     |
|    reward_explained_variance | -0.561       |
|    reward_value_loss         | 6.61         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.26           |
|    ep_rew_mean               | 6.23           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 831            |
|    time_elapsed              | 165559         |
|    total_timesteps           | 106368         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0604         |
|    cost_value_loss           | 4.11e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 8.49           |
|    mean_cost_advantages      | -0.00019486167 |
|    mean_reward_advantages    | 2.8374043      |
|    n_updates                 | 8300           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.91e-12       |
|    reward_explained_variance | -3.25          |
|    reward_value_loss         | 21             |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 832           |
|    time_elapsed              | 165662        |
|    total_timesteps           | 106496        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00531      |
|    cost_value_loss           | 5.33e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.98          |
|    mean_cost_advantages      | 0.00046180602 |
|    mean_reward_advantages    | -2.280782     |
|    n_updates                 | 8310          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.71e-09      |
|    reward_explained_variance | 0.0425        |
|    reward_value_loss         | 7.8           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.43          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 833           |
|    time_elapsed              | 165765        |
|    total_timesteps           | 106624        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.044         |
|    cost_value_loss           | 4.3e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.07          |
|    mean_cost_advantages      | 0.00018525292 |
|    mean_reward_advantages    | 0.994536      |
|    n_updates                 | 8320          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.96e-09     |
|    reward_explained_variance | -0.828        |
|    reward_value_loss         | 8.78          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.99           |
|    ep_rew_mean               | 5.95           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 834            |
|    time_elapsed              | 165870         |
|    total_timesteps           | 106752         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0213        |
|    cost_value_loss           | 4.26e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.21           |
|    mean_cost_advantages      | -0.00091381744 |
|    mean_reward_advantages    | -0.28776306    |
|    n_updates                 | 8330           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.19e-09      |
|    reward_explained_variance | -0.746         |
|    reward_value_loss         | 9.82           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.59          |
|    ep_rew_mean               | 5.55          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 835           |
|    time_elapsed              | 165974        |
|    total_timesteps           | 106880        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.024         |
|    cost_value_loss           | 4.08e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6e-06        |
|    learning_rate             | 0.0005        |
|    loss                      | 1.77          |
|    mean_cost_advantages      | 0.00021886286 |
|    mean_reward_advantages    | -0.47088408   |
|    n_updates                 | 8340          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.05e-09      |
|    reward_explained_variance | 0.309         |
|    reward_value_loss         | 4.28          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.46         |
|    ep_rew_mean               | 5.43         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 836          |
|    time_elapsed              | 166078       |
|    total_timesteps           | 107008       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00528      |
|    cost_value_loss           | 4.05e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.87e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 0.869        |
|    mean_cost_advantages      | 8.621119e-05 |
|    mean_reward_advantages    | -0.19307557  |
|    n_updates                 | 8350         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.77e-10     |
|    reward_explained_variance | 0.0561       |
|    reward_value_loss         | 3.17         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.51           |
|    ep_rew_mean               | 5.48           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 837            |
|    time_elapsed              | 166181         |
|    total_timesteps           | 107136         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0524        |
|    cost_value_loss           | 5.49e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.15           |
|    mean_cost_advantages      | -0.00045208685 |
|    mean_reward_advantages    | -0.2893381     |
|    n_updates                 | 8360           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.11e-09      |
|    reward_explained_variance | 0.414          |
|    reward_value_loss         | 2.63           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.56          |
|    ep_rew_mean               | 5.53          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 838           |
|    time_elapsed              | 166285        |
|    total_timesteps           | 107264        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00393       |
|    cost_value_loss           | 6.56e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.19          |
|    mean_cost_advantages      | -0.0012715317 |
|    mean_reward_advantages    | 1.6246235     |
|    n_updates                 | 8370          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.46e-09      |
|    reward_explained_variance | -1.87         |
|    reward_value_loss         | 9.22          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.34          |
|    ep_rew_mean               | 5.31          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 839           |
|    time_elapsed              | 166388        |
|    total_timesteps           | 107392        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0109       |
|    cost_value_loss           | 4.87e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.05          |
|    mean_cost_advantages      | 2.2902444e-05 |
|    mean_reward_advantages    | 0.7508658     |
|    n_updates                 | 8380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.82e-09      |
|    reward_explained_variance | 0.357         |
|    reward_value_loss         | 4.93          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.58          |
|    ep_rew_mean               | 5.55          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 840           |
|    time_elapsed              | 166489        |
|    total_timesteps           | 107520        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.013        |
|    cost_value_loss           | 5.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.08e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.96          |
|    mean_cost_advantages      | 0.00054014014 |
|    mean_reward_advantages    | -0.36900002   |
|    n_updates                 | 8390          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.05e-09      |
|    reward_explained_variance | -0.193        |
|    reward_value_loss         | 8.98          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.67          |
|    ep_rew_mean               | 5.63          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 841           |
|    time_elapsed              | 166594        |
|    total_timesteps           | 107648        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00813       |
|    cost_value_loss           | 3.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.56          |
|    mean_cost_advantages      | -0.0003276521 |
|    mean_reward_advantages    | 0.72880864    |
|    n_updates                 | 8400          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.88e-10     |
|    reward_explained_variance | -1.1          |
|    reward_value_loss         | 9.36          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.28          |
|    ep_rew_mean               | 5.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 842           |
|    time_elapsed              | 166697        |
|    total_timesteps           | 107776        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0233        |
|    cost_value_loss           | 4.01e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.1           |
|    mean_cost_advantages      | 0.00016922652 |
|    mean_reward_advantages    | -0.4263148    |
|    n_updates                 | 8410          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.5e-09       |
|    reward_explained_variance | -0.162        |
|    reward_value_loss         | 8             |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.39          |
|    ep_rew_mean               | 5.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 843           |
|    time_elapsed              | 166801        |
|    total_timesteps           | 107904        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0437        |
|    cost_value_loss           | 5.46e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.35          |
|    mean_cost_advantages      | 0.00042806612 |
|    mean_reward_advantages    | -0.88550264   |
|    n_updates                 | 8420          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.04e-09     |
|    reward_explained_variance | 0.367         |
|    reward_value_loss         | 3.81          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.72          |
|    ep_rew_mean               | 5.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 844           |
|    time_elapsed              | 166909        |
|    total_timesteps           | 108032        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0334       |
|    cost_value_loss           | 3.88e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.69e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.7           |
|    mean_cost_advantages      | -0.0005448393 |
|    mean_reward_advantages    | 0.6507058     |
|    n_updates                 | 8430          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.32e-10      |
|    reward_explained_variance | 0.0817        |
|    reward_value_loss         | 5.6           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.55          |
|    ep_rew_mean               | 5.53          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 845           |
|    time_elapsed              | 167020        |
|    total_timesteps           | 108160        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.019         |
|    cost_value_loss           | 3.54e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.83          |
|    mean_cost_advantages      | -7.352825e-05 |
|    mean_reward_advantages    | 0.7239617     |
|    n_updates                 | 8440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.82e-10     |
|    reward_explained_variance | -0.486        |
|    reward_value_loss         | 7.81          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.89         |
|    ep_rew_mean               | 5.86         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 846          |
|    time_elapsed              | 167142       |
|    total_timesteps           | 108288       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0137      |
|    cost_value_loss           | 3.56e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.92e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.49         |
|    mean_cost_advantages      | 0.0002499049 |
|    mean_reward_advantages    | -0.4869642   |
|    n_updates                 | 8450         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.26e-09    |
|    reward_explained_variance | -0.29        |
|    reward_value_loss         | 7.64         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.52           |
|    ep_rew_mean               | 6.49           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 847            |
|    time_elapsed              | 167251         |
|    total_timesteps           | 108416         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00451        |
|    cost_value_loss           | 3.51e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.02e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.85           |
|    mean_cost_advantages      | -0.00019094787 |
|    mean_reward_advantages    | 0.99930936     |
|    n_updates                 | 8460           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.27e-09      |
|    reward_explained_variance | -0.263         |
|    reward_value_loss         | 8.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.99           |
|    ep_rew_mean               | 6.95           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 848            |
|    time_elapsed              | 167360         |
|    total_timesteps           | 108544         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0206         |
|    cost_value_loss           | 3.59e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.51           |
|    mean_cost_advantages      | -9.5337775e-05 |
|    mean_reward_advantages    | 0.7044685      |
|    n_updates                 | 8470           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.51e-10      |
|    reward_explained_variance | -0.104         |
|    reward_value_loss         | 6.79           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.85          |
|    ep_rew_mean               | 6.81          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 849           |
|    time_elapsed              | 167464        |
|    total_timesteps           | 108672        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0134        |
|    cost_value_loss           | 3.25e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.02e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.14          |
|    mean_cost_advantages      | 0.00018570878 |
|    mean_reward_advantages    | -0.4539575    |
|    n_updates                 | 8480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.73e-09      |
|    reward_explained_variance | -0.149        |
|    reward_value_loss         | 5.16          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8              |
|    ep_rew_mean               | 6.95           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 850            |
|    time_elapsed              | 167568         |
|    total_timesteps           | 108800         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00918       |
|    cost_value_loss           | 4.08e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.88e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.93           |
|    mean_cost_advantages      | -0.00030675725 |
|    mean_reward_advantages    | 1.6579292      |
|    n_updates                 | 8490           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.39e-10      |
|    reward_explained_variance | -2.02          |
|    reward_value_loss         | 17.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.93          |
|    ep_rew_mean               | 6.89          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 851           |
|    time_elapsed              | 167672        |
|    total_timesteps           | 108928        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00337       |
|    cost_value_loss           | 3.14e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.87          |
|    mean_cost_advantages      | -0.0002637761 |
|    mean_reward_advantages    | -0.8610943    |
|    n_updates                 | 8500          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.55e-10     |
|    reward_explained_variance | -0.0619       |
|    reward_value_loss         | 7.28          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.71          |
|    ep_rew_mean               | 6.68          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 852           |
|    time_elapsed              | 167775        |
|    total_timesteps           | 109056        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.022        |
|    cost_value_loss           | 3.98e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.24          |
|    mean_cost_advantages      | 7.6150376e-05 |
|    mean_reward_advantages    | -0.8083591    |
|    n_updates                 | 8510          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.34e-10      |
|    reward_explained_variance | -0.157        |
|    reward_value_loss         | 6.21          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.59           |
|    ep_rew_mean               | 6.55           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 853            |
|    time_elapsed              | 167877         |
|    total_timesteps           | 109184         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0303        |
|    cost_value_loss           | 4.68e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.85e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.64           |
|    mean_cost_advantages      | -0.00029950822 |
|    mean_reward_advantages    | 0.1027611      |
|    n_updates                 | 8520           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.35e-09       |
|    reward_explained_variance | -0.459         |
|    reward_value_loss         | 5.87           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.46           |
|    ep_rew_mean               | 6.43           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 854            |
|    time_elapsed              | 167980         |
|    total_timesteps           | 109312         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0185        |
|    cost_value_loss           | 3.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.41           |
|    mean_cost_advantages      | -0.00015427505 |
|    mean_reward_advantages    | 0.61445165     |
|    n_updates                 | 8530           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.18e-09       |
|    reward_explained_variance | -0.424         |
|    reward_value_loss         | 6.09           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.75          |
|    ep_rew_mean               | 5.73          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 855           |
|    time_elapsed              | 168085        |
|    total_timesteps           | 109440        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00316       |
|    cost_value_loss           | 4.75e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.56          |
|    mean_cost_advantages      | -0.0007890487 |
|    mean_reward_advantages    | 0.157213      |
|    n_updates                 | 8540          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.53e-09      |
|    reward_explained_variance | 0.221         |
|    reward_value_loss         | 4.75          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.93         |
|    ep_rew_mean               | 5.91         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 856          |
|    time_elapsed              | 168191       |
|    total_timesteps           | 109568       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00833      |
|    cost_value_loss           | 5.61e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.88e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.93         |
|    mean_cost_advantages      | 0.0004234871 |
|    mean_reward_advantages    | -0.6463457   |
|    n_updates                 | 8550         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.39e-09     |
|    reward_explained_variance | 0.362        |
|    reward_value_loss         | 3.54         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.82         |
|    ep_rew_mean               | 5.79         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 857          |
|    time_elapsed              | 168293       |
|    total_timesteps           | 109696       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0372      |
|    cost_value_loss           | 3.92e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.74e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.14         |
|    mean_cost_advantages      | 0.0014951166 |
|    mean_reward_advantages    | 0.62803936   |
|    n_updates                 | 8560         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.01e-09    |
|    reward_explained_variance | 0.427        |
|    reward_value_loss         | 4.09         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.54         |
|    ep_rew_mean               | 5.52         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 858          |
|    time_elapsed              | 168401       |
|    total_timesteps           | 109824       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0325       |
|    cost_value_loss           | 4.75e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.81e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.5          |
|    mean_cost_advantages      | 0.0004939522 |
|    mean_reward_advantages    | 0.21275212   |
|    n_updates                 | 8570         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.3e-09     |
|    reward_explained_variance | 0.0854       |
|    reward_value_loss         | 5.65         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.5           |
|    ep_rew_mean               | 5.48          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 859           |
|    time_elapsed              | 168508        |
|    total_timesteps           | 109952        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0778        |
|    cost_value_loss           | 4.61e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.34          |
|    mean_cost_advantages      | 2.5834255e-05 |
|    mean_reward_advantages    | -0.14254022   |
|    n_updates                 | 8580          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.82e-09      |
|    reward_explained_variance | -0.179        |
|    reward_value_loss         | 6.18          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.68           |
|    ep_rew_mean               | 5.65           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 860            |
|    time_elapsed              | 168614         |
|    total_timesteps           | 110080         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0529         |
|    cost_value_loss           | 4.82e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.98e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.75           |
|    mean_cost_advantages      | -0.00026355797 |
|    mean_reward_advantages    | 0.58173084     |
|    n_updates                 | 8590           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -4.56e-10      |
|    reward_explained_variance | -0.123         |
|    reward_value_loss         | 5.91           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.81          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 861           |
|    time_elapsed              | 168723        |
|    total_timesteps           | 110208        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0385       |
|    cost_value_loss           | 4.37e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.58          |
|    mean_cost_advantages      | 2.7915627e-05 |
|    mean_reward_advantages    | -0.15399899   |
|    n_updates                 | 8600          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.14e-09     |
|    reward_explained_variance | 0.222         |
|    reward_value_loss         | 4.02          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.25          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 862           |
|    time_elapsed              | 168832        |
|    total_timesteps           | 110336        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0152        |
|    cost_value_loss           | 3.49e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.02          |
|    mean_cost_advantages      | 0.00025933448 |
|    mean_reward_advantages    | 0.98882395    |
|    n_updates                 | 8610          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.14e-10     |
|    reward_explained_variance | -0.5          |
|    reward_value_loss         | 10.4          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.37          |
|    ep_rew_mean               | 6.33          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 863           |
|    time_elapsed              | 168940        |
|    total_timesteps           | 110464        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0259       |
|    cost_value_loss           | 3.41e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.12          |
|    mean_cost_advantages      | -0.0004722152 |
|    mean_reward_advantages    | 0.6180016     |
|    n_updates                 | 8620          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.64e-09      |
|    reward_explained_variance | -0.0219       |
|    reward_value_loss         | 9.23          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.62          |
|    ep_rew_mean               | 6.58          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 864           |
|    time_elapsed              | 169047        |
|    total_timesteps           | 110592        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0324        |
|    cost_value_loss           | 3.09e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.77          |
|    mean_cost_advantages      | 0.00022859556 |
|    mean_reward_advantages    | -0.12197409   |
|    n_updates                 | 8630          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.32e-10      |
|    reward_explained_variance | -0.0218       |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.43          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 865           |
|    time_elapsed              | 169154        |
|    total_timesteps           | 110720        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0194       |
|    cost_value_loss           | 3.45e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.38          |
|    mean_cost_advantages      | 0.00014365875 |
|    mean_reward_advantages    | -0.60220504   |
|    n_updates                 | 8640          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.89e-09     |
|    reward_explained_variance | 0.0697        |
|    reward_value_loss         | 7.44          |
|    total_cost                | 0.0           |
------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 7.64            |
|    ep_rew_mean               | 6.6             |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 866             |
|    time_elapsed              | 169262          |
|    total_timesteps           | 110848          |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | -0.00812        |
|    cost_value_loss           | 4.05e-06        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -5.85e-06       |
|    learning_rate             | 0.0005          |
|    loss                      | 0.845           |
|    mean_cost_advantages      | -0.000102052094 |
|    mean_reward_advantages    | -0.8442602      |
|    n_updates                 | 8650            |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | -2.96e-09       |
|    reward_explained_variance | 0.53            |
|    reward_value_loss         | 3.47            |
|    total_cost                | 0.0             |
--------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.52           |
|    ep_rew_mean               | 6.48           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 867            |
|    time_elapsed              | 169370         |
|    total_timesteps           | 110976         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0153         |
|    cost_value_loss           | 4.22e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.92e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.86           |
|    mean_cost_advantages      | -0.00024864316 |
|    mean_reward_advantages    | 0.81415105     |
|    n_updates                 | 8660           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.92e-10      |
|    reward_explained_variance | -0.583         |
|    reward_value_loss         | 7.23           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.73           |
|    ep_rew_mean               | 5.7            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 868            |
|    time_elapsed              | 169476         |
|    total_timesteps           | 111104         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0134        |
|    cost_value_loss           | 4.41e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.29           |
|    mean_cost_advantages      | -0.00034413696 |
|    mean_reward_advantages    | -0.5483269     |
|    n_updates                 | 8670           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.46e-10      |
|    reward_explained_variance | 0.409          |
|    reward_value_loss         | 4.54           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.9           |
|    ep_rew_mean               | 5.87          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 869           |
|    time_elapsed              | 169585        |
|    total_timesteps           | 111232        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0211        |
|    cost_value_loss           | 4.95e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.68e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.63          |
|    mean_cost_advantages      | 0.00039825932 |
|    mean_reward_advantages    | -0.23789209   |
|    n_updates                 | 8680          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.55e-09     |
|    reward_explained_variance | 0.277         |
|    reward_value_loss         | 5.99          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.92         |
|    ep_rew_mean               | 5.89         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 870          |
|    time_elapsed              | 169691       |
|    total_timesteps           | 111360       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00265      |
|    cost_value_loss           | 3.68e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.76e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.07         |
|    mean_cost_advantages      | 0.0006549759 |
|    mean_reward_advantages    | 1.1828227    |
|    n_updates                 | 8690         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.24e-09     |
|    reward_explained_variance | -0.427       |
|    reward_value_loss         | 6.96         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.27          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 871           |
|    time_elapsed              | 169801        |
|    total_timesteps           | 111488        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00953      |
|    cost_value_loss           | 4.11e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.47          |
|    mean_cost_advantages      | 0.00033909464 |
|    mean_reward_advantages    | -0.11359121   |
|    n_updates                 | 8700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.84e-09      |
|    reward_explained_variance | 0.349         |
|    reward_value_loss         | 5.35          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.08           |
|    ep_rew_mean               | 6.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 872            |
|    time_elapsed              | 169908         |
|    total_timesteps           | 111616         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0111         |
|    cost_value_loss           | 3.44e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.88e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.25           |
|    mean_cost_advantages      | -0.00048126155 |
|    mean_reward_advantages    | 0.33883289     |
|    n_updates                 | 8710           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.5e-10        |
|    reward_explained_variance | -0.376         |
|    reward_value_loss         | 9.7            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.26          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 873           |
|    time_elapsed              | 170016        |
|    total_timesteps           | 111744        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0263        |
|    cost_value_loss           | 4.06e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.47          |
|    mean_cost_advantages      | 0.00026136707 |
|    mean_reward_advantages    | 0.30010343    |
|    n_updates                 | 8720          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.66e-10     |
|    reward_explained_variance | 0.00235       |
|    reward_value_loss         | 7.14          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.57          |
|    ep_rew_mean               | 5.53          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 874           |
|    time_elapsed              | 170125        |
|    total_timesteps           | 111872        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0233       |
|    cost_value_loss           | 3.34e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.88          |
|    mean_cost_advantages      | -0.0008977485 |
|    mean_reward_advantages    | 0.5097212     |
|    n_updates                 | 8730          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.03e-09      |
|    reward_explained_variance | -0.132        |
|    reward_value_loss         | 10.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.44          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 875           |
|    time_elapsed              | 170233        |
|    total_timesteps           | 112000        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00422      |
|    cost_value_loss           | 5.27e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.75          |
|    mean_cost_advantages      | 0.00013321746 |
|    mean_reward_advantages    | -0.88620967   |
|    n_updates                 | 8740          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.69e-09      |
|    reward_explained_variance | 0.346         |
|    reward_value_loss         | 4.29          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.62          |
|    ep_rew_mean               | 5.59          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 876           |
|    time_elapsed              | 170343        |
|    total_timesteps           | 112128        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0105        |
|    cost_value_loss           | 3.73e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.55          |
|    mean_cost_advantages      | -8.978071e-05 |
|    mean_reward_advantages    | -0.18806934   |
|    n_updates                 | 8750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.04e-10      |
|    reward_explained_variance | 0.478         |
|    reward_value_loss         | 3.43          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.4           |
|    ep_rew_mean               | 5.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 877           |
|    time_elapsed              | 170451        |
|    total_timesteps           | 112256        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0164        |
|    cost_value_loss           | 3.88e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.26          |
|    mean_cost_advantages      | 0.00022400534 |
|    mean_reward_advantages    | 0.96280587    |
|    n_updates                 | 8760          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.7e-10      |
|    reward_explained_variance | 0.18          |
|    reward_value_loss         | 4.47          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.06         |
|    ep_rew_mean               | 6.03         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 878          |
|    time_elapsed              | 170561       |
|    total_timesteps           | 112384       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0577       |
|    cost_value_loss           | 4.51e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.72e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.4          |
|    mean_cost_advantages      | -0.000708813 |
|    mean_reward_advantages    | 1.1067897    |
|    n_updates                 | 8770         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.56e-09    |
|    reward_explained_variance | -0.108       |
|    reward_value_loss         | 7.02         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.16           |
|    ep_rew_mean               | 6.13           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 879            |
|    time_elapsed              | 170674         |
|    total_timesteps           | 112512         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0183        |
|    cost_value_loss           | 4.08e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.52           |
|    mean_cost_advantages      | -0.00016843143 |
|    mean_reward_advantages    | 2.1251535      |
|    n_updates                 | 8780           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.27e-09       |
|    reward_explained_variance | -1.77          |
|    reward_value_loss         | 14.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.64          |
|    ep_rew_mean               | 6.6           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 880           |
|    time_elapsed              | 170785        |
|    total_timesteps           | 112640        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0144        |
|    cost_value_loss           | 5.42e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.46          |
|    mean_cost_advantages      | 0.00065002637 |
|    mean_reward_advantages    | -1.2645366    |
|    n_updates                 | 8790          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.45e-09      |
|    reward_explained_variance | -0.0584       |
|    reward_value_loss         | 7.62          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.59           |
|    ep_rew_mean               | 6.56           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 881            |
|    time_elapsed              | 170896         |
|    total_timesteps           | 112768         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0289         |
|    cost_value_loss           | 3.91e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.45           |
|    mean_cost_advantages      | -0.00043110576 |
|    mean_reward_advantages    | -0.07415223    |
|    n_updates                 | 8800           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.01e-10       |
|    reward_explained_variance | 0.00348        |
|    reward_value_loss         | 8.75           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.27          |
|    ep_rew_mean               | 6.24          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 882           |
|    time_elapsed              | 171005        |
|    total_timesteps           | 112896        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0705       |
|    cost_value_loss           | 3.76e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.25          |
|    mean_cost_advantages      | -4.646889e-05 |
|    mean_reward_advantages    | 0.11525777    |
|    n_updates                 | 8810          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.64e-10     |
|    reward_explained_variance | 0.419         |
|    reward_value_loss         | 4.08          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.71         |
|    ep_rew_mean               | 5.68         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 883          |
|    time_elapsed              | 171122       |
|    total_timesteps           | 113024       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0341      |
|    cost_value_loss           | 3.65e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.91e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.74         |
|    mean_cost_advantages      | 1.780562e-05 |
|    mean_reward_advantages    | -0.7161912   |
|    n_updates                 | 8820         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -6.38e-11    |
|    reward_explained_variance | 0.543        |
|    reward_value_loss         | 3.6          |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.72           |
|    ep_rew_mean               | 5.7            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 884            |
|    time_elapsed              | 171236         |
|    total_timesteps           | 113152         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00716       |
|    cost_value_loss           | 4.16e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.72           |
|    mean_cost_advantages      | -0.00025093227 |
|    mean_reward_advantages    | 0.34465632     |
|    n_updates                 | 8830           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.31e-09      |
|    reward_explained_variance | -0.685         |
|    reward_value_loss         | 5.27           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 6.52        |
|    ep_rew_mean               | 5.5         |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 885         |
|    time_elapsed              | 171350      |
|    total_timesteps           | 113280      |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | 0.0365      |
|    cost_value_loss           | 4.71e-06    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -5.63e-06   |
|    learning_rate             | 0.0005      |
|    loss                      | 4.22        |
|    mean_cost_advantages      | 0.000246272 |
|    mean_reward_advantages    | 0.022961915 |
|    n_updates                 | 8840        |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | -4.09e-10   |
|    reward_explained_variance | -0.843      |
|    reward_value_loss         | 8.63        |
|    total_cost                | 0.0         |
----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.83          |
|    ep_rew_mean               | 5.8           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 886           |
|    time_elapsed              | 171461        |
|    total_timesteps           | 113408        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.000471      |
|    cost_value_loss           | 4.67e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.13          |
|    mean_cost_advantages      | -0.0010341712 |
|    mean_reward_advantages    | 2.0534892     |
|    n_updates                 | 8850          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.48e-11     |
|    reward_explained_variance | -1.05         |
|    reward_value_loss         | 12.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.6          |
|    ep_rew_mean               | 5.58         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 887          |
|    time_elapsed              | 171573       |
|    total_timesteps           | 113536       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00563     |
|    cost_value_loss           | 4.11e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.99e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.17         |
|    mean_cost_advantages      | 0.0002077547 |
|    mean_reward_advantages    | -0.7829076   |
|    n_updates                 | 8860         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.93e-10    |
|    reward_explained_variance | 0.0421       |
|    reward_value_loss         | 4.37         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.4           |
|    ep_rew_mean               | 5.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 888           |
|    time_elapsed              | 171688        |
|    total_timesteps           | 113664        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.02          |
|    cost_value_loss           | 4.36e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.73          |
|    mean_cost_advantages      | 0.00024331099 |
|    mean_reward_advantages    | -0.46682668   |
|    n_updates                 | 8870          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.46e-10      |
|    reward_explained_variance | -0.298        |
|    reward_value_loss         | 5.82          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.88          |
|    ep_rew_mean               | 5.85          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 889           |
|    time_elapsed              | 171797        |
|    total_timesteps           | 113792        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0382       |
|    cost_value_loss           | 4.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.16          |
|    mean_cost_advantages      | 3.7883234e-05 |
|    mean_reward_advantages    | 0.36230126    |
|    n_updates                 | 8880          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.59e-09     |
|    reward_explained_variance | -0.134        |
|    reward_value_loss         | 5.39          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.49          |
|    ep_rew_mean               | 5.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 890           |
|    time_elapsed              | 171911        |
|    total_timesteps           | 113920        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0726        |
|    cost_value_loss           | 3.51e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.09e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.28          |
|    mean_cost_advantages      | 2.1376218e-05 |
|    mean_reward_advantages    | 1.8506942     |
|    n_updates                 | 8890          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.91e-10     |
|    reward_explained_variance | -0.0155       |
|    reward_value_loss         | 9.63          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.43           |
|    ep_rew_mean               | 5.4            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 891            |
|    time_elapsed              | 172022         |
|    total_timesteps           | 114048         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00356        |
|    cost_value_loss           | 3.6e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.06           |
|    mean_cost_advantages      | -2.5500507e-05 |
|    mean_reward_advantages    | -0.49367055    |
|    n_updates                 | 8900           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.18e-09       |
|    reward_explained_variance | -1.15          |
|    reward_value_loss         | 10.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.63           |
|    ep_rew_mean               | 5.6            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 892            |
|    time_elapsed              | 172135         |
|    total_timesteps           | 114176         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00422       |
|    cost_value_loss           | 3.8e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.25           |
|    mean_cost_advantages      | -0.00038729576 |
|    mean_reward_advantages    | -1.036376      |
|    n_updates                 | 8910           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.95e-09      |
|    reward_explained_variance | 0.448          |
|    reward_value_loss         | 5.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.71          |
|    ep_rew_mean               | 5.67          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 893           |
|    time_elapsed              | 172246        |
|    total_timesteps           | 114304        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.14          |
|    cost_value_loss           | 4.93e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.5           |
|    mean_cost_advantages      | -6.718268e-05 |
|    mean_reward_advantages    | 0.7230752     |
|    n_updates                 | 8920          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.91e-09      |
|    reward_explained_variance | -0.406        |
|    reward_value_loss         | 7.11          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.44          |
|    ep_rew_mean               | 6.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 894           |
|    time_elapsed              | 172359        |
|    total_timesteps           | 114432        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0256        |
|    cost_value_loss           | 3.21e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.44          |
|    mean_cost_advantages      | 0.00040072674 |
|    mean_reward_advantages    | 0.23207998    |
|    n_updates                 | 8930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.13e-10     |
|    reward_explained_variance | -1.12         |
|    reward_value_loss         | 10.6          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.45           |
|    ep_rew_mean               | 6.42           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 895            |
|    time_elapsed              | 172475         |
|    total_timesteps           | 114560         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00011       |
|    cost_value_loss           | 3.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.67e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.15           |
|    mean_cost_advantages      | -0.00018443068 |
|    mean_reward_advantages    | -0.4535011     |
|    n_updates                 | 8940           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.07e-09       |
|    reward_explained_variance | 0.157          |
|    reward_value_loss         | 6.18           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.4           |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 896           |
|    time_elapsed              | 172588        |
|    total_timesteps           | 114688        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0577        |
|    cost_value_loss           | 2.81e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.92e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.81          |
|    mean_cost_advantages      | 0.00016516936 |
|    mean_reward_advantages    | 1.0645473     |
|    n_updates                 | 8950          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.14e-09     |
|    reward_explained_variance | -0.472        |
|    reward_value_loss         | 8.33          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.52           |
|    ep_rew_mean               | 6.49           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 897            |
|    time_elapsed              | 172701         |
|    total_timesteps           | 114816         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0241         |
|    cost_value_loss           | 3.66e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.12           |
|    mean_cost_advantages      | -0.00013661393 |
|    mean_reward_advantages    | -0.11065598    |
|    n_updates                 | 8960           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.62e-09       |
|    reward_explained_variance | -0.248         |
|    reward_value_loss         | 11.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.37          |
|    ep_rew_mean               | 6.34          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 898           |
|    time_elapsed              | 172813        |
|    total_timesteps           | 114944        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0384        |
|    cost_value_loss           | 3.23e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.82          |
|    mean_cost_advantages      | 0.00046885875 |
|    mean_reward_advantages    | -0.70150477   |
|    n_updates                 | 8970          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.55e-09      |
|    reward_explained_variance | 0.341         |
|    reward_value_loss         | 4.62          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.79          |
|    ep_rew_mean               | 6.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 899           |
|    time_elapsed              | 172922        |
|    total_timesteps           | 115072        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0237        |
|    cost_value_loss           | 4.03e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.69e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.34          |
|    mean_cost_advantages      | 7.7005585e-05 |
|    mean_reward_advantages    | 0.2479618     |
|    n_updates                 | 8980          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.08e-10     |
|    reward_explained_variance | 0.0908        |
|    reward_value_loss         | 5.92          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.58           |
|    ep_rew_mean               | 6.55           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 900            |
|    time_elapsed              | 173031         |
|    total_timesteps           | 115200         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0231        |
|    cost_value_loss           | 3.17e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.15           |
|    mean_cost_advantages      | -0.00044037367 |
|    mean_reward_advantages    | -0.3249886     |
|    n_updates                 | 8990           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.74e-10      |
|    reward_explained_variance | 0.0693         |
|    reward_value_loss         | 5.79           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.61           |
|    ep_rew_mean               | 6.57           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 901            |
|    time_elapsed              | 173138         |
|    total_timesteps           | 115328         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0383         |
|    cost_value_loss           | 4.03e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.39           |
|    mean_cost_advantages      | -1.8126571e-05 |
|    mean_reward_advantages    | -0.005752586   |
|    n_updates                 | 9000           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.37e-09       |
|    reward_explained_variance | 0.323          |
|    reward_value_loss         | 4.74           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.49           |
|    ep_rew_mean               | 6.45           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 902            |
|    time_elapsed              | 173250         |
|    total_timesteps           | 115456         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.026         |
|    cost_value_loss           | 4.12e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.93e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.8            |
|    mean_cost_advantages      | -0.00040406318 |
|    mean_reward_advantages    | 0.5147196      |
|    n_updates                 | 9010           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.82e-10       |
|    reward_explained_variance | 0.146          |
|    reward_value_loss         | 7.49           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.69          |
|    ep_rew_mean               | 6.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 903           |
|    time_elapsed              | 173363        |
|    total_timesteps           | 115584        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00734      |
|    cost_value_loss           | 4.11e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.54          |
|    mean_cost_advantages      | 0.00016802533 |
|    mean_reward_advantages    | -1.1911359    |
|    n_updates                 | 9020          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.53e-09      |
|    reward_explained_variance | 0.413         |
|    reward_value_loss         | 5.54          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.08           |
|    ep_rew_mean               | 7.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 904            |
|    time_elapsed              | 173475         |
|    total_timesteps           | 115712         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0256        |
|    cost_value_loss           | 3.56e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.67           |
|    mean_cost_advantages      | -0.00011579624 |
|    mean_reward_advantages    | 1.3497338      |
|    n_updates                 | 9030           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.56e-10       |
|    reward_explained_variance | -0.566         |
|    reward_value_loss         | 9.81           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.74         |
|    ep_rew_mean               | 6.7          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 905          |
|    time_elapsed              | 173589       |
|    total_timesteps           | 115840       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00967      |
|    cost_value_loss           | 3.67e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.89e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.76         |
|    mean_cost_advantages      | 8.553856e-05 |
|    mean_reward_advantages    | -0.08127445  |
|    n_updates                 | 9040         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.05e-10    |
|    reward_explained_variance | 0.431        |
|    reward_value_loss         | 3.42         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.22          |
|    ep_rew_mean               | 6.18          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 906           |
|    time_elapsed              | 173704        |
|    total_timesteps           | 115968        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00612      |
|    cost_value_loss           | 3.76e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.54          |
|    mean_cost_advantages      | 0.00026168363 |
|    mean_reward_advantages    | 0.32075167    |
|    n_updates                 | 9050          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.09e-09      |
|    reward_explained_variance | -0.405        |
|    reward_value_loss         | 5.14          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.88          |
|    ep_rew_mean               | 5.86          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 907           |
|    time_elapsed              | 173816        |
|    total_timesteps           | 116096        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00213       |
|    cost_value_loss           | 4.39e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.4           |
|    mean_cost_advantages      | 3.9598468e-05 |
|    mean_reward_advantages    | -0.31051317   |
|    n_updates                 | 9060          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.48e-10     |
|    reward_explained_variance | 0.274         |
|    reward_value_loss         | 3.85          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.11           |
|    ep_rew_mean               | 6.08           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 908            |
|    time_elapsed              | 173935         |
|    total_timesteps           | 116224         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00602       |
|    cost_value_loss           | 3.31e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.01           |
|    mean_cost_advantages      | -0.00055014505 |
|    mean_reward_advantages    | 0.49348783     |
|    n_updates                 | 9070           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.69e-10       |
|    reward_explained_variance | 0.319          |
|    reward_value_loss         | 2.56           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.34          |
|    ep_rew_mean               | 6.31          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 909           |
|    time_elapsed              | 174051        |
|    total_timesteps           | 116352        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0309       |
|    cost_value_loss           | 2.66e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.03e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.71          |
|    mean_cost_advantages      | -0.0005815272 |
|    mean_reward_advantages    | 0.5958341     |
|    n_updates                 | 9080          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.97e-10     |
|    reward_explained_variance | 0.118         |
|    reward_value_loss         | 4.95          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.06           |
|    ep_rew_mean               | 6.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 910            |
|    time_elapsed              | 174165         |
|    total_timesteps           | 116480         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00192        |
|    cost_value_loss           | 3.49e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 3.9            |
|    mean_cost_advantages      | -0.00047382992 |
|    mean_reward_advantages    | 0.0943217      |
|    n_updates                 | 9090           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.39e-10      |
|    reward_explained_variance | -0.851         |
|    reward_value_loss         | 9.5            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.2           |
|    ep_rew_mean               | 6.18          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 911           |
|    time_elapsed              | 174284        |
|    total_timesteps           | 116608        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0387        |
|    cost_value_loss           | 3.74e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.74          |
|    mean_cost_advantages      | 0.00015796734 |
|    mean_reward_advantages    | -0.38966587   |
|    n_updates                 | 9100          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.44e-09     |
|    reward_explained_variance | 0.208         |
|    reward_value_loss         | 5.12          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.72          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 912           |
|    time_elapsed              | 174400        |
|    total_timesteps           | 116736        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0187       |
|    cost_value_loss           | 3.05e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.24          |
|    mean_cost_advantages      | -9.825451e-05 |
|    mean_reward_advantages    | -0.025379337  |
|    n_updates                 | 9110          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.27e-10     |
|    reward_explained_variance | 0.253         |
|    reward_value_loss         | 4.82          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.91          |
|    ep_rew_mean               | 6.88          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 913           |
|    time_elapsed              | 174517        |
|    total_timesteps           | 116864        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0224        |
|    cost_value_loss           | 2.89e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.94          |
|    mean_cost_advantages      | 0.00012501853 |
|    mean_reward_advantages    | 0.834887      |
|    n_updates                 | 9120          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.73e-09     |
|    reward_explained_variance | -0.295        |
|    reward_value_loss         | 9.31          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.16           |
|    ep_rew_mean               | 7.12           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 914            |
|    time_elapsed              | 174633         |
|    total_timesteps           | 116992         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0171         |
|    cost_value_loss           | 4.36e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.35           |
|    mean_cost_advantages      | -0.00020275882 |
|    mean_reward_advantages    | 0.25472903     |
|    n_updates                 | 9130           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.5e-09        |
|    reward_explained_variance | 0.371          |
|    reward_value_loss         | 4.6            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.67          |
|    ep_rew_mean               | 6.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 915           |
|    time_elapsed              | 174746        |
|    total_timesteps           | 117120        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0305       |
|    cost_value_loss           | 2.41e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.52          |
|    mean_cost_advantages      | 0.00018650413 |
|    mean_reward_advantages    | -0.043925077  |
|    n_updates                 | 9140          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.64e-09      |
|    reward_explained_variance | 0.21          |
|    reward_value_loss         | 5.79          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.64           |
|    ep_rew_mean               | 6.6            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 916            |
|    time_elapsed              | 174858         |
|    total_timesteps           | 117248         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00458       |
|    cost_value_loss           | 3.32e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.04           |
|    mean_cost_advantages      | -0.00032021507 |
|    mean_reward_advantages    | -0.6149341     |
|    n_updates                 | 9150           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.87e-11      |
|    reward_explained_variance | 0.312          |
|    reward_value_loss         | 5.12           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.75          |
|    ep_rew_mean               | 6.72          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 917           |
|    time_elapsed              | 174973        |
|    total_timesteps           | 117376        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00397      |
|    cost_value_loss           | 3.31e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.05          |
|    mean_cost_advantages      | 1.3191071e-05 |
|    mean_reward_advantages    | 0.49720436    |
|    n_updates                 | 9160          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.52e-09      |
|    reward_explained_variance | -0.0868       |
|    reward_value_loss         | 7.43          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.55          |
|    ep_rew_mean               | 6.51          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 918           |
|    time_elapsed              | 175083        |
|    total_timesteps           | 117504        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.057        |
|    cost_value_loss           | 3.56e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.78e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.84          |
|    mean_cost_advantages      | -0.0006833557 |
|    mean_reward_advantages    | 0.2604776     |
|    n_updates                 | 9170          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.2e-09      |
|    reward_explained_variance | -0.0559       |
|    reward_value_loss         | 4.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.69           |
|    ep_rew_mean               | 6.65           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 919            |
|    time_elapsed              | 175192         |
|    total_timesteps           | 117632         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00816        |
|    cost_value_loss           | 3.01e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.85e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.75           |
|    mean_cost_advantages      | -0.00014073023 |
|    mean_reward_advantages    | 0.40584323     |
|    n_updates                 | 9180           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.4e-10        |
|    reward_explained_variance | -0.862         |
|    reward_value_loss         | 9.26           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.26           |
|    ep_rew_mean               | 6.23           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 920            |
|    time_elapsed              | 175304         |
|    total_timesteps           | 117760         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0189        |
|    cost_value_loss           | 2.91e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.94e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.13           |
|    mean_cost_advantages      | -8.0641614e-05 |
|    mean_reward_advantages    | -0.21161366    |
|    n_updates                 | 9190           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.36e-10      |
|    reward_explained_variance | 0.101          |
|    reward_value_loss         | 6.72           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.46          |
|    ep_rew_mean               | 6.43          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 921           |
|    time_elapsed              | 175411        |
|    total_timesteps           | 117888        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0553        |
|    cost_value_loss           | 3.63e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.22          |
|    mean_cost_advantages      | 0.00052969647 |
|    mean_reward_advantages    | 0.381993      |
|    n_updates                 | 9200          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.15e-09      |
|    reward_explained_variance | 0.0942        |
|    reward_value_loss         | 9.14          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.57         |
|    ep_rew_mean               | 6.53         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 922          |
|    time_elapsed              | 175517       |
|    total_timesteps           | 118016       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0878      |
|    cost_value_loss           | 2.94e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 3            |
|    mean_cost_advantages      | 2.740405e-05 |
|    mean_reward_advantages    | 0.21220401   |
|    n_updates                 | 9210         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 8.87e-10     |
|    reward_explained_variance | 0.22         |
|    reward_value_loss         | 7.34         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.57           |
|    ep_rew_mean               | 6.53           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 923            |
|    time_elapsed              | 175625         |
|    total_timesteps           | 118144         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0253        |
|    cost_value_loss           | 2.89e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.03e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.08           |
|    mean_cost_advantages      | -1.2115839e-05 |
|    mean_reward_advantages    | 0.05780323     |
|    n_updates                 | 9220           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.56e-10      |
|    reward_explained_variance | 0.0127         |
|    reward_value_loss         | 7.38           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.55           |
|    ep_rew_mean               | 6.51           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 924            |
|    time_elapsed              | 175732         |
|    total_timesteps           | 118272         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0296         |
|    cost_value_loss           | 3.73e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.93e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.33           |
|    mean_cost_advantages      | -0.00016681779 |
|    mean_reward_advantages    | 0.83004296     |
|    n_updates                 | 9230           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.96e-09      |
|    reward_explained_variance | -0.195         |
|    reward_value_loss         | 6.89           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.31         |
|    ep_rew_mean               | 6.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 925          |
|    time_elapsed              | 175840       |
|    total_timesteps           | 118400       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0107      |
|    cost_value_loss           | 3.26e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.8e-06     |
|    learning_rate             | 0.0005       |
|    loss                      | 4.85         |
|    mean_cost_advantages      | 6.591097e-05 |
|    mean_reward_advantages    | 0.17641687   |
|    n_updates                 | 9240         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.13e-09     |
|    reward_explained_variance | -0.96        |
|    reward_value_loss         | 13.8         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.81           |
|    ep_rew_mean               | 6.77           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 926            |
|    time_elapsed              | 175947         |
|    total_timesteps           | 118528         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0155         |
|    cost_value_loss           | 3.03e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.78e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.31           |
|    mean_cost_advantages      | -0.00016135164 |
|    mean_reward_advantages    | 0.14918357     |
|    n_updates                 | 9250           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.59e-10      |
|    reward_explained_variance | 0.116          |
|    reward_value_loss         | 6.63           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.65           |
|    ep_rew_mean               | 6.61           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 927            |
|    time_elapsed              | 176053         |
|    total_timesteps           | 118656         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.031         |
|    cost_value_loss           | 3.44e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.09e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.4            |
|    mean_cost_advantages      | -0.00017277901 |
|    mean_reward_advantages    | 0.32270944     |
|    n_updates                 | 9260           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.55e-09       |
|    reward_explained_variance | 0.0472         |
|    reward_value_loss         | 8.32           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.63          |
|    ep_rew_mean               | 6.59          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 928           |
|    time_elapsed              | 176161        |
|    total_timesteps           | 118784        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0191       |
|    cost_value_loss           | 3.49e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.74e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.13          |
|    mean_cost_advantages      | 0.00022931155 |
|    mean_reward_advantages    | -0.29835933   |
|    n_updates                 | 9270          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.74e-10     |
|    reward_explained_variance | 0.492         |
|    reward_value_loss         | 4.98          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.16          |
|    ep_rew_mean               | 6.13          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 929           |
|    time_elapsed              | 176267        |
|    total_timesteps           | 118912        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.000191      |
|    cost_value_loss           | 7.9e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.52          |
|    mean_cost_advantages      | 0.00069213635 |
|    mean_reward_advantages    | 0.22424443    |
|    n_updates                 | 9280          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.17e-10     |
|    reward_explained_variance | 0.219         |
|    reward_value_loss         | 7.2           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.72         |
|    ep_rew_mean               | 5.68         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 930          |
|    time_elapsed              | 176373       |
|    total_timesteps           | 119040       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.106       |
|    cost_value_loss           | 8.67e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.87e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 0.876        |
|    mean_cost_advantages      | 0.0038241341 |
|    mean_reward_advantages    | -0.3763625   |
|    n_updates                 | 9290         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.17e-09    |
|    reward_explained_variance | 0.451        |
|    reward_value_loss         | 4.19         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.15           |
|    ep_rew_mean               | 5.13           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 931            |
|    time_elapsed              | 176478         |
|    total_timesteps           | 119168         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0261        |
|    cost_value_loss           | 3.87e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.81           |
|    mean_cost_advantages      | -0.00056521804 |
|    mean_reward_advantages    | 0.13017206     |
|    n_updates                 | 9300           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.8e-11       |
|    reward_explained_variance | 0.329          |
|    reward_value_loss         | 3.82           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.67           |
|    ep_rew_mean               | 5.64           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 932            |
|    time_elapsed              | 176582         |
|    total_timesteps           | 119296         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0215        |
|    cost_value_loss           | 4.3e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.22           |
|    mean_cost_advantages      | -0.00088289985 |
|    mean_reward_advantages    | -0.54595983    |
|    n_updates                 | 9310           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.86e-10      |
|    reward_explained_variance | 0.42           |
|    reward_value_loss         | 3.28           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.84           |
|    ep_rew_mean               | 5.8            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 933            |
|    time_elapsed              | 176689         |
|    total_timesteps           | 119424         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0297        |
|    cost_value_loss           | 2.46e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6e-06         |
|    learning_rate             | 0.0005         |
|    loss                      | 2.92           |
|    mean_cost_advantages      | -0.00034409203 |
|    mean_reward_advantages    | 1.1914959      |
|    n_updates                 | 9320           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.62e-09      |
|    reward_explained_variance | -0.258         |
|    reward_value_loss         | 7.03           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.14         |
|    ep_rew_mean               | 6.1          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 934          |
|    time_elapsed              | 176793       |
|    total_timesteps           | 119552       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00736     |
|    cost_value_loss           | 2.47e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.9e-06     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.88         |
|    mean_cost_advantages      | 8.398412e-05 |
|    mean_reward_advantages    | -0.18157068  |
|    n_updates                 | 9330         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.77e-09     |
|    reward_explained_variance | 0.151        |
|    reward_value_loss         | 6.38         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.29          |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 935           |
|    time_elapsed              | 176899        |
|    total_timesteps           | 119680        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00614       |
|    cost_value_loss           | 3.2e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.01e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.06          |
|    mean_cost_advantages      | -0.0001667554 |
|    mean_reward_advantages    | 0.7290939     |
|    n_updates                 | 9340          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.71e-09      |
|    reward_explained_variance | 0.195         |
|    reward_value_loss         | 6.29          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.49          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 936           |
|    time_elapsed              | 177006        |
|    total_timesteps           | 119808        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0299       |
|    cost_value_loss           | 4.36e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.94e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.74          |
|    mean_cost_advantages      | 0.00038627788 |
|    mean_reward_advantages    | -0.26742318   |
|    n_updates                 | 9350          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.22e-10      |
|    reward_explained_variance | 0.0717        |
|    reward_value_loss         | 4.48          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.54         |
|    ep_rew_mean               | 6.49         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 937          |
|    time_elapsed              | 177110       |
|    total_timesteps           | 119936       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0274      |
|    cost_value_loss           | 2.92e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.91e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.7          |
|    mean_cost_advantages      | -0.000890083 |
|    mean_reward_advantages    | 0.5033437    |
|    n_updates                 | 9360         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.73e-09     |
|    reward_explained_variance | 0.0679       |
|    reward_value_loss         | 6.48         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.96         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 938          |
|    time_elapsed              | 177219       |
|    total_timesteps           | 120064       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00807     |
|    cost_value_loss           | 3.69e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.96e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.08         |
|    mean_cost_advantages      | 6.932829e-05 |
|    mean_reward_advantages    | -0.06493236  |
|    n_updates                 | 9370         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.82e-10     |
|    reward_explained_variance | -0.114       |
|    reward_value_loss         | 3.8          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.34          |
|    ep_rew_mean               | 5.31          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 939           |
|    time_elapsed              | 177323        |
|    total_timesteps           | 120192        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0224       |
|    cost_value_loss           | 3.55e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.35          |
|    mean_cost_advantages      | -0.0001550896 |
|    mean_reward_advantages    | -0.2137553    |
|    n_updates                 | 9380          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.99e-10      |
|    reward_explained_variance | 0.194         |
|    reward_value_loss         | 6.34          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.47         |
|    ep_rew_mean               | 5.44         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 940          |
|    time_elapsed              | 177428       |
|    total_timesteps           | 120320       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0112       |
|    cost_value_loss           | 3.66e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.12         |
|    mean_cost_advantages      | 0.0007177107 |
|    mean_reward_advantages    | -0.56368196  |
|    n_updates                 | 9390         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.05e-09    |
|    reward_explained_variance | 0.531        |
|    reward_value_loss         | 2.85         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.86          |
|    ep_rew_mean               | 5.83          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 941           |
|    time_elapsed              | 177535        |
|    total_timesteps           | 120448        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0148       |
|    cost_value_loss           | 3.62e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.69          |
|    mean_cost_advantages      | 0.00015001775 |
|    mean_reward_advantages    | 0.64056736    |
|    n_updates                 | 9400          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.92e-09     |
|    reward_explained_variance | -0.0837       |
|    reward_value_loss         | 4.72          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.74           |
|    ep_rew_mean               | 5.72           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 942            |
|    time_elapsed              | 177641         |
|    total_timesteps           | 120576         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.000475       |
|    cost_value_loss           | 3.41e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.49           |
|    mean_cost_advantages      | -0.00011299166 |
|    mean_reward_advantages    | 1.2052248      |
|    n_updates                 | 9410           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.56e-10       |
|    reward_explained_variance | -0.922         |
|    reward_value_loss         | 8.76           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.53          |
|    ep_rew_mean               | 5.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 943           |
|    time_elapsed              | 177746        |
|    total_timesteps           | 120704        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00546       |
|    cost_value_loss           | 3.42e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.52          |
|    mean_cost_advantages      | 0.00055587187 |
|    mean_reward_advantages    | -0.06459659   |
|    n_updates                 | 9420          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.64e-09      |
|    reward_explained_variance | -0.101        |
|    reward_value_loss         | 5.8           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.64           |
|    ep_rew_mean               | 5.62           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 944            |
|    time_elapsed              | 177853         |
|    total_timesteps           | 120832         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0216         |
|    cost_value_loss           | 4.27e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.79e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.47           |
|    mean_cost_advantages      | -0.00059456634 |
|    mean_reward_advantages    | -0.29066977    |
|    n_updates                 | 9430           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.27e-09      |
|    reward_explained_variance | -0.0292        |
|    reward_value_loss         | 6.85           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.79          |
|    ep_rew_mean               | 5.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 945           |
|    time_elapsed              | 177959        |
|    total_timesteps           | 120960        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00936      |
|    cost_value_loss           | 3.76e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.78e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.48          |
|    mean_cost_advantages      | 5.4580687e-05 |
|    mean_reward_advantages    | -0.16026124   |
|    n_updates                 | 9440          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.82e-09     |
|    reward_explained_variance | 0.227         |
|    reward_value_loss         | 3.86          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.95          |
|    ep_rew_mean               | 5.92          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 946           |
|    time_elapsed              | 178067        |
|    total_timesteps           | 121088        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00886       |
|    cost_value_loss           | 2.94e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.05e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.31          |
|    mean_cost_advantages      | -0.0003145979 |
|    mean_reward_advantages    | 0.47673836    |
|    n_updates                 | 9450          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.78e-10     |
|    reward_explained_variance | 0.382         |
|    reward_value_loss         | 4.93          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.17           |
|    ep_rew_mean               | 6.14           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 947            |
|    time_elapsed              | 178173         |
|    total_timesteps           | 121216         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0284         |
|    cost_value_loss           | 2.92e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.52           |
|    mean_cost_advantages      | -0.00013896333 |
|    mean_reward_advantages    | 0.579697       |
|    n_updates                 | 9460           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.19e-10      |
|    reward_explained_variance | 0.0107         |
|    reward_value_loss         | 6.24           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 948           |
|    time_elapsed              | 178281        |
|    total_timesteps           | 121344        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0058       |
|    cost_value_loss           | 2.9e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.16          |
|    mean_cost_advantages      | -7.709119e-05 |
|    mean_reward_advantages    | 1.6366354     |
|    n_updates                 | 9470          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.02e-10     |
|    reward_explained_variance | -0.977        |
|    reward_value_loss         | 12.3          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 949           |
|    time_elapsed              | 178385        |
|    total_timesteps           | 121472        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0244       |
|    cost_value_loss           | 3.11e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.42          |
|    mean_cost_advantages      | 0.00028748537 |
|    mean_reward_advantages    | -0.807639     |
|    n_updates                 | 9480          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.17e-10     |
|    reward_explained_variance | -0.0739       |
|    reward_value_loss         | 7.76          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.61           |
|    ep_rew_mean               | 6.57           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 950            |
|    time_elapsed              | 178493         |
|    total_timesteps           | 121600         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0392         |
|    cost_value_loss           | 3.16e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.98           |
|    mean_cost_advantages      | -0.00021459146 |
|    mean_reward_advantages    | -0.5386901     |
|    n_updates                 | 9490           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.5e-09        |
|    reward_explained_variance | -0.00239       |
|    reward_value_loss         | 6.97           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.22           |
|    ep_rew_mean               | 6.19           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 951            |
|    time_elapsed              | 178599         |
|    total_timesteps           | 121728         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00988        |
|    cost_value_loss           | 3.4e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.17           |
|    mean_cost_advantages      | -0.00041437996 |
|    mean_reward_advantages    | -0.7667192     |
|    n_updates                 | 9500           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.86e-09      |
|    reward_explained_variance | 0.605          |
|    reward_value_loss         | 2.86           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.73          |
|    ep_rew_mean               | 5.71          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 952           |
|    time_elapsed              | 178703        |
|    total_timesteps           | 121856        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00824      |
|    cost_value_loss           | 3.94e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.27          |
|    mean_cost_advantages      | -9.511264e-05 |
|    mean_reward_advantages    | 0.4722188     |
|    n_updates                 | 9510          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.5e-09      |
|    reward_explained_variance | 0.341         |
|    reward_value_loss         | 3.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.4           |
|    ep_rew_mean               | 5.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 953           |
|    time_elapsed              | 178809        |
|    total_timesteps           | 121984        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0141        |
|    cost_value_loss           | 3.39e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.03          |
|    mean_cost_advantages      | 0.00014297382 |
|    mean_reward_advantages    | 0.7412499     |
|    n_updates                 | 9520          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.98e-09      |
|    reward_explained_variance | -0.0549       |
|    reward_value_loss         | 7.22          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.56          |
|    ep_rew_mean               | 5.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 954           |
|    time_elapsed              | 178913        |
|    total_timesteps           | 122112        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00691      |
|    cost_value_loss           | 3.5e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.37          |
|    mean_cost_advantages      | 0.00034302895 |
|    mean_reward_advantages    | -0.7447355    |
|    n_updates                 | 9530          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.23e-09     |
|    reward_explained_variance | 0.472         |
|    reward_value_loss         | 4.57          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.8            |
|    ep_rew_mean               | 5.79           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 955            |
|    time_elapsed              | 179019         |
|    total_timesteps           | 122240         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0664        |
|    cost_value_loss           | 3.08e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.71e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.54           |
|    mean_cost_advantages      | -0.00055723713 |
|    mean_reward_advantages    | -0.3838045     |
|    n_updates                 | 9540           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.29e-09       |
|    reward_explained_variance | 0.499          |
|    reward_value_loss         | 2.86           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.15          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 956           |
|    time_elapsed              | 179122        |
|    total_timesteps           | 122368        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0368       |
|    cost_value_loss           | 2.84e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.7           |
|    mean_cost_advantages      | -0.0007377393 |
|    mean_reward_advantages    | 0.6409458     |
|    n_updates                 | 9550          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.57e-10      |
|    reward_explained_variance | -0.233        |
|    reward_value_loss         | 5.04          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.16           |
|    ep_rew_mean               | 6.15           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 957            |
|    time_elapsed              | 179226         |
|    total_timesteps           | 122496         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.122         |
|    cost_value_loss           | 3.83e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.69           |
|    mean_cost_advantages      | -0.00062696176 |
|    mean_reward_advantages    | -0.5074543     |
|    n_updates                 | 9560           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.64e-09       |
|    reward_explained_variance | 0.257          |
|    reward_value_loss         | 4.31           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.3          |
|    ep_rew_mean               | 6.28         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 958          |
|    time_elapsed              | 179329       |
|    total_timesteps           | 122624       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00171      |
|    cost_value_loss           | 3.23e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.92e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.18         |
|    mean_cost_advantages      | 4.594036e-05 |
|    mean_reward_advantages    | 0.14384732   |
|    n_updates                 | 9570         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 4.58e-10     |
|    reward_explained_variance | 0.374        |
|    reward_value_loss         | 4.62         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.31           |
|    ep_rew_mean               | 6.29           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 959            |
|    time_elapsed              | 179439         |
|    total_timesteps           | 122752         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0253         |
|    cost_value_loss           | 2.84e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.87e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.31           |
|    mean_cost_advantages      | -6.7169465e-05 |
|    mean_reward_advantages    | 1.1614858      |
|    n_updates                 | 9580           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.16e-09      |
|    reward_explained_variance | -0.478         |
|    reward_value_loss         | 8.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 960           |
|    time_elapsed              | 179556        |
|    total_timesteps           | 122880        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00497       |
|    cost_value_loss           | 3.18e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.18          |
|    mean_cost_advantages      | 0.00016300732 |
|    mean_reward_advantages    | -0.28275287   |
|    n_updates                 | 9590          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.49e-11     |
|    reward_explained_variance | -0.153        |
|    reward_value_loss         | 5.47          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.01           |
|    ep_rew_mean               | 5.98           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 961            |
|    time_elapsed              | 179666         |
|    total_timesteps           | 123008         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0534        |
|    cost_value_loss           | 3.63e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 0.754          |
|    mean_cost_advantages      | -0.00077909534 |
|    mean_reward_advantages    | 0.0012879446   |
|    n_updates                 | 9600           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 7.54e-10       |
|    reward_explained_variance | 0.531          |
|    reward_value_loss         | 2.69           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.11          |
|    ep_rew_mean               | 6.08          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 962           |
|    time_elapsed              | 179774        |
|    total_timesteps           | 123136        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0154       |
|    cost_value_loss           | 3.62e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.78e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.87          |
|    mean_cost_advantages      | -0.0003905389 |
|    mean_reward_advantages    | 0.17298645    |
|    n_updates                 | 9610          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.28e-09     |
|    reward_explained_variance | -0.118        |
|    reward_value_loss         | 5.72          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.86         |
|    ep_rew_mean               | 5.82         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 963          |
|    time_elapsed              | 179880       |
|    total_timesteps           | 123264       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0178       |
|    cost_value_loss           | 3.05e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.5          |
|    mean_cost_advantages      | 0.0002672862 |
|    mean_reward_advantages    | -0.56114197  |
|    n_updates                 | 9620         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.76e-10    |
|    reward_explained_variance | 0.139        |
|    reward_value_loss         | 5.11         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.71         |
|    ep_rew_mean               | 5.68         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 964          |
|    time_elapsed              | 179984       |
|    total_timesteps           | 123392       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00801      |
|    cost_value_loss           | 3.52e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -6.03e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.36         |
|    mean_cost_advantages      | 0.0004022122 |
|    mean_reward_advantages    | -0.23146152  |
|    n_updates                 | 9630         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.82e-10     |
|    reward_explained_variance | 0.244        |
|    reward_value_loss         | 3.91         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.69          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 965           |
|    time_elapsed              | 180108        |
|    total_timesteps           | 123520        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0284       |
|    cost_value_loss           | 3.75e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.01          |
|    mean_cost_advantages      | -0.0001396854 |
|    mean_reward_advantages    | 0.64748913    |
|    n_updates                 | 9640          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.14e-09     |
|    reward_explained_variance | 0.386         |
|    reward_value_loss         | 4.14          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.38          |
|    ep_rew_mean               | 5.36          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 966           |
|    time_elapsed              | 180228        |
|    total_timesteps           | 123648        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0267       |
|    cost_value_loss           | 2.94e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.4           |
|    mean_cost_advantages      | 0.00020839239 |
|    mean_reward_advantages    | -0.52906656   |
|    n_updates                 | 9650          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.05e-09      |
|    reward_explained_variance | 0.409         |
|    reward_value_loss         | 3.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.4           |
|    ep_rew_mean               | 5.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 967           |
|    time_elapsed              | 180350        |
|    total_timesteps           | 123776        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.014         |
|    cost_value_loss           | 3.18e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.93e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.691         |
|    mean_cost_advantages      | 0.00023615653 |
|    mean_reward_advantages    | 0.2626581     |
|    n_updates                 | 9660          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.36e-09      |
|    reward_explained_variance | 0.605         |
|    reward_value_loss         | 2.34          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.65           |
|    ep_rew_mean               | 5.63           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 968            |
|    time_elapsed              | 180460         |
|    total_timesteps           | 123904         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0292        |
|    cost_value_loss           | 3.23e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.01e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.09           |
|    mean_cost_advantages      | -0.00021528642 |
|    mean_reward_advantages    | 0.10062256     |
|    n_updates                 | 9670           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.05e-09       |
|    reward_explained_variance | 0.398          |
|    reward_value_loss         | 2.84           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.69           |
|    ep_rew_mean               | 5.66           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 969            |
|    time_elapsed              | 180562         |
|    total_timesteps           | 124032         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0568        |
|    cost_value_loss           | 4.44e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.02           |
|    mean_cost_advantages      | -0.00016716303 |
|    mean_reward_advantages    | 0.83705485     |
|    n_updates                 | 9680           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.17e-10       |
|    reward_explained_variance | 0.0179         |
|    reward_value_loss         | 5.27           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.57           |
|    ep_rew_mean               | 5.54           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 970            |
|    time_elapsed              | 180667         |
|    total_timesteps           | 124160         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00969        |
|    cost_value_loss           | 4.37e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.13e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.1            |
|    mean_cost_advantages      | -0.00015168343 |
|    mean_reward_advantages    | 0.64517677     |
|    n_updates                 | 9690           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.72e-10      |
|    reward_explained_variance | -0.195         |
|    reward_value_loss         | 7.22           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.5           |
|    ep_rew_mean               | 5.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 971           |
|    time_elapsed              | 180770        |
|    total_timesteps           | 124288        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.06         |
|    cost_value_loss           | 3.71e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.47          |
|    mean_cost_advantages      | 0.00020196575 |
|    mean_reward_advantages    | -0.92023456   |
|    n_updates                 | 9700          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.03e-11      |
|    reward_explained_variance | 0.2           |
|    reward_value_loss         | 4.95          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.29          |
|    ep_rew_mean               | 5.26          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 972           |
|    time_elapsed              | 180873        |
|    total_timesteps           | 124416        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0121       |
|    cost_value_loss           | 4.76e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.94e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.4           |
|    mean_cost_advantages      | 0.00072342827 |
|    mean_reward_advantages    | -0.9028633    |
|    n_updates                 | 9710          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.13e-09      |
|    reward_explained_variance | 0.518         |
|    reward_value_loss         | 3.96          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.35          |
|    ep_rew_mean               | 5.31          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 973           |
|    time_elapsed              | 180976        |
|    total_timesteps           | 124544        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0119       |
|    cost_value_loss           | 3.21e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.07e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.833         |
|    mean_cost_advantages      | 0.00033990166 |
|    mean_reward_advantages    | 0.66389555    |
|    n_updates                 | 9720          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.32e-09      |
|    reward_explained_variance | 0.497         |
|    reward_value_loss         | 2.74          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.17           |
|    ep_rew_mean               | 5.14           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 974            |
|    time_elapsed              | 181052         |
|    total_timesteps           | 124672         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0102        |
|    cost_value_loss           | 2.84e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.92           |
|    mean_cost_advantages      | -0.00025217963 |
|    mean_reward_advantages    | 0.72587115     |
|    n_updates                 | 9730           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.86e-10       |
|    reward_explained_variance | 0.146          |
|    reward_value_loss         | 4.93           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.47          |
|    ep_rew_mean               | 5.43          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 975           |
|    time_elapsed              | 181121        |
|    total_timesteps           | 124800        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0237        |
|    cost_value_loss           | 2.97e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.55          |
|    mean_cost_advantages      | 0.00013973299 |
|    mean_reward_advantages    | -0.15442571   |
|    n_updates                 | 9740          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.5e-09      |
|    reward_explained_variance | 0.103         |
|    reward_value_loss         | 5.06          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 976           |
|    time_elapsed              | 181188        |
|    total_timesteps           | 124928        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00972      |
|    cost_value_loss           | 3.46e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.95e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.82          |
|    mean_cost_advantages      | 5.7662215e-05 |
|    mean_reward_advantages    | 0.2923711     |
|    n_updates                 | 9750          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.41e-09      |
|    reward_explained_variance | -0.0819       |
|    reward_value_loss         | 6.08          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.5           |
|    ep_rew_mean               | 6.46          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 977           |
|    time_elapsed              | 181254        |
|    total_timesteps           | 125056        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0499       |
|    cost_value_loss           | 3.06e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.94e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.61          |
|    mean_cost_advantages      | 0.00027934642 |
|    mean_reward_advantages    | 0.21455455    |
|    n_updates                 | 9760          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.09e-09     |
|    reward_explained_variance | -0.239        |
|    reward_value_loss         | 8.57          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.58           |
|    ep_rew_mean               | 6.54           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 978            |
|    time_elapsed              | 181322         |
|    total_timesteps           | 125184         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0208         |
|    cost_value_loss           | 2.59e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.92e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.73           |
|    mean_cost_advantages      | -0.00036815729 |
|    mean_reward_advantages    | 0.20022866     |
|    n_updates                 | 9770           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 9.07e-10       |
|    reward_explained_variance | 0.0611         |
|    reward_value_loss         | 7.44           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.53          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 979           |
|    time_elapsed              | 181388        |
|    total_timesteps           | 125312        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0021       |
|    cost_value_loss           | 3.43e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.82e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.47          |
|    mean_cost_advantages      | 0.00023041942 |
|    mean_reward_advantages    | -0.17861024   |
|    n_updates                 | 9780          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.6e-10       |
|    reward_explained_variance | 0.443         |
|    reward_value_loss         | 5.81          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.13           |
|    ep_rew_mean               | 7.09           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 980            |
|    time_elapsed              | 181455         |
|    total_timesteps           | 125440         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0167        |
|    cost_value_loss           | 2.63e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.71e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.39           |
|    mean_cost_advantages      | -0.00038424207 |
|    mean_reward_advantages    | 0.91167563     |
|    n_updates                 | 9790           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.2e-09        |
|    reward_explained_variance | 0.205          |
|    reward_value_loss         | 5.57           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.39         |
|    ep_rew_mean               | 7.35         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 981          |
|    time_elapsed              | 181522       |
|    total_timesteps           | 125568       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0266       |
|    cost_value_loss           | 1.66e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -6.01e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.01         |
|    mean_cost_advantages      | 9.565367e-05 |
|    mean_reward_advantages    | 1.074681     |
|    n_updates                 | 9800         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.43e-09     |
|    reward_explained_variance | -0.185       |
|    reward_value_loss         | 11.6         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.31          |
|    ep_rew_mean               | 7.27          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 982           |
|    time_elapsed              | 181589        |
|    total_timesteps           | 125696        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00403       |
|    cost_value_loss           | 3.1e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.39          |
|    mean_cost_advantages      | 0.00021194253 |
|    mean_reward_advantages    | -1.0415977    |
|    n_updates                 | 9810          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.38e-09     |
|    reward_explained_variance | 0.112         |
|    reward_value_loss         | 6.92          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.16           |
|    ep_rew_mean               | 7.13           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 983            |
|    time_elapsed              | 181658         |
|    total_timesteps           | 125824         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00103       |
|    cost_value_loss           | 2.25e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.9e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 4.13           |
|    mean_cost_advantages      | -0.00010396533 |
|    mean_reward_advantages    | 0.61945987     |
|    n_updates                 | 9820           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.94e-11       |
|    reward_explained_variance | -0.0729        |
|    reward_value_loss         | 9.53           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.41          |
|    ep_rew_mean               | 7.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 984           |
|    time_elapsed              | 181726        |
|    total_timesteps           | 125952        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0119        |
|    cost_value_loss           | 3.04e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 7.34          |
|    mean_cost_advantages      | 0.00020942408 |
|    mean_reward_advantages    | -0.13967712   |
|    n_updates                 | 9830          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.27e-10      |
|    reward_explained_variance | -0.255        |
|    reward_value_loss         | 13.7          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.27         |
|    ep_rew_mean               | 7.24         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 985          |
|    time_elapsed              | 181794       |
|    total_timesteps           | 126080       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0363      |
|    cost_value_loss           | 3.5e-06      |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.7e-06     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.47         |
|    mean_cost_advantages      | 7.549269e-05 |
|    mean_reward_advantages    | -0.75983006  |
|    n_updates                 | 9840         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.53e-10    |
|    reward_explained_variance | 0.289        |
|    reward_value_loss         | 5.61         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.49           |
|    ep_rew_mean               | 6.46           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 986            |
|    time_elapsed              | 181863         |
|    total_timesteps           | 126208         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00945       |
|    cost_value_loss           | 3.32e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.35           |
|    mean_cost_advantages      | -0.00011881966 |
|    mean_reward_advantages    | -0.13389142    |
|    n_updates                 | 9850           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.2e-10       |
|    reward_explained_variance | 0.449          |
|    reward_value_loss         | 3.93           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.76           |
|    ep_rew_mean               | 6.73           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 987            |
|    time_elapsed              | 181931         |
|    total_timesteps           | 126336         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0212         |
|    cost_value_loss           | 3.64e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 0.92           |
|    mean_cost_advantages      | -0.00054786046 |
|    mean_reward_advantages    | 0.20343874     |
|    n_updates                 | 9860           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.69e-09      |
|    reward_explained_variance | 0.518          |
|    reward_value_loss         | 2.89           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.03           |
|    ep_rew_mean               | 6.01           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 988            |
|    time_elapsed              | 182000         |
|    total_timesteps           | 126464         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0097         |
|    cost_value_loss           | 3.99e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.94           |
|    mean_cost_advantages      | -0.00039299182 |
|    mean_reward_advantages    | 1.1154906      |
|    n_updates                 | 9870           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.5e-10       |
|    reward_explained_variance | -0.11          |
|    reward_value_loss         | 8.29           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.16          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 989           |
|    time_elapsed              | 182067        |
|    total_timesteps           | 126592        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0138       |
|    cost_value_loss           | 4.41e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.95e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.07          |
|    mean_cost_advantages      | 2.1767231e-05 |
|    mean_reward_advantages    | -1.7277882    |
|    n_updates                 | 9880          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.37e-09     |
|    reward_explained_variance | 0.595         |
|    reward_value_loss         | 3.69          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.13           |
|    ep_rew_mean               | 6.11           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 990            |
|    time_elapsed              | 182136         |
|    total_timesteps           | 126720         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0458         |
|    cost_value_loss           | 2.94e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.19           |
|    mean_cost_advantages      | -0.00057803816 |
|    mean_reward_advantages    | 1.221468       |
|    n_updates                 | 9890           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.89e-10      |
|    reward_explained_variance | 0.164          |
|    reward_value_loss         | 4.93           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.73         |
|    ep_rew_mean               | 5.7          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 991          |
|    time_elapsed              | 182204       |
|    total_timesteps           | 126848       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0221      |
|    cost_value_loss           | 3.06e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.05         |
|    mean_cost_advantages      | 0.0002621362 |
|    mean_reward_advantages    | -0.07734525  |
|    n_updates                 | 9900         |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.05e-09    |
|    reward_explained_variance | 0.631        |
|    reward_value_loss         | 2.57         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.66          |
|    ep_rew_mean               | 5.63          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 992           |
|    time_elapsed              | 182272        |
|    total_timesteps           | 126976        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00226      |
|    cost_value_loss           | 3.33e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.801         |
|    mean_cost_advantages      | 0.00014944829 |
|    mean_reward_advantages    | -0.36046916   |
|    n_updates                 | 9910          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.92e-10      |
|    reward_explained_variance | 0.783         |
|    reward_value_loss         | 1.65          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 993           |
|    time_elapsed              | 182340        |
|    total_timesteps           | 127104        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0867       |
|    cost_value_loss           | 3.04e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.65          |
|    mean_cost_advantages      | -0.0009120285 |
|    mean_reward_advantages    | 1.1346573     |
|    n_updates                 | 9920          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.54e-09     |
|    reward_explained_variance | 0.12          |
|    reward_value_loss         | 5.52          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.31          |
|    ep_rew_mean               | 6.28          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 994           |
|    time_elapsed              | 182407        |
|    total_timesteps           | 127232        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0199       |
|    cost_value_loss           | 3.5e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.1           |
|    mean_cost_advantages      | -0.0005763781 |
|    mean_reward_advantages    | 0.48623264    |
|    n_updates                 | 9930          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.82e-09     |
|    reward_explained_variance | 0.0944        |
|    reward_value_loss         | 6.76          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.08          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 995           |
|    time_elapsed              | 182481        |
|    total_timesteps           | 127360        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0193       |
|    cost_value_loss           | 3.92e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.92e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.84          |
|    mean_cost_advantages      | -0.0008223066 |
|    mean_reward_advantages    | -1.0711062    |
|    n_updates                 | 9940          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.3e-09      |
|    reward_explained_variance | 0.271         |
|    reward_value_loss         | 4.51          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.25           |
|    ep_rew_mean               | 6.21           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 996            |
|    time_elapsed              | 182550         |
|    total_timesteps           | 127488         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0456        |
|    cost_value_loss           | 2.9e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.95e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.26           |
|    mean_cost_advantages      | -0.00053994765 |
|    mean_reward_advantages    | 0.4113183      |
|    n_updates                 | 9950           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.14e-09       |
|    reward_explained_variance | 0.0906         |
|    reward_value_loss         | 4.59           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.57           |
|    ep_rew_mean               | 6.53           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 997            |
|    time_elapsed              | 182620         |
|    total_timesteps           | 127616         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0242        |
|    cost_value_loss           | 2.96e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.98e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.43           |
|    mean_cost_advantages      | -8.2329774e-05 |
|    mean_reward_advantages    | 0.16636825     |
|    n_updates                 | 9960           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.49e-10      |
|    reward_explained_variance | -0.156         |
|    reward_value_loss         | 6.47           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.77           |
|    ep_rew_mean               | 5.74           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 998            |
|    time_elapsed              | 182688         |
|    total_timesteps           | 127744         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0138        |
|    cost_value_loss           | 3.02e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.5            |
|    mean_cost_advantages      | -0.00013211922 |
|    mean_reward_advantages    | 0.65228695     |
|    n_updates                 | 9970           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.32e-09      |
|    reward_explained_variance | -0.165         |
|    reward_value_loss         | 7.11           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.71          |
|    ep_rew_mean               | 5.68          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 999           |
|    time_elapsed              | 182755        |
|    total_timesteps           | 127872        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00465       |
|    cost_value_loss           | 3.68e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.02          |
|    mean_cost_advantages      | 0.00024331905 |
|    mean_reward_advantages    | -0.8106885    |
|    n_updates                 | 9980          |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.12e-09      |
|    reward_explained_variance | 0.564         |
|    reward_value_loss         | 3.14          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.48           |
|    ep_rew_mean               | 5.46           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1000           |
|    time_elapsed              | 182822         |
|    total_timesteps           | 128000         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0265        |
|    cost_value_loss           | 2.55e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.7e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.97           |
|    mean_cost_advantages      | -0.00033975643 |
|    mean_reward_advantages    | 0.86603993     |
|    n_updates                 | 9990           |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -8.19e-10      |
|    reward_explained_variance | 0.302          |
|    reward_value_loss         | 5.7            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.62          |
|    ep_rew_mean               | 5.6           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1001          |
|    time_elapsed              | 182889        |
|    total_timesteps           | 128128        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0197       |
|    cost_value_loss           | 3.79e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.67          |
|    mean_cost_advantages      | 0.00025291162 |
|    mean_reward_advantages    | -0.15304187   |
|    n_updates                 | 10000         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.59e-10      |
|    reward_explained_variance | 0.112         |
|    reward_value_loss         | 4.23          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.89           |
|    ep_rew_mean               | 5.87           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1002           |
|    time_elapsed              | 182957         |
|    total_timesteps           | 128256         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0437        |
|    cost_value_loss           | 3.95e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.71e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.47           |
|    mean_cost_advantages      | -0.00038912034 |
|    mean_reward_advantages    | 0.70640075     |
|    n_updates                 | 10010          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.12e-10      |
|    reward_explained_variance | -0.266         |
|    reward_value_loss         | 5.78           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.68          |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1003          |
|    time_elapsed              | 183024        |
|    total_timesteps           | 128384        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.031        |
|    cost_value_loss           | 2.72e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.65          |
|    mean_cost_advantages      | -0.0005779096 |
|    mean_reward_advantages    | 0.5128218     |
|    n_updates                 | 10020         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.59e-09     |
|    reward_explained_variance | 0.337         |
|    reward_value_loss         | 4.54          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.02         |
|    ep_rew_mean               | 6.01         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1004         |
|    time_elapsed              | 183092       |
|    total_timesteps           | 128512       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0126      |
|    cost_value_loss           | 3.39e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -6.01e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.27         |
|    mean_cost_advantages      | 0.0002544435 |
|    mean_reward_advantages    | -0.460517    |
|    n_updates                 | 10030        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.93e-09     |
|    reward_explained_variance | 0.634        |
|    reward_value_loss         | 2.44         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.34           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1005           |
|    time_elapsed              | 183170         |
|    total_timesteps           | 128640         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 7.0198334e-06  |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0611         |
|    cost_value_loss           | 2.86e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.77e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 8.2            |
|    mean_cost_advantages      | -0.00016163301 |
|    mean_reward_advantages    | 3.2032099      |
|    n_updates                 | 10040          |
|    nu                        | 1.05           |
|    nu_loss                   | -7.35e-06      |
|    policy_gradient_loss      | -2.5e-09       |
|    reward_explained_variance | -2.77          |
|    reward_value_loss         | 23             |
|    total_cost                | 0.0008985387   |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.13           |
|    ep_rew_mean               | 6.1            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1006           |
|    time_elapsed              | 183247         |
|    total_timesteps           | 128768         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0207         |
|    cost_value_loss           | 2.48e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.98e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 7.56           |
|    mean_cost_advantages      | -0.00038758008 |
|    mean_reward_advantages    | -1.3924533     |
|    n_updates                 | 10050          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.49e-09       |
|    reward_explained_variance | -0.328         |
|    reward_value_loss         | 15.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.41          |
|    ep_rew_mean               | 6.38          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1007          |
|    time_elapsed              | 183324        |
|    total_timesteps           | 128896        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00859      |
|    cost_value_loss           | 3.26e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.43          |
|    mean_cost_advantages      | 0.00013020943 |
|    mean_reward_advantages    | -2.455278     |
|    n_updates                 | 10060         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.12e-09      |
|    reward_explained_variance | 0.305         |
|    reward_value_loss         | 7.87          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.83           |
|    ep_rew_mean               | 6.79           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1008           |
|    time_elapsed              | 183401         |
|    total_timesteps           | 129024         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.023         |
|    cost_value_loss           | 2.46e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.95e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.14           |
|    mean_cost_advantages      | -0.00026717258 |
|    mean_reward_advantages    | 1.0941632      |
|    n_updates                 | 10070          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.86e-09       |
|    reward_explained_variance | -0.76          |
|    reward_value_loss         | 11.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.26         |
|    ep_rew_mean               | 7.22         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1009         |
|    time_elapsed              | 183480       |
|    total_timesteps           | 129152       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0316      |
|    cost_value_loss           | 3.18e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.8e-06     |
|    learning_rate             | 0.0005       |
|    loss                      | 2.25         |
|    mean_cost_advantages      | 8.061502e-05 |
|    mean_reward_advantages    | -0.45899132  |
|    n_updates                 | 10080        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.94e-10     |
|    reward_explained_variance | 0.357        |
|    reward_value_loss         | 6.04         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.68           |
|    ep_rew_mean               | 7.63           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1010           |
|    time_elapsed              | 183557         |
|    total_timesteps           | 129280         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0294         |
|    cost_value_loss           | 1.87e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.97e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.68           |
|    mean_cost_advantages      | -8.1819126e-05 |
|    mean_reward_advantages    | 1.3796322      |
|    n_updates                 | 10090          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.14e-09       |
|    reward_explained_variance | -0.51          |
|    reward_value_loss         | 11             |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.69          |
|    ep_rew_mean               | 7.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1011          |
|    time_elapsed              | 183633        |
|    total_timesteps           | 129408        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0185        |
|    cost_value_loss           | 2.72e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.75e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.16          |
|    mean_cost_advantages      | 0.00017508186 |
|    mean_reward_advantages    | 1.2826889     |
|    n_updates                 | 10100         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.41e-10     |
|    reward_explained_variance | -0.25         |
|    reward_value_loss         | 10.1          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.42          |
|    ep_rew_mean               | 7.39          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1012          |
|    time_elapsed              | 183710        |
|    total_timesteps           | 129536        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 1.7213194e-06 |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.056         |
|    cost_value_loss           | 2.66e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 10.6          |
|    mean_cost_advantages      | 0.00035295996 |
|    mean_reward_advantages    | 2.6166077     |
|    n_updates                 | 10110         |
|    nu                        | 1.05          |
|    nu_loss                   | -1.8e-06      |
|    policy_gradient_loss      | 1e-09         |
|    reward_explained_variance | -1.57         |
|    reward_value_loss         | 25.5          |
|    total_cost                | 0.00022032889 |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 8.77          |
|    ep_rew_mean               | 7.73          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1013          |
|    time_elapsed              | 183789        |
|    total_timesteps           | 129664        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0531        |
|    cost_value_loss           | 2.77e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.13          |
|    mean_cost_advantages      | 1.2281875e-05 |
|    mean_reward_advantages    | -1.3416996    |
|    n_updates                 | 10120         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.05e-09      |
|    reward_explained_variance | 0.164         |
|    reward_value_loss         | 10            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.99          |
|    ep_rew_mean               | 6.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1014          |
|    time_elapsed              | 183865        |
|    total_timesteps           | 129792        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0427        |
|    cost_value_loss           | 2.8e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.11e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.21          |
|    mean_cost_advantages      | -0.0003899413 |
|    mean_reward_advantages    | -0.6380156    |
|    n_updates                 | 10130         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -7.28e-10     |
|    reward_explained_variance | -0.0776       |
|    reward_value_loss         | 7.74          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.97          |
|    ep_rew_mean               | 6.95          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1015          |
|    time_elapsed              | 183941        |
|    total_timesteps           | 129920        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00801      |
|    cost_value_loss           | 2.81e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.6           |
|    mean_cost_advantages      | 0.00016540007 |
|    mean_reward_advantages    | -1.0597949    |
|    n_updates                 | 10140         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.33e-09      |
|    reward_explained_variance | 0.615         |
|    reward_value_loss         | 3.94          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1016          |
|    time_elapsed              | 184019        |
|    total_timesteps           | 130048        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00657       |
|    cost_value_loss           | 2.19e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.79e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.07          |
|    mean_cost_advantages      | -0.0001384367 |
|    mean_reward_advantages    | -0.11507649   |
|    n_updates                 | 10150         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.46e-09      |
|    reward_explained_variance | 0.675         |
|    reward_value_loss         | 2.76          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.05         |
|    ep_rew_mean               | 6.02         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1017         |
|    time_elapsed              | 184095       |
|    total_timesteps           | 130176       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0238       |
|    cost_value_loss           | 2.96e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.95e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.29         |
|    mean_cost_advantages      | 0.0002826071 |
|    mean_reward_advantages    | 0.61773324   |
|    n_updates                 | 10160        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -3.2e-10     |
|    reward_explained_variance | 0.307        |
|    reward_value_loss         | 4.3          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.86          |
|    ep_rew_mean               | 5.83          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1018          |
|    time_elapsed              | 184172        |
|    total_timesteps           | 130304        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0219       |
|    cost_value_loss           | 4.01e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.37          |
|    mean_cost_advantages      | -0.0002971483 |
|    mean_reward_advantages    | -0.48649287   |
|    n_updates                 | 10170         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.26e-10      |
|    reward_explained_variance | 0.374         |
|    reward_value_loss         | 3.53          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.67          |
|    ep_rew_mean               | 5.64          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1019          |
|    time_elapsed              | 184247        |
|    total_timesteps           | 130432        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0184        |
|    cost_value_loss           | 2.95e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.54          |
|    mean_cost_advantages      | 0.00020452801 |
|    mean_reward_advantages    | 0.19605199    |
|    n_updates                 | 10180         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 3.01e-09      |
|    reward_explained_variance | 0.29          |
|    reward_value_loss         | 4.21          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.7           |
|    ep_rew_mean               | 5.66          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1020          |
|    time_elapsed              | 184321        |
|    total_timesteps           | 130560        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0219       |
|    cost_value_loss           | 3.37e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.19          |
|    mean_cost_advantages      | 4.2003277e-05 |
|    mean_reward_advantages    | -0.28985047   |
|    n_updates                 | 10190         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.18e-09     |
|    reward_explained_variance | 0.523         |
|    reward_value_loss         | 3.85          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.86           |
|    ep_rew_mean               | 5.82           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1021           |
|    time_elapsed              | 184394         |
|    total_timesteps           | 130688         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0227        |
|    cost_value_loss           | 3.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.96e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 0.816          |
|    mean_cost_advantages      | -0.00015016904 |
|    mean_reward_advantages    | 0.1918185      |
|    n_updates                 | 10200          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.21e-09      |
|    reward_explained_variance | 0.502          |
|    reward_value_loss         | 2.59           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.82          |
|    ep_rew_mean               | 5.78          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1022          |
|    time_elapsed              | 184465        |
|    total_timesteps           | 130816        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00436       |
|    cost_value_loss           | 8.67e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.97e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.73          |
|    mean_cost_advantages      | -0.0006856327 |
|    mean_reward_advantages    | 0.68760324    |
|    n_updates                 | 10210         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.97e-09      |
|    reward_explained_variance | 0.174         |
|    reward_value_loss         | 4.93          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.07         |
|    ep_rew_mean               | 6.03         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1023         |
|    time_elapsed              | 184536       |
|    total_timesteps           | 130944       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.123       |
|    cost_value_loss           | 4.56e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.82e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.9          |
|    mean_cost_advantages      | -0.002119198 |
|    mean_reward_advantages    | -0.12191717  |
|    n_updates                 | 10220        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.77e-09     |
|    reward_explained_variance | -0.229       |
|    reward_value_loss         | 6.68         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.08          |
|    ep_rew_mean               | 6.04          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1024          |
|    time_elapsed              | 184607        |
|    total_timesteps           | 131072        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00154       |
|    cost_value_loss           | 3.24e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.57          |
|    mean_cost_advantages      | -6.239285e-05 |
|    mean_reward_advantages    | -0.1930202    |
|    n_updates                 | 10230         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.64e-10     |
|    reward_explained_variance | 0.293         |
|    reward_value_loss         | 3.88          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.03           |
|    ep_rew_mean               | 5.99           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1025           |
|    time_elapsed              | 184679         |
|    total_timesteps           | 131200         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0219        |
|    cost_value_loss           | 2.81e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.18           |
|    mean_cost_advantages      | -0.00017405135 |
|    mean_reward_advantages    | 1.4057733      |
|    n_updates                 | 10240          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.73e-10      |
|    reward_explained_variance | 0.22           |
|    reward_value_loss         | 6.97           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.08           |
|    ep_rew_mean               | 6.05           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1026           |
|    time_elapsed              | 184751         |
|    total_timesteps           | 131328         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0629         |
|    cost_value_loss           | 2.63e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.9e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.48           |
|    mean_cost_advantages      | -0.00022356592 |
|    mean_reward_advantages    | -0.28501058    |
|    n_updates                 | 10250          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 9.53e-10       |
|    reward_explained_variance | -0.0395        |
|    reward_value_loss         | 5.48           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.36           |
|    ep_rew_mean               | 6.33           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1027           |
|    time_elapsed              | 184822         |
|    total_timesteps           | 131456         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0516        |
|    cost_value_loss           | 3.33e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.67e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.98           |
|    mean_cost_advantages      | -0.00021870802 |
|    mean_reward_advantages    | 1.0345695      |
|    n_updates                 | 10260          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.37e-09      |
|    reward_explained_variance | -0.958         |
|    reward_value_loss         | 7.09           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.68           |
|    ep_rew_mean               | 6.64           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1028           |
|    time_elapsed              | 184894         |
|    total_timesteps           | 131584         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0125         |
|    cost_value_loss           | 2.57e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.94e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.89           |
|    mean_cost_advantages      | -5.6289275e-05 |
|    mean_reward_advantages    | 0.18187186     |
|    n_updates                 | 10270          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.4e-10        |
|    reward_explained_variance | -0.353         |
|    reward_value_loss         | 8.66           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.81          |
|    ep_rew_mean               | 6.77          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1029          |
|    time_elapsed              | 184965        |
|    total_timesteps           | 131712        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00662      |
|    cost_value_loss           | 2.51e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.84e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.72          |
|    mean_cost_advantages      | 6.7955894e-05 |
|    mean_reward_advantages    | 0.5066369     |
|    n_updates                 | 10280         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.7e-09      |
|    reward_explained_variance | -0.817        |
|    reward_value_loss         | 10.2          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.61          |
|    ep_rew_mean               | 6.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1030          |
|    time_elapsed              | 185037        |
|    total_timesteps           | 131840        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.011         |
|    cost_value_loss           | 2.28e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.58          |
|    mean_cost_advantages      | 0.00010588214 |
|    mean_reward_advantages    | 0.6006144     |
|    n_updates                 | 10290         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.61e-09      |
|    reward_explained_variance | -0.242        |
|    reward_value_loss         | 8.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.16          |
|    ep_rew_mean               | 6.12          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1031          |
|    time_elapsed              | 185109        |
|    total_timesteps           | 131968        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0121       |
|    cost_value_loss           | 3.66e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.05e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.19          |
|    mean_cost_advantages      | 4.7251633e-05 |
|    mean_reward_advantages    | -0.989688     |
|    n_updates                 | 10300         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.87e-09     |
|    reward_explained_variance | 0.646         |
|    reward_value_loss         | 4.32          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.57          |
|    ep_rew_mean               | 5.54          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1032          |
|    time_elapsed              | 185181        |
|    total_timesteps           | 132096        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0161        |
|    cost_value_loss           | 3.46e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.77e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.12          |
|    mean_cost_advantages      | 0.00047858933 |
|    mean_reward_advantages    | 0.003291592   |
|    n_updates                 | 10310         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.02e-10      |
|    reward_explained_variance | 0.267         |
|    reward_value_loss         | 4.17          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.34           |
|    ep_rew_mean               | 5.31           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1033           |
|    time_elapsed              | 185253         |
|    total_timesteps           | 132224         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00457        |
|    cost_value_loss           | 4.25e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.91           |
|    mean_cost_advantages      | -0.00032561066 |
|    mean_reward_advantages    | 0.33704433     |
|    n_updates                 | 10320          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.82e-09       |
|    reward_explained_variance | -0.00599       |
|    reward_value_loss         | 6.79           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.69         |
|    ep_rew_mean               | 5.66         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1034         |
|    time_elapsed              | 185325       |
|    total_timesteps           | 132352       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0234       |
|    cost_value_loss           | 3.91e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -6.16e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 6.09         |
|    mean_cost_advantages      | 0.0006838592 |
|    mean_reward_advantages    | 0.36643615   |
|    n_updates                 | 10330        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 7.74e-10     |
|    reward_explained_variance | -0.284       |
|    reward_value_loss         | 9.6          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.43          |
|    ep_rew_mean               | 5.41          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1035          |
|    time_elapsed              | 185397        |
|    total_timesteps           | 132480        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0192       |
|    cost_value_loss           | 2.42e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.75e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.41          |
|    mean_cost_advantages      | -0.0008184506 |
|    mean_reward_advantages    | 0.020196127   |
|    n_updates                 | 10340         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.56e-10      |
|    reward_explained_variance | 0.161         |
|    reward_value_loss         | 6.09          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.74          |
|    ep_rew_mean               | 5.72          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1036          |
|    time_elapsed              | 185468        |
|    total_timesteps           | 132608        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0328       |
|    cost_value_loss           | 4.72e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.02e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.21          |
|    mean_cost_advantages      | -9.694756e-05 |
|    mean_reward_advantages    | -0.62090504   |
|    n_updates                 | 10350         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.58e-12     |
|    reward_explained_variance | 0.46          |
|    reward_value_loss         | 3.32          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1037          |
|    time_elapsed              | 185540        |
|    total_timesteps           | 132736        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00414       |
|    cost_value_loss           | 4.52e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.93          |
|    mean_cost_advantages      | 0.00049983343 |
|    mean_reward_advantages    | 1.3343579     |
|    n_updates                 | 10360         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.09e-09     |
|    reward_explained_variance | 0.0866        |
|    reward_value_loss         | 5.96          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.71          |
|    ep_rew_mean               | 5.68          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1038          |
|    time_elapsed              | 185611        |
|    total_timesteps           | 132864        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0271        |
|    cost_value_loss           | 3.29e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.09e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.76          |
|    mean_cost_advantages      | -0.0005881189 |
|    mean_reward_advantages    | 0.6648599     |
|    n_updates                 | 10370         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.93e-09      |
|    reward_explained_variance | -0.126        |
|    reward_value_loss         | 5.52          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.17           |
|    ep_rew_mean               | 5.14           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1039           |
|    time_elapsed              | 185683         |
|    total_timesteps           | 132992         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0426        |
|    cost_value_loss           | 3.94e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 0.821          |
|    mean_cost_advantages      | -0.00074188143 |
|    mean_reward_advantages    | 0.40498334     |
|    n_updates                 | 10380          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.28e-09      |
|    reward_explained_variance | 0.45           |
|    reward_value_loss         | 3.45           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.68          |
|    ep_rew_mean               | 5.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1040          |
|    time_elapsed              | 185754        |
|    total_timesteps           | 133120        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00878      |
|    cost_value_loss           | 3.54e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.89e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.01          |
|    mean_cost_advantages      | 0.00069077447 |
|    mean_reward_advantages    | 0.6407033     |
|    n_updates                 | 10390         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.52e-10     |
|    reward_explained_variance | -0.326        |
|    reward_value_loss         | 7.05          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.45          |
|    ep_rew_mean               | 5.42          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1041          |
|    time_elapsed              | 185826        |
|    total_timesteps           | 133248        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0352        |
|    cost_value_loss           | 2.73e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.18          |
|    mean_cost_advantages      | 0.00014565793 |
|    mean_reward_advantages    | 0.66897       |
|    n_updates                 | 10400         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.58e-10     |
|    reward_explained_variance | 0.106         |
|    reward_value_loss         | 7.63          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.58          |
|    ep_rew_mean               | 5.55          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1042          |
|    time_elapsed              | 185897        |
|    total_timesteps           | 133376        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00423       |
|    cost_value_loss           | 2.7e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.71e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.33          |
|    mean_cost_advantages      | 0.00020373549 |
|    mean_reward_advantages    | 0.1491391     |
|    n_updates                 | 10410         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.59e-09     |
|    reward_explained_variance | -0.302        |
|    reward_value_loss         | 6.53          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.21          |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1043          |
|    time_elapsed              | 185968        |
|    total_timesteps           | 133504        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.000683     |
|    cost_value_loss           | 3.34e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.21          |
|    mean_cost_advantages      | 0.00034691143 |
|    mean_reward_advantages    | -0.06397979   |
|    n_updates                 | 10420         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.55e-09     |
|    reward_explained_variance | 0.286         |
|    reward_value_loss         | 4.7           |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.41          |
|    ep_rew_mean               | 6.37          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1044          |
|    time_elapsed              | 186040        |
|    total_timesteps           | 133632        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00568       |
|    cost_value_loss           | 2.03e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.02e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.57          |
|    mean_cost_advantages      | 4.2337735e-05 |
|    mean_reward_advantages    | 0.36114618    |
|    n_updates                 | 10430         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.91e-09     |
|    reward_explained_variance | -0.163        |
|    reward_value_loss         | 7.39          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.29          |
|    ep_rew_mean               | 6.25          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1045          |
|    time_elapsed              | 186113        |
|    total_timesteps           | 133760        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0153        |
|    cost_value_loss           | 3.22e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.02e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.86          |
|    mean_cost_advantages      | 3.7746504e-05 |
|    mean_reward_advantages    | -0.30328757   |
|    n_updates                 | 10440         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.1e-10      |
|    reward_explained_variance | 0.312         |
|    reward_value_loss         | 5.6           |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.35           |
|    ep_rew_mean               | 6.3            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1046           |
|    time_elapsed              | 186184         |
|    total_timesteps           | 133888         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0172        |
|    cost_value_loss           | 3.9e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.97           |
|    mean_cost_advantages      | -6.4197855e-05 |
|    mean_reward_advantages    | -0.36982927    |
|    n_updates                 | 10450          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.25e-10       |
|    reward_explained_variance | 0.0833         |
|    reward_value_loss         | 4.25           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.15          |
|    ep_rew_mean               | 6.11          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1047          |
|    time_elapsed              | 186255        |
|    total_timesteps           | 134016        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0421        |
|    cost_value_loss           | 2.59e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.27          |
|    mean_cost_advantages      | 8.9242574e-05 |
|    mean_reward_advantages    | 0.5557587     |
|    n_updates                 | 10460         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.38e-10      |
|    reward_explained_variance | 0.344         |
|    reward_value_loss         | 5.11          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.72          |
|    ep_rew_mean               | 5.69          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1048          |
|    time_elapsed              | 186326        |
|    total_timesteps           | 134144        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0046        |
|    cost_value_loss           | 3.46e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.63          |
|    mean_cost_advantages      | 0.00013302156 |
|    mean_reward_advantages    | -0.754655     |
|    n_updates                 | 10470         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.5e-09      |
|    reward_explained_variance | 0.153         |
|    reward_value_loss         | 4.27          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.6           |
|    ep_rew_mean               | 5.57          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1049          |
|    time_elapsed              | 186397        |
|    total_timesteps           | 134272        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00106       |
|    cost_value_loss           | 4.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.85e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.779         |
|    mean_cost_advantages      | 0.00037737557 |
|    mean_reward_advantages    | 0.06930567    |
|    n_updates                 | 10480         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.97e-10      |
|    reward_explained_variance | 0.597         |
|    reward_value_loss         | 2.31          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.14          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1050          |
|    time_elapsed              | 186468        |
|    total_timesteps           | 134400        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00125      |
|    cost_value_loss           | 7.3e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.9e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.13          |
|    mean_cost_advantages      | -0.0003815874 |
|    mean_reward_advantages    | 0.70023173    |
|    n_updates                 | 10490         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 5.06e-10      |
|    reward_explained_variance | 0.023         |
|    reward_value_loss         | 7.29          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.07          |
|    ep_rew_mean               | 6.05          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1051          |
|    time_elapsed              | 186539        |
|    total_timesteps           | 134528        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0514        |
|    cost_value_loss           | 2.95e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.72e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.7           |
|    mean_cost_advantages      | 5.8943617e-05 |
|    mean_reward_advantages    | 0.985299      |
|    n_updates                 | 10500         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.98e-09     |
|    reward_explained_variance | -0.305        |
|    reward_value_loss         | 8.53          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.17          |
|    ep_rew_mean               | 6.15          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1052          |
|    time_elapsed              | 186610        |
|    total_timesteps           | 134656        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00286       |
|    cost_value_loss           | 4.35e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.71          |
|    mean_cost_advantages      | 0.00011107869 |
|    mean_reward_advantages    | -0.7879217    |
|    n_updates                 | 10510         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.59e-09      |
|    reward_explained_variance | 0.256         |
|    reward_value_loss         | 6.54          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.05           |
|    ep_rew_mean               | 6.02           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1053           |
|    time_elapsed              | 186681         |
|    total_timesteps           | 134784         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00254        |
|    cost_value_loss           | 2.67e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.29           |
|    mean_cost_advantages      | -2.0533209e-05 |
|    mean_reward_advantages    | 0.13986762     |
|    n_updates                 | 10520          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.06e-10       |
|    reward_explained_variance | 0.0378         |
|    reward_value_loss         | 4.62           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.79          |
|    ep_rew_mean               | 5.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1054          |
|    time_elapsed              | 186752        |
|    total_timesteps           | 134912        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.011        |
|    cost_value_loss           | 2.74e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.1e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.55          |
|    mean_cost_advantages      | 0.00050339755 |
|    mean_reward_advantages    | 0.79501784    |
|    n_updates                 | 10530         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 8.16e-10      |
|    reward_explained_variance | -0.27         |
|    reward_value_loss         | 3.71          |
|    total_cost                | 0.0           |
------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 6.45            |
|    ep_rew_mean               | 5.42            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 1055            |
|    time_elapsed              | 186824          |
|    total_timesteps           | 135040          |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | 0.0325          |
|    cost_value_loss           | 2.72e-06        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -5.89e-06       |
|    learning_rate             | 0.0005          |
|    loss                      | 2.24            |
|    mean_cost_advantages      | -0.000109658926 |
|    mean_reward_advantages    | 0.6486398       |
|    n_updates                 | 10540           |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | -1.18e-09       |
|    reward_explained_variance | 0.258           |
|    reward_value_loss         | 4.56            |
|    total_cost                | 0.0             |
--------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.44           |
|    ep_rew_mean               | 5.4            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1056           |
|    time_elapsed              | 186911         |
|    total_timesteps           | 135168         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0373        |
|    cost_value_loss           | 3.37e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.87e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.32           |
|    mean_cost_advantages      | -0.00014704287 |
|    mean_reward_advantages    | -0.24206169    |
|    n_updates                 | 10550          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -7.31e-10      |
|    reward_explained_variance | 0.403          |
|    reward_value_loss         | 3.25           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.16          |
|    ep_rew_mean               | 5.13          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1057          |
|    time_elapsed              | 186997        |
|    total_timesteps           | 135296        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.022        |
|    cost_value_loss           | 3.4e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.72e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.27          |
|    mean_cost_advantages      | 0.00018319866 |
|    mean_reward_advantages    | -0.115564816  |
|    n_updates                 | 10560         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.92e-10     |
|    reward_explained_variance | 0.399         |
|    reward_value_loss         | 2.97          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.41           |
|    ep_rew_mean               | 5.38           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1058           |
|    time_elapsed              | 187079         |
|    total_timesteps           | 135424         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0645        |
|    cost_value_loss           | 3.36e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.9e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.62           |
|    mean_cost_advantages      | -3.8427544e-05 |
|    mean_reward_advantages    | 0.052834988    |
|    n_updates                 | 10570          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.73e-09       |
|    reward_explained_variance | 0.459          |
|    reward_value_loss         | 4.23           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.64          |
|    ep_rew_mean               | 5.61          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1059          |
|    time_elapsed              | 187162        |
|    total_timesteps           | 135552        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0109        |
|    cost_value_loss           | 2.57e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.88e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.05          |
|    mean_cost_advantages      | -0.0002042563 |
|    mean_reward_advantages    | -0.1207312    |
|    n_updates                 | 10580         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.42e-10      |
|    reward_explained_variance | 0.143         |
|    reward_value_loss         | 6.31          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.85          |
|    ep_rew_mean               | 5.82          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1060          |
|    time_elapsed              | 187243        |
|    total_timesteps           | 135680        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0154        |
|    cost_value_loss           | 3.09e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.01e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.954         |
|    mean_cost_advantages      | 0.00038829306 |
|    mean_reward_advantages    | 0.09625843    |
|    n_updates                 | 10590         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.2e-09      |
|    reward_explained_variance | 0.386         |
|    reward_value_loss         | 3.88          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.39          |
|    ep_rew_mean               | 6.35          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1061          |
|    time_elapsed              | 187322        |
|    total_timesteps           | 135808        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0506        |
|    cost_value_loss           | 1.81e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.89          |
|    mean_cost_advantages      | 0.00022092325 |
|    mean_reward_advantages    | 1.3182077     |
|    n_updates                 | 10600         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.27e-09     |
|    reward_explained_variance | -1            |
|    reward_value_loss         | 11.2          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.93           |
|    ep_rew_mean               | 6.89           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1062           |
|    time_elapsed              | 187403         |
|    total_timesteps           | 135936         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.113         |
|    cost_value_loss           | 2.64e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.78e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.25           |
|    mean_cost_advantages      | -5.4880693e-05 |
|    mean_reward_advantages    | 0.004616447    |
|    n_updates                 | 10610          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.23e-09       |
|    reward_explained_variance | 0.451          |
|    reward_value_loss         | 4.18           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.95           |
|    ep_rew_mean               | 6.91           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1063           |
|    time_elapsed              | 187479         |
|    total_timesteps           | 136064         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.00185        |
|    cost_value_loss           | 2.33e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.72e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.68           |
|    mean_cost_advantages      | -1.7626226e-05 |
|    mean_reward_advantages    | -0.23380646    |
|    n_updates                 | 10620          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 2.73e-10       |
|    reward_explained_variance | 0.333          |
|    reward_value_loss         | 4.78           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 8.09         |
|    ep_rew_mean               | 7.05         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1064         |
|    time_elapsed              | 187552       |
|    total_timesteps           | 136192       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.013        |
|    cost_value_loss           | 2.63e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.7          |
|    mean_cost_advantages      | 0.0003211041 |
|    mean_reward_advantages    | 0.16335304   |
|    n_updates                 | 10630        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.46e-09    |
|    reward_explained_variance | -0.21        |
|    reward_value_loss         | 7.43         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.63           |
|    ep_rew_mean               | 7.59           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1065           |
|    time_elapsed              | 187625         |
|    total_timesteps           | 136320         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0382        |
|    cost_value_loss           | 2.82e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.99e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.47           |
|    mean_cost_advantages      | -5.0915085e-05 |
|    mean_reward_advantages    | 0.25010532     |
|    n_updates                 | 10640          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.23e-09      |
|    reward_explained_variance | 0.185          |
|    reward_value_loss         | 8.46           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 8.34           |
|    ep_rew_mean               | 7.3            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1066           |
|    time_elapsed              | 187699         |
|    total_timesteps           | 136448         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0146        |
|    cost_value_loss           | 2.16e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.37           |
|    mean_cost_advantages      | -0.00031514443 |
|    mean_reward_advantages    | 0.974947       |
|    n_updates                 | 10650          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.05e-09      |
|    reward_explained_variance | -0.768         |
|    reward_value_loss         | 13.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.73          |
|    ep_rew_mean               | 6.7           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1067          |
|    time_elapsed              | 187772        |
|    total_timesteps           | 136576        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00209       |
|    cost_value_loss           | 2.53e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.87e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.24          |
|    mean_cost_advantages      | 0.00022941818 |
|    mean_reward_advantages    | -0.9077159    |
|    n_updates                 | 10660         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.16e-09      |
|    reward_explained_variance | 0.316         |
|    reward_value_loss         | 6.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.25          |
|    ep_rew_mean               | 6.21          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1068          |
|    time_elapsed              | 187845        |
|    total_timesteps           | 136704        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0257       |
|    cost_value_loss           | 3.36e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.35          |
|    mean_cost_advantages      | 0.00042067416 |
|    mean_reward_advantages    | -0.17152983   |
|    n_updates                 | 10670         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.06e-09      |
|    reward_explained_variance | 0.454         |
|    reward_value_loss         | 4.64          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.9           |
|    ep_rew_mean               | 5.86          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1069          |
|    time_elapsed              | 187919        |
|    total_timesteps           | 136832        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0101       |
|    cost_value_loss           | 3.45e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.98e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.25          |
|    mean_cost_advantages      | -0.0003191595 |
|    mean_reward_advantages    | -0.9258127    |
|    n_updates                 | 10680         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -9.37e-10     |
|    reward_explained_variance | 0.725         |
|    reward_value_loss         | 2.75          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.44           |
|    ep_rew_mean               | 6.39           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1070           |
|    time_elapsed              | 187993         |
|    total_timesteps           | 136960         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0208         |
|    cost_value_loss           | 2.89e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.94           |
|    mean_cost_advantages      | -8.8526394e-05 |
|    mean_reward_advantages    | 1.6561571      |
|    n_updates                 | 10690          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.38e-10      |
|    reward_explained_variance | -0.842         |
|    reward_value_loss         | 10.6           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.82           |
|    ep_rew_mean               | 5.78           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1071           |
|    time_elapsed              | 188066         |
|    total_timesteps           | 137088         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00722       |
|    cost_value_loss           | 2.06e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.94e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.89           |
|    mean_cost_advantages      | -0.00014088058 |
|    mean_reward_advantages    | 0.6569154      |
|    n_updates                 | 10700          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.23e-09      |
|    reward_explained_variance | -0.0881        |
|    reward_value_loss         | 8.59           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.89         |
|    ep_rew_mean               | 5.85         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1072         |
|    time_elapsed              | 188144       |
|    total_timesteps           | 137216       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00267      |
|    cost_value_loss           | 3.39e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.77e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.05         |
|    mean_cost_advantages      | 6.355939e-05 |
|    mean_reward_advantages    | -0.0979897   |
|    n_updates                 | 10710        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 3.21e-12     |
|    reward_explained_variance | 0.547        |
|    reward_value_loss         | 3.74         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.03          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1073          |
|    time_elapsed              | 188223        |
|    total_timesteps           | 137344        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0952       |
|    cost_value_loss           | 3.71e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.93e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.62          |
|    mean_cost_advantages      | -3.987865e-05 |
|    mean_reward_advantages    | -0.024258837  |
|    n_updates                 | 10720         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.1e-10       |
|    reward_explained_variance | 0.432         |
|    reward_value_loss         | 3.66          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.48           |
|    ep_rew_mean               | 6.45           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1074           |
|    time_elapsed              | 188300         |
|    total_timesteps           | 137472         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0145        |
|    cost_value_loss           | 2.46e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.71e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.69           |
|    mean_cost_advantages      | -0.00026808662 |
|    mean_reward_advantages    | -0.3563072     |
|    n_updates                 | 10730          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.76e-09      |
|    reward_explained_variance | 0.541          |
|    reward_value_loss         | 4.79           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.7          |
|    ep_rew_mean               | 6.67         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1075         |
|    time_elapsed              | 188373       |
|    total_timesteps           | 137600       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0404      |
|    cost_value_loss           | 2.53e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.78e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.3          |
|    mean_cost_advantages      | 0.0003418961 |
|    mean_reward_advantages    | 0.40713197   |
|    n_updates                 | 10740        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.55e-09    |
|    reward_explained_variance | 0.197        |
|    reward_value_loss         | 4.92         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.05           |
|    ep_rew_mean               | 6.03           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1076           |
|    time_elapsed              | 188447         |
|    total_timesteps           | 137728         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0267        |
|    cost_value_loss           | 2.94e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.72e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.94           |
|    mean_cost_advantages      | -6.2738894e-05 |
|    mean_reward_advantages    | 0.069187455    |
|    n_updates                 | 10750          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.37e-10       |
|    reward_explained_variance | -0.338         |
|    reward_value_loss         | 6.77           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.12           |
|    ep_rew_mean               | 6.1            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1077           |
|    time_elapsed              | 188521         |
|    total_timesteps           | 137856         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0211         |
|    cost_value_loss           | 3.96e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.43           |
|    mean_cost_advantages      | -0.00020646358 |
|    mean_reward_advantages    | -0.5754119     |
|    n_updates                 | 10760          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.32e-09      |
|    reward_explained_variance | -0.237         |
|    reward_value_loss         | 6.28           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.81          |
|    ep_rew_mean               | 5.79          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1078          |
|    time_elapsed              | 188594        |
|    total_timesteps           | 137984        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0105        |
|    cost_value_loss           | 3.33e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.96e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.58          |
|    mean_cost_advantages      | 0.00017986301 |
|    mean_reward_advantages    | 0.47589007    |
|    n_updates                 | 10770         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.62e-10     |
|    reward_explained_variance | 0.296         |
|    reward_value_loss         | 4.71          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.87           |
|    ep_rew_mean               | 5.85           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1079           |
|    time_elapsed              | 188666         |
|    total_timesteps           | 138112         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0124         |
|    cost_value_loss           | 3.42e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.76e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.42           |
|    mean_cost_advantages      | -8.0330545e-05 |
|    mean_reward_advantages    | -0.95563334    |
|    n_updates                 | 10780          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.32e-09       |
|    reward_explained_variance | 0.142          |
|    reward_value_loss         | 6.45           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.08           |
|    ep_rew_mean               | 6.06           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1080           |
|    time_elapsed              | 188738         |
|    total_timesteps           | 138240         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0386        |
|    cost_value_loss           | 2.21e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.5            |
|    mean_cost_advantages      | -0.00058672985 |
|    mean_reward_advantages    | 1.1679152      |
|    n_updates                 | 10790          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.28e-09       |
|    reward_explained_variance | -0.136         |
|    reward_value_loss         | 8.62           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.04          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1081          |
|    time_elapsed              | 188814        |
|    total_timesteps           | 138368        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00655       |
|    cost_value_loss           | 2.4e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.71e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.71          |
|    mean_cost_advantages      | 0.00034232537 |
|    mean_reward_advantages    | -0.13860403   |
|    n_updates                 | 10800         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.09e-10      |
|    reward_explained_variance | 0.515         |
|    reward_value_loss         | 3.31          |
|    total_cost                | 0.0           |
------------------------------------------------
--------------------------------------------------
| rollout/                     |                 |
|    ep_len_mean               | 6.94            |
|    ep_rew_mean               | 5.92            |
| time/                        |                 |
|    fps                       | 0               |
|    iterations                | 1082            |
|    time_elapsed              | 188888          |
|    total_timesteps           | 138496          |
| train/                       |                 |
|    approx_kl                 | 0.0             |
|    average_cost              | 0.0             |
|    clip_fraction             | 0               |
|    clip_range                | 0.2             |
|    cost_explained_variance   | -0.0156         |
|    cost_value_loss           | 3.09e-06        |
|    early_stop_epoch          | 10              |
|    entropy_loss              | -5.87e-06       |
|    learning_rate             | 0.0005          |
|    loss                      | 3.21            |
|    mean_cost_advantages      | -0.000103185645 |
|    mean_reward_advantages    | 0.95579463      |
|    n_updates                 | 10810           |
|    nu                        | 1.05            |
|    nu_loss                   | -0              |
|    policy_gradient_loss      | 1.14e-09        |
|    reward_explained_variance | -0.664          |
|    reward_value_loss         | 9.45            |
|    total_cost                | 0.0             |
--------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.18         |
|    ep_rew_mean               | 6.16         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1083         |
|    time_elapsed              | 188960       |
|    total_timesteps           | 138624       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0137       |
|    cost_value_loss           | 3.55e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.84e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.54         |
|    mean_cost_advantages      | 0.0002995762 |
|    mean_reward_advantages    | -1.0011983   |
|    n_updates                 | 10820        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -2.96e-09    |
|    reward_explained_variance | 0.288        |
|    reward_value_loss         | 5.09         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.92           |
|    ep_rew_mean               | 5.89           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1084           |
|    time_elapsed              | 189032         |
|    total_timesteps           | 138752         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0217         |
|    cost_value_loss           | 2.55e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.93e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.84           |
|    mean_cost_advantages      | -0.00018317322 |
|    mean_reward_advantages    | 0.5389238      |
|    n_updates                 | 10830          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.75e-09      |
|    reward_explained_variance | 0.195          |
|    reward_value_loss         | 5.88           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.26          |
|    ep_rew_mean               | 6.22          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1085          |
|    time_elapsed              | 189102        |
|    total_timesteps           | 138880        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0166        |
|    cost_value_loss           | 3.38e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.98e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.07          |
|    mean_cost_advantages      | 5.0882023e-05 |
|    mean_reward_advantages    | -0.22341631   |
|    n_updates                 | 10840         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.68e-09      |
|    reward_explained_variance | 0.492         |
|    reward_value_loss         | 3.39          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.02          |
|    ep_rew_mean               | 5.98          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1086          |
|    time_elapsed              | 189174        |
|    total_timesteps           | 139008        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0148        |
|    cost_value_loss           | 2.41e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.41          |
|    mean_cost_advantages      | 0.00028435956 |
|    mean_reward_advantages    | 2.1080046     |
|    n_updates                 | 10850         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.02e-09      |
|    reward_explained_variance | -2.55         |
|    reward_value_loss         | 17.9          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.96         |
|    ep_rew_mean               | 5.92         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1087         |
|    time_elapsed              | 189247       |
|    total_timesteps           | 139136       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0279       |
|    cost_value_loss           | 2.77e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.95e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.9          |
|    mean_cost_advantages      | 0.0006023827 |
|    mean_reward_advantages    | -0.95105726  |
|    n_updates                 | 10860        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.25e-09     |
|    reward_explained_variance | 0.228        |
|    reward_value_loss         | 6.24         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.54          |
|    ep_rew_mean               | 6.5           |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1088          |
|    time_elapsed              | 189321        |
|    total_timesteps           | 139264        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0147        |
|    cost_value_loss           | 2.75e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.91e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.884         |
|    mean_cost_advantages      | 0.00012391103 |
|    mean_reward_advantages    | -0.12553021   |
|    n_updates                 | 10870         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -5.45e-10     |
|    reward_explained_variance | 0.605         |
|    reward_value_loss         | 2.98          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7            |
|    ep_rew_mean               | 5.96         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1089         |
|    time_elapsed              | 189393       |
|    total_timesteps           | 139392       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0932      |
|    cost_value_loss           | 2.39e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.82e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 3.97         |
|    mean_cost_advantages      | 8.688419e-05 |
|    mean_reward_advantages    | 1.6732211    |
|    n_updates                 | 10880        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 5.91e-10     |
|    reward_explained_variance | -1.07        |
|    reward_value_loss         | 10.1         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.51         |
|    ep_rew_mean               | 5.49         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1090         |
|    time_elapsed              | 189465       |
|    total_timesteps           | 139520       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.000966     |
|    cost_value_loss           | 3.68e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.9e-06     |
|    learning_rate             | 0.0005       |
|    loss                      | 1.47         |
|    mean_cost_advantages      | 0.0007184801 |
|    mean_reward_advantages    | -0.4072297   |
|    n_updates                 | 10890        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.78e-09     |
|    reward_explained_variance | 0.457        |
|    reward_value_loss         | 4.46         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.59           |
|    ep_rew_mean               | 5.56           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1091           |
|    time_elapsed              | 189536         |
|    total_timesteps           | 139648         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00518       |
|    cost_value_loss           | 2.37e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.39           |
|    mean_cost_advantages      | -0.00061428826 |
|    mean_reward_advantages    | 0.2608578      |
|    n_updates                 | 10900          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 9.25e-11       |
|    reward_explained_variance | -0.225         |
|    reward_value_loss         | 7.31           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.09           |
|    ep_rew_mean               | 6.05           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1092           |
|    time_elapsed              | 189607         |
|    total_timesteps           | 139776         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0188         |
|    cost_value_loss           | 2.51e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.92e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.94           |
|    mean_cost_advantages      | -0.00026839238 |
|    mean_reward_advantages    | 0.6036085      |
|    n_updates                 | 10910          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.89e-10      |
|    reward_explained_variance | 0.357          |
|    reward_value_loss         | 4.7            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.21          |
|    ep_rew_mean               | 6.17          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1093          |
|    time_elapsed              | 189680        |
|    total_timesteps           | 139904        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0432       |
|    cost_value_loss           | 2.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.06e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.5           |
|    mean_cost_advantages      | -0.0001327676 |
|    mean_reward_advantages    | 0.9582498     |
|    n_updates                 | 10920         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.23e-09     |
|    reward_explained_variance | -0.403        |
|    reward_value_loss         | 7.76          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.45           |
|    ep_rew_mean               | 6.41           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1094           |
|    time_elapsed              | 189750         |
|    total_timesteps           | 140032         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0385        |
|    cost_value_loss           | 2.66e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.04e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.07           |
|    mean_cost_advantages      | -0.00010448899 |
|    mean_reward_advantages    | 0.37735745     |
|    n_updates                 | 10930          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.11e-10       |
|    reward_explained_variance | -0.116         |
|    reward_value_loss         | 7.17           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.81          |
|    ep_rew_mean               | 6.76          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1095          |
|    time_elapsed              | 189821        |
|    total_timesteps           | 140160        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0271        |
|    cost_value_loss           | 1.9e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.13          |
|    mean_cost_advantages      | -5.434758e-05 |
|    mean_reward_advantages    | -0.19257243   |
|    n_updates                 | 10940         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 6.73e-11      |
|    reward_explained_variance | 0.497         |
|    reward_value_loss         | 4.02          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.9            |
|    ep_rew_mean               | 6.85           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1096           |
|    time_elapsed              | 189892         |
|    total_timesteps           | 140288         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00105       |
|    cost_value_loss           | 2.81e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.94e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.96           |
|    mean_cost_advantages      | -0.00013626755 |
|    mean_reward_advantages    | -0.24478443    |
|    n_updates                 | 10950          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.39e-09      |
|    reward_explained_variance | 0.337          |
|    reward_value_loss         | 5.08           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.9           |
|    ep_rew_mean               | 6.84          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1097          |
|    time_elapsed              | 189962        |
|    total_timesteps           | 140416        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00469       |
|    cost_value_loss           | 3.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.78e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 0.849         |
|    mean_cost_advantages      | 0.00052280625 |
|    mean_reward_advantages    | 0.5595716     |
|    n_updates                 | 10960         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -8.88e-10     |
|    reward_explained_variance | 0.66          |
|    reward_value_loss         | 2.39          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.89           |
|    ep_rew_mean               | 6.84           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1098           |
|    time_elapsed              | 190033         |
|    total_timesteps           | 140544         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0308        |
|    cost_value_loss           | 2.53e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.08e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.68           |
|    mean_cost_advantages      | -0.00058663887 |
|    mean_reward_advantages    | 0.063133344    |
|    n_updates                 | 10970          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.64e-10      |
|    reward_explained_variance | 0.529          |
|    reward_value_loss         | 3.51           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.29           |
|    ep_rew_mean               | 6.25           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1099           |
|    time_elapsed              | 190104         |
|    total_timesteps           | 140672         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.119         |
|    cost_value_loss           | 2.69e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.78e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.47           |
|    mean_cost_advantages      | -0.00021584517 |
|    mean_reward_advantages    | 0.15334712     |
|    n_updates                 | 10980          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.23e-09       |
|    reward_explained_variance | -0.123         |
|    reward_value_loss         | 5.5            |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.25           |
|    ep_rew_mean               | 6.21           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1100           |
|    time_elapsed              | 190176         |
|    total_timesteps           | 140800         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0122         |
|    cost_value_loss           | 3.73e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.03           |
|    mean_cost_advantages      | -0.00017781512 |
|    mean_reward_advantages    | 0.0020675585   |
|    n_updates                 | 10990          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.4e-09       |
|    reward_explained_variance | 0.134          |
|    reward_value_loss         | 4.33           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.89         |
|    ep_rew_mean               | 5.85         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1101         |
|    time_elapsed              | 190246       |
|    total_timesteps           | 140928       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0179       |
|    cost_value_loss           | 3.14e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.74e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 2.01         |
|    mean_cost_advantages      | 0.0004512306 |
|    mean_reward_advantages    | 0.27511007   |
|    n_updates                 | 11000        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.19e-09    |
|    reward_explained_variance | 0.251        |
|    reward_value_loss         | 5.61         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.17           |
|    ep_rew_mean               | 6.13           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1102           |
|    time_elapsed              | 190316         |
|    total_timesteps           | 141056         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.044         |
|    cost_value_loss           | 3.8e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.13e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 0.639          |
|    mean_cost_advantages      | -0.00016875731 |
|    mean_reward_advantages    | -0.52743214    |
|    n_updates                 | 11010          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.82e-09      |
|    reward_explained_variance | 0.597          |
|    reward_value_loss         | 1.93           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.06          |
|    ep_rew_mean               | 6.02          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1103          |
|    time_elapsed              | 190387        |
|    total_timesteps           | 141184        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.021         |
|    cost_value_loss           | 2.39e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.73e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.94          |
|    mean_cost_advantages      | 0.00043421195 |
|    mean_reward_advantages    | 2.3885584     |
|    n_updates                 | 11020         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 1.93e-09      |
|    reward_explained_variance | -0.761        |
|    reward_value_loss         | 12            |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.97          |
|    ep_rew_mean               | 5.94          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1104          |
|    time_elapsed              | 190457        |
|    total_timesteps           | 141312        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00385       |
|    cost_value_loss           | 2.67e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.95e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.9           |
|    mean_cost_advantages      | 0.00012495456 |
|    mean_reward_advantages    | 0.15375288    |
|    n_updates                 | 11030         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.36e-10     |
|    reward_explained_variance | 0.0499        |
|    reward_value_loss         | 8.25          |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.63         |
|    ep_rew_mean               | 5.6          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1105         |
|    time_elapsed              | 190527       |
|    total_timesteps           | 141440       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.0367      |
|    cost_value_loss           | 2.96e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.88e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.67         |
|    mean_cost_advantages      | -4.91381e-05 |
|    mean_reward_advantages    | -0.99273086  |
|    n_updates                 | 11040        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.68e-09     |
|    reward_explained_variance | 0.384        |
|    reward_value_loss         | 4.78         |
|    total_cost                | 0.0          |
-----------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.93         |
|    ep_rew_mean               | 5.9          |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1106         |
|    time_elapsed              | 190597       |
|    total_timesteps           | 141568       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0138       |
|    cost_value_loss           | 3.92e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.75e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 0.876        |
|    mean_cost_advantages      | 0.0004572948 |
|    mean_reward_advantages    | 0.36014012   |
|    n_updates                 | 11050        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.19e-09    |
|    reward_explained_variance | 0.606        |
|    reward_value_loss         | 2.31         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.99          |
|    ep_rew_mean               | 5.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1107          |
|    time_elapsed              | 190667        |
|    total_timesteps           | 141696        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.024         |
|    cost_value_loss           | 4.01e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 3.17          |
|    mean_cost_advantages      | -0.0001873639 |
|    mean_reward_advantages    | 1.376401      |
|    n_updates                 | 11060         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.67e-09      |
|    reward_explained_variance | -0.3          |
|    reward_value_loss         | 8.56          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.02           |
|    ep_rew_mean               | 5.99           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1108           |
|    time_elapsed              | 190737         |
|    total_timesteps           | 141824         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0083         |
|    cost_value_loss           | 2.66e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -6.04e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.52           |
|    mean_cost_advantages      | -0.00014178421 |
|    mean_reward_advantages    | 1.2493892      |
|    n_updates                 | 11070          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.55e-09      |
|    reward_explained_variance | -0.17          |
|    reward_value_loss         | 7.06           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.82           |
|    ep_rew_mean               | 5.8            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1109           |
|    time_elapsed              | 190808         |
|    total_timesteps           | 141952         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0546         |
|    cost_value_loss           | 2.45e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.44           |
|    mean_cost_advantages      | -0.00025883428 |
|    mean_reward_advantages    | 0.93244725     |
|    n_updates                 | 11080          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 8.63e-10       |
|    reward_explained_variance | -0.104         |
|    reward_value_loss         | 7.37           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.85         |
|    ep_rew_mean               | 5.82         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1110         |
|    time_elapsed              | 190878       |
|    total_timesteps           | 142080       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.00659      |
|    cost_value_loss           | 3.09e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.88e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 0.606        |
|    mean_cost_advantages      | 0.0005006803 |
|    mean_reward_advantages    | -0.65856314  |
|    n_updates                 | 11090        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -4.51e-09    |
|    reward_explained_variance | 0.56         |
|    reward_value_loss         | 2.1          |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.42           |
|    ep_rew_mean               | 6.38           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1111           |
|    time_elapsed              | 190948         |
|    total_timesteps           | 142208         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0304        |
|    cost_value_loss           | 2.73e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.93e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.51           |
|    mean_cost_advantages      | -0.00022907188 |
|    mean_reward_advantages    | 1.7859304      |
|    n_updates                 | 11100          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.09e-09       |
|    reward_explained_variance | -0.534         |
|    reward_value_loss         | 10.5           |
|    total_cost                | 0.0            |
-------------------------------------------------
----------------------------------------------
| rollout/                     |             |
|    ep_len_mean               | 7.62        |
|    ep_rew_mean               | 6.57        |
| time/                        |             |
|    fps                       | 0           |
|    iterations                | 1112        |
|    time_elapsed              | 191019      |
|    total_timesteps           | 142336      |
| train/                       |             |
|    approx_kl                 | 0.0         |
|    average_cost              | 0.0         |
|    clip_fraction             | 0           |
|    clip_range                | 0.2         |
|    cost_explained_variance   | -0.00166    |
|    cost_value_loss           | 1.87e-06    |
|    early_stop_epoch          | 10          |
|    entropy_loss              | -5.99e-06   |
|    learning_rate             | 0.0005      |
|    loss                      | 1.73        |
|    mean_cost_advantages      | 3.78031e-05 |
|    mean_reward_advantages    | 0.29509902  |
|    n_updates                 | 11110       |
|    nu                        | 1.05        |
|    nu_loss                   | -0          |
|    policy_gradient_loss      | 9.08e-10    |
|    reward_explained_variance | 0.236       |
|    reward_value_loss         | 5.44        |
|    total_cost                | 0.0         |
----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.27           |
|    ep_rew_mean               | 6.23           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1113           |
|    time_elapsed              | 191089         |
|    total_timesteps           | 142464         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0441        |
|    cost_value_loss           | 2.13e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.95e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 4.87           |
|    mean_cost_advantages      | -0.00012570132 |
|    mean_reward_advantages    | 1.206672       |
|    n_updates                 | 11120          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.85e-11       |
|    reward_explained_variance | -0.958         |
|    reward_value_loss         | 10.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.08           |
|    ep_rew_mean               | 6.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1114           |
|    time_elapsed              | 191159         |
|    total_timesteps           | 142592         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0472         |
|    cost_value_loss           | 2.86e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.42           |
|    mean_cost_advantages      | -1.7677012e-05 |
|    mean_reward_advantages    | -0.3366354     |
|    n_updates                 | 11130          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.41e-09       |
|    reward_explained_variance | 0.212          |
|    reward_value_loss         | 4.76           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 6.99         |
|    ep_rew_mean               | 5.95         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1115         |
|    time_elapsed              | 191230       |
|    total_timesteps           | 142720       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.000543    |
|    cost_value_loss           | 2.99e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.89e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 1.86         |
|    mean_cost_advantages      | 5.522593e-05 |
|    mean_reward_advantages    | -0.2787439   |
|    n_updates                 | 11140        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -1.6e-09     |
|    reward_explained_variance | 0.499        |
|    reward_value_loss         | 3.53         |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.12          |
|    ep_rew_mean               | 6.09          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1116          |
|    time_elapsed              | 191300        |
|    total_timesteps           | 142848        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.023         |
|    cost_value_loss           | 3.06e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.74e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.7           |
|    mean_cost_advantages      | 0.00079271034 |
|    mean_reward_advantages    | 0.63232327    |
|    n_updates                 | 11150         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -2.59e-09     |
|    reward_explained_variance | 0.166         |
|    reward_value_loss         | 4.82          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.33           |
|    ep_rew_mean               | 5.32           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1117           |
|    time_elapsed              | 191371         |
|    total_timesteps           | 142976         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0539        |
|    cost_value_loss           | 2.84e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.83e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.12           |
|    mean_cost_advantages      | -0.00047447227 |
|    mean_reward_advantages    | -0.33968014    |
|    n_updates                 | 11160          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.94e-10       |
|    reward_explained_variance | 0.509          |
|    reward_value_loss         | 2.77           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.02           |
|    ep_rew_mean               | 5              |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1118           |
|    time_elapsed              | 191443         |
|    total_timesteps           | 143104         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0399         |
|    cost_value_loss           | 3.31e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.67e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.38           |
|    mean_cost_advantages      | -0.00018880659 |
|    mean_reward_advantages    | 0.79155123     |
|    n_updates                 | 11170          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.21e-09      |
|    reward_explained_variance | -0.0951        |
|    reward_value_loss         | 5.09           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.54          |
|    ep_rew_mean               | 5.52          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1119          |
|    time_elapsed              | 191513        |
|    total_timesteps           | 143232        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00365      |
|    cost_value_loss           | 3.72e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.83e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.19          |
|    mean_cost_advantages      | -0.0002700474 |
|    mean_reward_advantages    | -0.41247714   |
|    n_updates                 | 11180         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.75e-09     |
|    reward_explained_variance | 0.312         |
|    reward_value_loss         | 3.41          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.88           |
|    ep_rew_mean               | 5.86           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1120           |
|    time_elapsed              | 191584         |
|    total_timesteps           | 143360         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0899        |
|    cost_value_loss           | 3.28e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.89e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.21           |
|    mean_cost_advantages      | -0.00012677249 |
|    mean_reward_advantages    | 1.3280442      |
|    n_updates                 | 11190          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 3.3e-09        |
|    reward_explained_variance | -0.055         |
|    reward_value_loss         | 8.11           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.87          |
|    ep_rew_mean               | 5.85          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1121          |
|    time_elapsed              | 191654        |
|    total_timesteps           | 143488        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00127      |
|    cost_value_loss           | 2.34e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.17          |
|    mean_cost_advantages      | 0.00054470473 |
|    mean_reward_advantages    | 0.37304482    |
|    n_updates                 | 11200         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -1.21e-09     |
|    reward_explained_variance | 0.19          |
|    reward_value_loss         | 6.16          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.57          |
|    ep_rew_mean               | 5.56          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1122          |
|    time_elapsed              | 191725        |
|    total_timesteps           | 143616        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.00539       |
|    cost_value_loss           | 2.73e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.8e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 2.44          |
|    mean_cost_advantages      | 0.00019922086 |
|    mean_reward_advantages    | 0.73021746    |
|    n_updates                 | 11210         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.42e-11     |
|    reward_explained_variance | -0.0877       |
|    reward_value_loss         | 6.2           |
|    total_cost                | 0.0           |
------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.2          |
|    ep_rew_mean               | 6.17         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1123         |
|    time_elapsed              | 191795       |
|    total_timesteps           | 143744       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | -0.00157     |
|    cost_value_loss           | 3.93e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.77e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 0.718        |
|    mean_cost_advantages      | 0.0004322515 |
|    mean_reward_advantages    | -0.9534472   |
|    n_updates                 | 11220        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 2.25e-10     |
|    reward_explained_variance | 0.721        |
|    reward_value_loss         | 2.2          |
|    total_cost                | 0.0          |
-----------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.46          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1124          |
|    time_elapsed              | 191865        |
|    total_timesteps           | 143872        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0342       |
|    cost_value_loss           | 2.17e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.02e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.46          |
|    mean_cost_advantages      | -0.0007936362 |
|    mean_reward_advantages    | 1.0199802     |
|    n_updates                 | 11230         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3e-09        |
|    reward_explained_variance | 0.323         |
|    reward_value_loss         | 5.51          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.87          |
|    ep_rew_mean               | 5.85          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1125          |
|    time_elapsed              | 191935        |
|    total_timesteps           | 144000        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0151        |
|    cost_value_loss           | 2.75e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.72e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.57          |
|    mean_cost_advantages      | 0.00015420272 |
|    mean_reward_advantages    | 0.8277111     |
|    n_updates                 | 11240         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 4.73e-09      |
|    reward_explained_variance | 0.0716        |
|    reward_value_loss         | 3.51          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.84           |
|    ep_rew_mean               | 5.82           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1126           |
|    time_elapsed              | 192005         |
|    total_timesteps           | 144128         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0241         |
|    cost_value_loss           | 2.54e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.73e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 5.79           |
|    mean_cost_advantages      | 0.000104765015 |
|    mean_reward_advantages    | 2.1516523      |
|    n_updates                 | 11250          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -2.14e-09      |
|    reward_explained_variance | -1.38          |
|    reward_value_loss         | 13.2           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.31           |
|    ep_rew_mean               | 6.28           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1127           |
|    time_elapsed              | 192076         |
|    total_timesteps           | 144256         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0194         |
|    cost_value_loss           | 2.42e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.95e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3              |
|    mean_cost_advantages      | -0.00027609512 |
|    mean_reward_advantages    | 0.31274414     |
|    n_updates                 | 11260          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.69e-09      |
|    reward_explained_variance | 0.317          |
|    reward_value_loss         | 6.64           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.79           |
|    ep_rew_mean               | 6.76           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1128           |
|    time_elapsed              | 192146         |
|    total_timesteps           | 144384         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0267         |
|    cost_value_loss           | 1.72e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 6.07           |
|    mean_cost_advantages      | -2.5481975e-05 |
|    mean_reward_advantages    | 1.2608756      |
|    n_updates                 | 11270          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.35e-10      |
|    reward_explained_variance | -0.986         |
|    reward_value_loss         | 17.3           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.99          |
|    ep_rew_mean               | 6.96          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1129          |
|    time_elapsed              | 192217        |
|    total_timesteps           | 144512        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0211       |
|    cost_value_loss           | 2.56e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.76e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.53          |
|    mean_cost_advantages      | -9.962976e-05 |
|    mean_reward_advantages    | -1.4249       |
|    n_updates                 | 11280         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.78e-09     |
|    reward_explained_variance | 0.488         |
|    reward_value_loss         | 6.29          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.89          |
|    ep_rew_mean               | 6.86          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1130          |
|    time_elapsed              | 192287        |
|    total_timesteps           | 144640        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0448       |
|    cost_value_loss           | 2.2e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.99e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 5.42          |
|    mean_cost_advantages      | -8.682844e-05 |
|    mean_reward_advantages    | 0.9653554     |
|    n_updates                 | 11290         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 9.28e-11      |
|    reward_explained_variance | -1.32         |
|    reward_value_loss         | 14.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.7            |
|    ep_rew_mean               | 6.67           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1131           |
|    time_elapsed              | 192358         |
|    total_timesteps           | 144768         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0167        |
|    cost_value_loss           | 3.01e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.72e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.71           |
|    mean_cost_advantages      | -0.00040617853 |
|    mean_reward_advantages    | 0.93616563     |
|    n_updates                 | 11300          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.73e-09       |
|    reward_explained_variance | -0.569         |
|    reward_value_loss         | 9.69           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.68          |
|    ep_rew_mean               | 6.65          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1132          |
|    time_elapsed              | 192428        |
|    total_timesteps           | 144896        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.096        |
|    cost_value_loss           | 3e-06         |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.92e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 2.56          |
|    mean_cost_advantages      | -9.008507e-05 |
|    mean_reward_advantages    | 0.7693981     |
|    n_updates                 | 11310         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 2.14e-09      |
|    reward_explained_variance | -0.186        |
|    reward_value_loss         | 6.83          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.37           |
|    ep_rew_mean               | 6.35           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1133           |
|    time_elapsed              | 192498         |
|    total_timesteps           | 145024         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0294        |
|    cost_value_loss           | 3.32e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.37           |
|    mean_cost_advantages      | -3.0458072e-05 |
|    mean_reward_advantages    | 0.7865695      |
|    n_updates                 | 11320          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -5.24e-10      |
|    reward_explained_variance | 0.0148         |
|    reward_value_loss         | 5.82           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.4          |
|    ep_rew_mean               | 6.38         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1134         |
|    time_elapsed              | 192569       |
|    total_timesteps           | 145152       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0174       |
|    cost_value_loss           | 2.58e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.69e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.09         |
|    mean_cost_advantages      | 0.0005317354 |
|    mean_reward_advantages    | 0.6465831    |
|    n_updates                 | 11330        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | -8.08e-10    |
|    reward_explained_variance | -0.366       |
|    reward_value_loss         | 11           |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.17           |
|    ep_rew_mean               | 6.15           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1135           |
|    time_elapsed              | 192640         |
|    total_timesteps           | 145280         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0272        |
|    cost_value_loss           | 2.63e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.84           |
|    mean_cost_advantages      | -0.00048010453 |
|    mean_reward_advantages    | 0.859071       |
|    n_updates                 | 11340          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.96e-09      |
|    reward_explained_variance | -0.108         |
|    reward_value_loss         | 6.84           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.99           |
|    ep_rew_mean               | 5.97           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1136           |
|    time_elapsed              | 192712         |
|    total_timesteps           | 145408         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.013         |
|    cost_value_loss           | 3.18e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.81e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.69           |
|    mean_cost_advantages      | -0.00032374673 |
|    mean_reward_advantages    | 0.463111       |
|    n_updates                 | 11350          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 1.91e-09       |
|    reward_explained_variance | -0.219         |
|    reward_value_loss         | 7.62           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.19          |
|    ep_rew_mean               | 6.16          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1137          |
|    time_elapsed              | 192784        |
|    total_timesteps           | 145536        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0145       |
|    cost_value_loss           | 2.14e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -6.1e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 1.88          |
|    mean_cost_advantages      | 0.00022919422 |
|    mean_reward_advantages    | 0.11576594    |
|    n_updates                 | 11360         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | 7.05e-10      |
|    reward_explained_variance | 0.0499        |
|    reward_value_loss         | 5.85          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.42           |
|    ep_rew_mean               | 6.39           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1138           |
|    time_elapsed              | 192861         |
|    total_timesteps           | 145664         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.00109       |
|    cost_value_loss           | 3.04e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.91e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.78           |
|    mean_cost_advantages      | -0.00025755615 |
|    mean_reward_advantages    | 1.2734218      |
|    n_updates                 | 11370          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.38e-10      |
|    reward_explained_variance | -0.601         |
|    reward_value_loss         | 9.66           |
|    total_cost                | 0.0            |
-------------------------------------------------
-----------------------------------------------
| rollout/                     |              |
|    ep_len_mean               | 7.13         |
|    ep_rew_mean               | 6.09         |
| time/                        |              |
|    fps                       | 0            |
|    iterations                | 1139         |
|    time_elapsed              | 192934       |
|    total_timesteps           | 145792       |
| train/                       |              |
|    approx_kl                 | 0.0          |
|    average_cost              | 0.0          |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    cost_explained_variance   | 0.0444       |
|    cost_value_loss           | 2.16e-06     |
|    early_stop_epoch          | 10           |
|    entropy_loss              | -5.85e-06    |
|    learning_rate             | 0.0005       |
|    loss                      | 5.68         |
|    mean_cost_advantages      | 4.664636e-05 |
|    mean_reward_advantages    | 1.0654857    |
|    n_updates                 | 11380        |
|    nu                        | 1.05         |
|    nu_loss                   | -0           |
|    policy_gradient_loss      | 1.21e-09     |
|    reward_explained_variance | -0.641       |
|    reward_value_loss         | 12.5         |
|    total_cost                | 0.0          |
-----------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.43           |
|    ep_rew_mean               | 6.39           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1140           |
|    time_elapsed              | 193006         |
|    total_timesteps           | 145920         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0203        |
|    cost_value_loss           | 3.22e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.85e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.11           |
|    mean_cost_advantages      | -0.00010399705 |
|    mean_reward_advantages    | -1.0207633     |
|    n_updates                 | 11390          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -3.35e-09      |
|    reward_explained_variance | 0.264          |
|    reward_value_loss         | 6.69           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.09           |
|    ep_rew_mean               | 6.04           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1141           |
|    time_elapsed              | 193081         |
|    total_timesteps           | 146048         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.0232         |
|    cost_value_loss           | 2.08e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.87e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.22           |
|    mean_cost_advantages      | -0.00021848414 |
|    mean_reward_advantages    | -0.22015786    |
|    n_updates                 | 11400          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.55e-10      |
|    reward_explained_variance | 0.671          |
|    reward_value_loss         | 2.84           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 6.66          |
|    ep_rew_mean               | 5.62          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1142          |
|    time_elapsed              | 193154        |
|    total_timesteps           | 146176        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.0316       |
|    cost_value_loss           | 2.74e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.98e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.45          |
|    mean_cost_advantages      | 0.00020193977 |
|    mean_reward_advantages    | -0.1385591    |
|    n_updates                 | 11410         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -6.39e-10     |
|    reward_explained_variance | 0.448         |
|    reward_value_loss         | 3.81          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.73           |
|    ep_rew_mean               | 5.7            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1143           |
|    time_elapsed              | 193229         |
|    total_timesteps           | 146304         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.000682       |
|    cost_value_loss           | 3.32e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.74e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.2            |
|    mean_cost_advantages      | -0.00029366894 |
|    mean_reward_advantages    | 0.9008065      |
|    n_updates                 | 11420          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -1.73e-09      |
|    reward_explained_variance | 0.168          |
|    reward_value_loss         | 4.71           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.95           |
|    ep_rew_mean               | 5.91           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1144           |
|    time_elapsed              | 193302         |
|    total_timesteps           | 146432         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0263        |
|    cost_value_loss           | 2.4e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.78e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.57           |
|    mean_cost_advantages      | -0.00059214374 |
|    mean_reward_advantages    | 0.47437197     |
|    n_updates                 | 11430          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 5.25e-10       |
|    reward_explained_variance | 0.121          |
|    reward_value_loss         | 7.29           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.02           |
|    ep_rew_mean               | 5.98           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1145           |
|    time_elapsed              | 193372         |
|    total_timesteps           | 146560         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -3.31e-05      |
|    cost_value_loss           | 2.88e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.82e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 2.49           |
|    mean_cost_advantages      | -9.9583995e-06 |
|    mean_reward_advantages    | 0.29657778     |
|    n_updates                 | 11440          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 6.32e-11       |
|    reward_explained_variance | 0.462          |
|    reward_value_loss         | 4.83           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 6.92           |
|    ep_rew_mean               | 5.89           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1146           |
|    time_elapsed              | 193445         |
|    total_timesteps           | 146688         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | 0.041          |
|    cost_value_loss           | 2.8e-06        |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.98e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 1.22           |
|    mean_cost_advantages      | -0.00033857406 |
|    mean_reward_advantages    | 0.10059781     |
|    n_updates                 | 11450          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -6.37e-10      |
|    reward_explained_variance | 0.582          |
|    reward_value_loss         | 3.28           |
|    total_cost                | 0.0            |
-------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.49           |
|    ep_rew_mean               | 6.46           |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1147           |
|    time_elapsed              | 193520         |
|    total_timesteps           | 146816         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0676        |
|    cost_value_loss           | 2.99e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.8e-06       |
|    learning_rate             | 0.0005         |
|    loss                      | 1.28           |
|    mean_cost_advantages      | -0.00085031363 |
|    mean_reward_advantages    | -0.73464096    |
|    n_updates                 | 11460          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | -9.54e-10      |
|    reward_explained_variance | 0.318          |
|    reward_value_loss         | 3.9            |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.45          |
|    ep_rew_mean               | 6.42          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1148          |
|    time_elapsed              | 193594        |
|    total_timesteps           | 146944        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | -0.00444      |
|    cost_value_loss           | 3.3e-06       |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.7e-06      |
|    learning_rate             | 0.0005        |
|    loss                      | 3.21          |
|    mean_cost_advantages      | 3.0762603e-06 |
|    mean_reward_advantages    | 1.4604768     |
|    n_updates                 | 11470         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3e-09        |
|    reward_explained_variance | -0.544        |
|    reward_value_loss         | 11.5          |
|    total_cost                | 0.0           |
------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.47          |
|    ep_rew_mean               | 6.44          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1149          |
|    time_elapsed              | 193669        |
|    total_timesteps           | 147072        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.0248        |
|    cost_value_loss           | 3.06e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.86e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 4.05          |
|    mean_cost_advantages      | 0.00013578567 |
|    mean_reward_advantages    | 0.1325878     |
|    n_updates                 | 11480         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -3.88e-10     |
|    reward_explained_variance | -0.484        |
|    reward_value_loss         | 10.3          |
|    total_cost                | 0.0           |
------------------------------------------------
-------------------------------------------------
| rollout/                     |                |
|    ep_len_mean               | 7.23           |
|    ep_rew_mean               | 6.2            |
| time/                        |                |
|    fps                       | 0              |
|    iterations                | 1150           |
|    time_elapsed              | 193744         |
|    total_timesteps           | 147200         |
| train/                       |                |
|    approx_kl                 | 0.0            |
|    average_cost              | 0.0            |
|    clip_fraction             | 0              |
|    clip_range                | 0.2            |
|    cost_explained_variance   | -0.0452        |
|    cost_value_loss           | 2.25e-06       |
|    early_stop_epoch          | 10             |
|    entropy_loss              | -5.84e-06      |
|    learning_rate             | 0.0005         |
|    loss                      | 3.51           |
|    mean_cost_advantages      | -0.00018103104 |
|    mean_reward_advantages    | 0.4487382      |
|    n_updates                 | 11490          |
|    nu                        | 1.05           |
|    nu_loss                   | -0             |
|    policy_gradient_loss      | 4.3e-09        |
|    reward_explained_variance | -1.03          |
|    reward_value_loss         | 9.53           |
|    total_cost                | 0.0            |
-------------------------------------------------
------------------------------------------------
| rollout/                     |               |
|    ep_len_mean               | 7.28          |
|    ep_rew_mean               | 6.26          |
| time/                        |               |
|    fps                       | 0             |
|    iterations                | 1151          |
|    time_elapsed              | 193818        |
|    total_timesteps           | 147328        |
| train/                       |               |
|    approx_kl                 | 0.0           |
|    average_cost              | 0.0           |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    cost_explained_variance   | 0.000885      |
|    cost_value_loss           | 1.82e-06      |
|    early_stop_epoch          | 10            |
|    entropy_loss              | -5.81e-06     |
|    learning_rate             | 0.0005        |
|    loss                      | 1.81          |
|    mean_cost_advantages      | 0.00037620883 |
|    mean_reward_advantages    | -0.27666312   |
|    n_updates                 | 11500         |
|    nu                        | 1.05          |
|    nu_loss                   | -0            |
|    policy_gradient_loss      | -4.33e-10     |
|    reward_explained_variance | 0.461         |
|    reward_value_loss         | 5.08          |
|    total_cost                | 0.0           |
------------------------------------------------
